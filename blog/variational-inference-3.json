{"title":"Variational Inference III 🤖🎯","slug":"variational-inference-3","tags":["#probabilistic-machine-learning","#variational-inference","#statistics"],"date":"2017-05-11","html":"\n<p class=\"post_p\">\n\tIn this post I'm going to continue explaining concepts that I introduced in previous posts\n\t(<a href=\"https://albertopou.dev/blog/variational-inference-1/\" class=\"post_link\" target=\"_blank\">1</a> and\n\t<a href=\"https://albertopou.dev/blog/variational-inference-2/\" class=\"post_link\" target=\"_blank\">2</a>) about Variational\n\tinference (VI). Algorithms presented till now have scalability problems. For each iteration the algorithm\n\trequires to go through all the data and this, for massive volumes of data, is impracticable. An alternative to\n\tsolve this problem is: Sthocastic Variational Inference. This version is based on using a batch (set of points)\n\tof data in each iteration. In this way, after more iterations than the conventional solutions, the solution\n\twill tend to a local optimum. The main advantage of this mechanism is that it doesn't require to keep all dataset in\n\tmemory, solving the bottleneck that could be formed by using VI with very big datasets.\n</p>\n<h4 class=\"post_section_title text-2xl text-black\">Sthocastic Variational Inference</h4>\n<h5 class=\"post_section_title text-xl text-black\">Sthocastic optimization</h5>\n<p class=\"post_p\">\n\tThis technique obtains estimations of the real gradient of an objective function.\n\tThus we obtain an algorithm which iterates each batch and adjusts the hidden structure of the model based only\n\ton that batch of data. Stochastic optimization finds a function relative maximum or minimum using real gradient\n\testimations. Estimations expectation  E(∇λC(λ,x)), being x a batch of data, is equivalent to the\n\treal gradient ∇λC(λ,X), being X the full dataset.\n</p>\n<figure class=\"post_figure flex justify-center\">\n\t<img class=\"post_image\"\n\t\t src=\"variational-inference-3/svi.png\"\n\t\t alt=\"Sthocastic Optimization\">\n</figure>\n<p class=\"post_p\">\n\tUnder ideal conditions these stochastic algorithms converge to a local optimum of the function\n\tif ρ meets the Robins-Monro conditions:\n</p>\n<figure class=\"post_figure flex justify-center\">\n\t<img class=\"post_image\"\n\t\t src=\"variational-inference-3/robins.png\"\n\t\t alt=\"Robins-Monro\">\n</figure>\n<p class=\"post_p\">\n\tThe use of this technique results in the algorithm Stochastic Gradient Ascent Variational Inference (SGAVI)\n\tand Stochastic Natural Gradient Ascent Variational Inference (SNGAVI) if natural gradients are used to estimate.\n\tThese algorthims, thanks to the real gradient estimations, can avoid saddle points.\n</p>\n<figure class=\"post_figure flex justify-center\">\n\t<img class=\"post_image\"\n\t\t src=\"variational-inference-3/saddle-point.png\"\n\t\t alt=\"Saddle point\">\n</figure>\n<figcaption class=\"post_image_figcaption\">\n\tSaddle point. (Source: Wikipedia)\n</figcaption>\n<h5 class=\"post_section_title text-xl text-black\">Algorithm</h5>\n<p class=\"post_p\">\n\tIn VI, the function to be optimized is called ELBO. If variational parameters are updated by closed analytical\n\tformulas, the algorithm is known as Sthocastic Coordinate Ascent Variational Inference (SCAVI) while if we use\n\tsthocastic optimization the algorithm is known as SGAVI. This version uses a corrector term using calculations\n\tfrom previous iterations. The generic algorithm would be as follows:\n</p>\n<figure class=\"post_figure flex justify-center\">\n\t<img class=\"post_image\"\n\t\t src=\"variational-inference-3/svi-algorithm.png\"\n\t\t alt=\"Sthocastic Variational Inference Algorithm\">\n</figure>\n<h4 class=\"post_section_title text-2xl text-black\">Black Box Variational Inference</h4>\n<p class=\"post_p\">\n\tStarting from the ELBO formula that was reached in the\n\t<a href=\"https://albertopou.herokuapp.com/#variational_inference_2\" class=\"post_link\" target=\"_blank\">previous post</a>:\n</p>\n<figure class=\"post_figure flex justify-center\">\n\t<img class=\"post_image\"\n\t\t src=\"variational-inference-3/elbo2.png\"\n\t\t alt=\"ELBO\">\n</figure>\n<p class=\"post_p\">\n\tThe main idea of Black Box Variational Inference (BBVI) consists of sampling the variational model q(θ|λ) with\n\tthe purpose of obtain an approach of formula expectations. Such expectations regarding the variational model\n\tcan be computationally expensive and can be a bottleneck at computer memory level.\n</p>\n<h5 class=\"post_section_title text-xl text-black\">Score Gradients</h5>\n<p class=\"post_p\">\n\tNow we apply gradients and some algebraic transformations into the analytic ELBO formula (complete derivation\n\tis not shown):\n</p>\n<figure class=\"post_figure flex justify-center\">\n\t<img class=\"post_image\"\n\t\t src=\"variational-inference-3/elbo3.png\"\n\t\t alt=\"ELBO\">\n</figure>\n<p class=\"post_p\">\n\tAfter these transformations you could notice that it is not necesary to obtain gradients of complete ELBO, it\n\tis enough to derive the variational model (q(θ|λ)).\n</p>\n<h5 class=\"post_section_title text-xl text-black\">Monte-Carlo Integration</h5>\n<p class=\"post_p\">\n\tMonte-Carlo integration is a mechanism to approximate integrals. It consists of sampling the variable\n\twith respect to which we are integrating and do a summation of function values given those samples.\n\tThe more samples are taken from the variable more exact will be the approximation. In our case we are going\n\tto use this mechanism to approximate ELBO integral, which integrates with respect to the q(θ|λ) distribution.\n\tThen a set of samples (θs) obtained from the q(θ|λ) distribution will allow us to get an approximation of\n\tthe integral. The formula would be as follows:\n</p>\n<figure class=\"post_figure flex justify-center\">\n\t<img class=\"post_image\"\n\t\t src=\"variational-inference-3/montecarlo.png\"\n\t\t alt=\"Monte-Carlo Integration\">\n</figure>\n<h5 class=\"post_section_title text-xl text-black\">Algorithm</h5>\n<p class=\"post_p\">\n\tBelow is the BBVI algorithm:\n</p>\n<figure class=\"post_figure flex justify-center\">\n\t<img class=\"post_image\"\n\t\t src=\"variational-inference-3/bbvi.png\"\n\t\t alt=\"Black Box Variational Inference Algorithm\">\n</figure>\n<h4 class=\"post_section_title text-2xl text-black\">Considerations</h4>\n<p class=\"post_p\">\n\tThis algorithm is the result of get some measures that can question the convergence of VI algorithm. Starting by\n\tsupposing q(θ|λ) factorizes (mean field assumption) to approximate ELBO integral using Monte-Carlo.\n\tAll this causes that this algorithm is subjected to a high variance, and depending on the model, slow convergence.\n\tIn order to reduce the variance that this method can cause some mechanisms have appeared:\n</p>\n<ul class=\"post_list\">\n\t<li>\n\t\t<strong>Rao-Blackwellization</strong>. This method reduces the variance of an aleatory variable replacing it\n\t\tby its conditional expectation with respect to a subset of variables.\n\t</li>\n\t<li>\n\t\t<strong>Control variates</strong>. This method replaces Monte-Carlo expectation by another function with\n\t\tless variance.\n\t</li>\n</ul>\n<p class=\"post_p\">\n\tA positive aspect of this method is that it is not necessary to derive the analytical formulas used to update\n\tthe variational parameters either analytical ELBO. This permits the access to these algorithms to people who have\n\tless statistical knowledge (as me). It exists another approximation called Automatic Differentiation Variational\n\tInference (ADVI). This method also has these advantages and improves the convergence of unconjugated models\n\tthat can be a challenge for the rest of VI variants.\n</p>\n<p class=\"post_p\">\n\tFormerly with VI only conjugated models could be inferred since the unconjugated models didn't be easily derivable.\n\tThe discovery of algorithms like ADVI and BBVI allowed the inference of this kind of models because they changed\n\tthe analytical calculus for an approximate strategy.\n</p>\n<h4 class=\"post_section_title text-2xl text-black\">Variational inference libraries</h4>\n<p class=\"post_p\">\n\tBest-known libraries for the use of VI and Markov Chain Monte-Carlo (MCMC):\n</p>\n<ul class=\"post_list\">\n\t<li>\n\t\t<a href=\"http://edwardlib.org/\" class=\"post_link\" target=\"_blank\">Edward</a>: It uses Tensorflow for gradients computations\n\t\tand has BBVI, reparameterization BBVI and Metropolis-Hastings implementations.\n\t</li>\n\t<li>\n\t\t<a href=\"http://mc-stan.org/\" class=\"post_link\" target=\"_blank\">Stan</a>: It uses C++ Automatic Differentiation\n\t\treverse mode for gradients computation and has ADVI and HMC implementations.\n\t</li>\n\t<li>\n\t\t<a href=\"https://github.com/pymc-devs/pymc3\" class=\"post_link\" target=\"_blank\">PyMC3</a>: It uses Theano for gradient computations\n\t\tand has ADVI, Gibbs sampling and Metropolis-Hastings implementations.\n\t</li>\n\t<li>\n\t\t<a href=\"https://www.tensorflow.org/versions/r0.12/api_docs/python/contrib.bayesflow.variational_inference/\" class=\"post_link\" target=\"_blank\">Bayesflow</a>:\n\t\tYoung Google module for VI.\n\t</li>\n</ul>\n<p class=\"post_p\">\n\tIf you want to learn more about Variational Inference and its uses in probabilistic models inference you can take\n\ta look to my <a href=\"https://github.com/bertini36/GMM/blob/master/docs/doc.pdf\" class=\"post_link\" target=\"_blank\">\n\tmaster thesis</a>. It focuses on the use of automatic differentiation tools to apply Variational Inference into a\n\tGaussian Mixture Model (GMM). At the <a href=\"https://github.com/bertini36/GMM\" class=\"post_link\" target=\"_blank\">repository</a> you can\n\tfind implementations of Gaussian Mixture Model with different technologies like\n\t<a href=\"https://www.tensorflow.org/\" class=\"post_link\" target=\"_blank\">Tensorflow</a>, Python,\n\t<a href=\"https://github.com/blei-lab/edward\" class=\"post_link\" target=\"_blank\">Edward</a>,\n\t<a href=\"https://github.com/HIPS/autograd\" class=\"post_link\" target=\"_blank\">Autograd</a>, ...\n\tAnd also other probabilistic models that helped me to learn.\n</p>\n<h4 class=\"post_section_title text-2xl text-black\">References</h4>\n<ul class=\"post_list\">\n\t<li>\n\t\tJournal of the American Statistical AssociationGeorge\n\t\t(E. P. Box)\n\t</li>\n\t<li>\n\t\tBuild, Compute, Critique, Repeat: Data Analysis with Latent Variable Models\n\t\t(David M. Blei)\n\t</li>\n\t<li>\n\t\tProbabilistic graphical models: principles and techniques\n\t\t(Koller, Daphne, and Nir Friedman)\n\t</li>\n\t<li>\n\t\tModel-based Machine Learning\n\t\t(Christopher M. Bishop)\n\t</li>\n\t<li>\n\t\tMachine Learning. A probabilistic perspective\n\t\t(Kevin P. Murphy)\n\t</li>\n\t<li>\n\t\tAn overview of gradient descent optimization algorithms\n\t\t(Sebastian Rudes)\n\t</li>\n\t<li>\n\t\tVariational Bayesian inference (slides)\n\t\t(Kay H. Brodersen)\n\t</li>\n\t<li>\n\t\tVariational Inference\n\t\t(David M. Blei)\n\t</li>\n\t<li>\n\t\tSthocastic Variational Inference\n\t\t(Matthew D. Hoffman, David M. Blei, Chong Wang and John Paisley)\n\t</li>\n\t<li>\n\t\tBlack Box Variational Inference\n\t\t(Rajesh Ranganath, Sean Gerrish and David M. Blei)\n\t</li>\n\t<li>\n\t\tThe Stan Math Library: Reverse-Mode Automatic Differentiation in C++\n\t\t(Bob Carpenter, Matthew D. Hoffman, Marcus Brubaker, Daniel Lee, Peter Li and Michael Betancourt)\n\t</li>\n\t<li>\n\t\tAutomatic Differentiation Variational Inference\n\t\t(Alp Kucukelbir, Dustin Tran, Rajesh Ranganath, Andrew Gelman and David M. Blei)\n\t</li>\n</ul>\n\t\t"}