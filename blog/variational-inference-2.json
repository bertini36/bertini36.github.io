{"title":"Variational Inference II ⛰️","slug":"variational-inference-2","tags":["#probabilistic-machine-learning","#variational-inference","#statistics"],"date":"2017-02-25","html":"\n<p class=\"post_p\">\n    In this post I'm going to continue explaining concepts that I introduced in my\n\t<a href=\"https://albertopou.dev/blog/variational-inference-1\" class=\"post_link\" target=\"_blank\">previous post</a> about\n\tVariational Inference (VI). At first I'm going to show that to find the best posterior approach it's necessary\n\tto minimize the Kullback-Leibler divergence (KL) between the variational model q(θ|λ) and the probabilistic\n\tmodel p(θ|x). Starting from Bayes' rule we have:\n</p>\n<figure class=\"post_figure flex justify-center\">\n\t<img class=\"post_image\"\n\t\t src=\"variational-inference-2/ELBO_derivation.png\"\n\t\t alt=\"ELBO derivation\">\n</figure>\n<p class=\"post_p\">\n\tThis expression gives us a more affordable way to calculate the evidence of the model where each factor is:\n</p>\n<figure class=\"post_figure flex justify-center\">\n\t<img class=\"post_image\"\n\t\t src=\"variational-inference-2/ELBO_explanation.png\"\n\t\t alt=\"ELBO explantation\">\n</figure>\n<p class=\"post_p\">\n\tWith this demonstration we get that minimize KL[q(θ|λ)||p(θ|x)] is equivalent to maximize ELBO(q(θ|λ),p(x,θ))\n\twhich is easier to evaluate. An intractable integral has been transformed to an expectation of a known distribution.\n</p>\n<figure class=\"post_figure flex justify-center\">\n\t<img class=\"post_image\"\n\t\t src=\"variational-inference-2/kullback_leibler.png\"\n\t\t alt=\"ELBO\">\n</figure>\n<figcaption class=\"post_image_figcaption\">\n\tMaximize ELBO is equivalent to minimize the distance between variational model and probabilistic model.\n</figcaption>\n<p class=\"post_p\">\n\tVariational inference uses ELBO as algorithm stop condition. When some iterations, where ELBO value does not\n\tincrease, are executed it means that good values for variational model λ params have been found. These are values of\n\tλ which get closer probabilistic model (the posterior). We can rewrite ELBO as:\n</p>\n<figure class=\"post_figure flex justify-center\">\n\t<img class=\"post_image\"\n\t\t src=\"variational-inference-2/ELBO_reescritura.png\"\n\t\t alt=\"Rewrited ELBO\">\n</figure>\n<p class=\"post_p\">\n\tWhere each factor is:\n</p>\n<figure class=\"post_figure flex justify-center\">\n\t<img class=\"post_image\"\n\t\t src=\"variational-inference-2/ELBO_terminos.png\"\n\t\t alt=\"ELBO factors\">\n</figure>\n<p class=\"post_p\">\n\tAnd now, if we take into account the Mean-Field assumption commented in the\n\t<a href=\"https://albertopou.dev/blog/variational-inference-1\" class=\"post_link\" target=\"_blank\">previous post</a>,\n\tELBO ends as follows:\n</p>\n<figure class=\"post_figure flex justify-center\">\n\t<img class=\"post_image\" style=\"width: 90%\"\n\t\t src=\"variational-inference-2/ELBO_MeanField.png\"\n\t\t alt=\"ELBO taking into account Mean Field assumption\">\n</figure>\n<h4>Variational Inference algorithm</h4>\n<figure class=\"post_figure flex justify-center\">\n\t<img class=\"post_image\"\n\t\t src=\"variational-inference-2/VI.png\"\n\t\t alt=\"Variational Inference algorithm\">\n</figure>\n<p class=\"post_p\">\n\tThis algorithm represents the basic idea of VI. In practice more things must be taken into account for its correct\n\toperation. First, variational model could be formed by local and global variables and the updates of these have to be\n\tdone in a concrete way. You also have to choose an inference method: Coordinate Ascent, Gradient Ascent, Sthocastic\n\tGradient Descent, ... Chosen inference method doesn't change the basic structure of the VI algorithm just change\n\tthe method of obtaining the λ new values of variational model in each iteration.\n</p>\n<h5>Coordinate Ascent Variational Inference</h5>\n<p class=\"post_p\">\n\tThe most traditional method for the inference of probabilistic models is: Coordinate Ascent Variational Inference\n\t(CAVI). For the implementation of this kind of inference you require knowledge of Bayesian statistics because\n\tfor the update of each variational model λ parameter and ELBO some analytical closed formulas have to be derived.\n\tAs previously mentioned when the model is completely conjugated (as Dirichlet-Categorical model or Normal Inverse\n\tWishart-Normal model) posterior can be analytically calculated, that is, without the need to approximate it.\n\tHowever if we have a bast amount of data this analytical calculus is impracticable due to the operations with\n\tvery large matrices in memory. For this reason, in this case and the case of no conjugated models it is a good\n\toption to approximate the posterior using VI.\n</p>\n<p class=\"post_p\">\n\tThe derivation of the analytical updates for the variational model parameters can be done in two ways: generic\n\tderivation or using the properties of the\n\t<a href=\"https://en.wikipedia.org/wiki/Exponential_family\" class=\"post_link\" target=\"_blank\">Exponential Family</a>.\n</p>\n<p class=\"post_p\">\n\tGeneric derivarion is based on the following formula (assuming the Mean-Field assumption):\n</p>\n<figure class=\"post_figure flex justify-center\">\n\t<img class=\"post_image\"\n\t\t src=\"variational-inference-2/generic_derivation.png\"\n\t\t alt=\"Generic derivation\">\n</figure>\n<p class=\"post_p\">\n\tThis derivation has to be done for each variable of the variational model θ<small>i</small> and after that,\n\ta statistician could deduce the distribution type of the variational parameter and how to update it.\n</p>\n<p class=\"post_p\">\n\tAnother way to obtain the variational parameters updates is to derive them using the properties of the Exponential\n\tFamily. To this family belong all the distributions that can be written in the form:\n</p>\n<figure class=\"post_figure flex justify-center\">\n\t<img class=\"post_image\"\n\t\t src=\"variational-inference-2/exponential_family.png\"\n\t\t alt=\"Exponential family\">\n</figure>\n<ul class=\"post_list\">\n\t<li>h(x): Base measure.</li>\n\t<li>η(θ): Natural parameters(it just depends on the parameters).</li>\n\t<li>t(x): Sufficient statistics(it just depends on the data). Lets know the shape of the distribution.\n\t\tDescribe the possible space for the distribution parameters.</li>\n\t<li>a(η(θ)): Cumulant. It is a normalizer.</li>\n</ul>\n<p class=\"post_p\">\n\tThis family allows to establish conjugation relations between distributions. When creating the joint\n\tdistribution based on two distributions the natural parameters of one allow some simplification in the\n\tformulation together with the sufficient statistics of the other one we say that the first distribution is\n\tconjugated of the second. The conjugated models, as already mentioned, thanks to these simplifications,\n\tallow to calculate the posterior analytically.\n</p>\n<h5>Coordinate Ascent Variational Inference algorithm</h5>\n<figure class=\"post_figure flex justify-center\">\n\t<img class=\"post_image\"\n\t\t src=\"variational-inference-2/CAVI.png\"\n\t\t alt=\"Coordinate Ascent Variational Inference Algorithm\">\n</figure>\n<p class=\"post_p\">\n\tIn this version of CAVI algorithm the distinction between updating local and global variables of the model has\n\talready been taken into accoun.\n</p>\n<h5>Gradient Ascent Variational Inference</h5>\n<p class=\"post_p\">\n\tA more naive inference alternative is Gradient Ascent Variational Inference (GAVI). The difference is how is\n\tthe variational parameters update. It is not done analytically with derived formulas by a statistician,\n\tit is an exploratory process. GAVI is based on the Gradient Descent/Ascent algorithm.\n</p>\n<h5>Gradient Ascent</h5>\n<p class=\"post_p\">\n\tGradient Ascent aims to maximize a cost function C(λ) parameterized by the model parameters, λ. The algorithm\n\toptimizes these parameters λ in the gradient direction (in the case of Gradient Descent, in the opposite\n\tdirection of the gradient) of the objective function ∇<small>λ</small>C(λ).\n\tIn our case we need Gradient Ascent because we want to maximize ELBO function. Learning rate η>0  determines the\n\tsize of the step in the direction of the local maximum. Gradient Ascent explores latent variables space of the\n\tmodel and moves in the direction of maximum slope (which is indicated by the gradient of the function) until find\n\ta local maximum. Variational model parameters are updated as follows:\n</p>\n<figure class=\"post_figure flex justify-center\">\n\t<img class=\"post_image\"\n\t\t src=\"variational-inference-2/gradient.png\"\n\t\t alt=\"Gradient application\">\n</figure>\n<p class=\"post_p\">\n\tOver the last years optimizations of this algorithm have been appearing: Momentum, Adagrad, Adadelta, RMSprop,\n\tAdam, ... the improvements they offer are based on aspects such as each parameter λ<small>i</small> has its\n    own learning rate η<small>i</small> or taking into account the value of previous iterations gradients to calculate\n\tthe following.\n</p>\n<h5>Gradient Ascent Variational Inference algorithm</h5>\n<figure class=\"post_figure flex justify-center\">\n\t<img class=\"post_image\"\n\t\t src=\"variational-inference-2/GAVI.png\"\n\t\t alt=\"Gradient Ascent Variational Inference algorithm\">\n</figure>\n<p class=\"post_p\">\n\tA problem of this algorithm to approximate the posterior (which causes more inaccurate convergences) is the use of\n\tthe gradient to optimize variational parameters. The gradient supposes that latent variables space is an\n\tEuclidean space. This fact implies the assumption that distance between the distributions is mesured by the\n\tEuclidean distance of their parameters. The solution to this problem, to find the real distance between two\n\tdistributions, is to use the natural gradient.\n</p>\n<figure class=\"post_figure flex justify-center\">\n\t<img class=\"post_image\"\n\t\t src=\"variational-inference-2/natural_gradient.png\"\n\t\t alt=\"Natural gradient definition\">\n</figure>\n<p class=\"post_p\">\n\tNatural gradient indicates the direction of maximum slope in other space, the Riemman space, where the real\n\tdistance between distributions is taken into account. This distance can be calculated premultiplying the normal\n\tgradient by the inverse of the matrix Fisher's, G(λ).\n</p>\n<figure class=\"post_figure flex justify-center\">\n\t<img class=\"post_image\"\n\t\t src=\"variational-inference-2/natural_gradient2.png\"\n\t\t alt=\"Natural gradient definition\">\n</figure>\n<p class=\"post_p\">\n\tIn the case of CAVI, when the analytical updates of each variational parameter are derived, the shape of the\n\tdistributions to measure the distance between them is already taking into account.\n</p>\n<h5>Efficiency problems</h5>\n<p class=\"post_p\">\n\tNowadays, in the already known as the information age, algorithms used in machine learning use huge volumes of\n\tdata. This causes programmers to scale algorithms or design alternatives less computationally expensive. CAVI and\n\tGAVI have to pass through all the data for each iteration. This procedure for massive datasets is intractable. In\n\tthe next post I'm going to explain the measures you can take in this cases and how to solve the scalability problem.\n</p>\n<h4>References</h4>\n<ul class=\"post_list\">\n\t<li>\n\t\tJournal of the American Statistical AssociationGeorge\n\t\t(E. P. Box)\n\t</li>\n\t<li>\n\t\tBuild, Compute, Critique, Repeat: Data Analysis with Latent Variable Models\n\t\t(David M. Blei)\n\t</li>\n\t<li>\n\t\tProbabilistic graphical models: principles and techniques\n\t\t(Koller, Daphne, and Nir Friedman)\n\t</li>\n\t<li>\n\t\tModel-based Machine Learning\n\t\t(Christopher M. Bishop)\n\t</li>\n\t<li>\n\t\tMachine Learning. A probabilistic perspective\n\t\t(Kevin P. Murphy)\n\t</li>\n\t<li>\n\t\tAn overview of gradient descent optimization algorithms\n\t\t(Sebastian Rudes)\n\t</li>\n\t<li>\n\t\tVariational Bayesian inference (slides)\n\t\t(Kay H. Brodersen)\n\t</li>\n\t<li>\n\t\tVariational Inference\n\t\t(David M. Blei)\n\t</li>\n</ul>\n\t\t"}