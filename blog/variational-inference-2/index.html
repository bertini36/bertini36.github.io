<!doctype html> <html lang=en> <head> <meta charset=utf-8> <meta content="width=device-width,initial-scale=1" name=viewport> <meta content=#333333 name=theme-color> <base href=/ > <link href=https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css rel=stylesheet> <link href=global.css rel=stylesheet> <link href=manifest.json rel=manifest crossorigin=use-credentials> <link href=black_logo.png rel=icon type=image/png> <link href=client/main.419280361.css rel=stylesheet> <noscript id=sapper-head-start></noscript><title>Variational Inference II ðŸ¤–ðŸ¤–</title><noscript id=sapper-head-end></noscript> </head> <body> <div id=sapper> <nav class="svelte-71l2nh border-b-2 font-light px-4 py-0"><ul class="svelte-71l2nh m-0 p-0"><li class=float-left><a class="block hover:text-red-500 no-underline px-3 py-4 svelte-71l2nh" href=.>Home</a></li> <li class=float-left><a class="block hover:text-red-500 no-underline px-3 py-4 svelte-71l2nh" href=blog aria-current=page rel=prefetch>Blog</a></li> <li class=float-right><a class="text-black block hover:text-red-500 no-underline px-3 py-4" href=https://github.com/bertini36/blogv2 target=_blank><i class="fa fa-github"></i></a></ul></nav> <main class=svelte-1uhnsl8> <h1 class=text-4xl>Variational Inference II ðŸ¤–ðŸ¤–</h1> <div class="mb-10 content svelte-1lhkq7r"> <p class=post_p> In this post I'm going to continue explaining concepts that I introduced in my <a class=post_link href=https://albertopou.dev/blog/variational-inference-1 target=_blank>previous post</a> about Variational Inference (VI). At first I'm going to show that to find the best posterior approach it's necessary to minimize the Kullback-Leibler divergence (KL) between the variational model q(Î¸|Î») and the probabilistic model p(Î¸|x). Starting from Bayes' rule we have: </p> <figure class="flex justify-center post_figure"> <img alt="ELBO derivation" class=post_image src=variational-inference-2/ELBO_derivation.png> </figure> <p class=post_p> This expression gives us a more affordable way to calculate the evidence of the model where each factor is: </p> <figure class="flex justify-center post_figure"> <img alt="ELBO explantation" class=post_image src=variational-inference-2/ELBO_explanation.png> </figure> <p class=post_p> With this demonstration we get that minimize KL[q(Î¸|Î»)||p(Î¸|x)] is equivalent to maximize ELBO(q(Î¸|Î»),p(x,Î¸)) which is easier to evaluate. An intractable integral has been transformed to an expectation of a known distribution. </p> <figure class="flex justify-center post_figure"> <img alt=ELBO class=post_image src=variational-inference-2/kullback_leibler.png> </figure> <figcaption class=post_image_figcaption> Maximize ELBO is equivalent to minimize the distance between variational model and probabilistic model. </figcaption> <p class=post_p> Variational inference uses ELBO as algorithm stop condition. When some iterations, where ELBO value does not increase, are executed it means that good values for variational model Î» params have been found. These are values of Î» which get closer probabilistic model (the posterior). We can rewrite ELBO as: </p> <figure class="flex justify-center post_figure"> <img alt="Rewrited ELBO" class=post_image src=variational-inference-2/ELBO_reescritura.png> </figure> <p class=post_p> Where each factor is: </p> <figure class="flex justify-center post_figure"> <img alt="ELBO factors" class=post_image src=variational-inference-2/ELBO_terminos.png> </figure> <p class=post_p> And now, if we take into account the Mean-Field assumption commented in the <a class=post_link href=https://albertopou.dev/blog/variational-inference-1 target=_blank>previous post</a>, ELBO ends as follows: </p> <figure class="flex justify-center post_figure"> <img alt="ELBO taking into account Mean Field assumption" class=post_image src=variational-inference-2/ELBO_MeanField.png style=width:90%> </figure> <h4 class="text-black post_section_title text-2xl">Variational Inference algorithm</h4> <figure class="flex justify-center post_figure"> <img alt="Variational Inference algorithm" class=post_image src=variational-inference-2/VI.png> </figure> <p class=post_p> This algorithm represents the basic idea of VI. In practice more things must be taken into account for its correct operation. First, variational model could be formed by local and global variables and the updates of these have to be done in a concrete way. You also have to choose an inference method: Coordinate Ascent, Gradient Ascent, Sthocastic Gradient Descent, ... Chosen inference method doesn't change the basic structure of the VI algorithm just change the method of obtaining the Î» new values of variational model in each iteration. </p> <h5 class="text-black post_section_title text-xl">Coordinate Ascent Variational Inference</h5> <p class=post_p> The most traditional method for the inference of probabilistic models is: Coordinate Ascent Variational Inference (CAVI). For the implementation of this kind of inference you require knowledge of Bayesian statistics because for the update of each variational model Î» parameter and ELBO some analytical closed formulas have to be derived. As previously mentioned when the model is completely conjugated (as Dirichlet-Categorical model or Normal Inverse Wishart-Normal model) posterior can be analytically calculated, that is, without the need to approximate it. However if we have a bast amount of data this analytical calculus is impracticable due to the operations with very large matrices in memory. For this reason, in this case and the case of no conjugated models it is a good option to approximate the posterior using VI. </p> <p class=post_p> The derivation of the analytical updates for the variational model parameters can be done in two ways: generic derivation or using the properties of the <a class=post_link href=https://en.wikipedia.org/wiki/Exponential_family target=_blank>Exponential Family</a>. </p> <p class=post_p> Generic derivarion is based on the following formula (assuming the Mean-Field assumption): </p> <figure class="flex justify-center post_figure"> <img alt="Generic derivation" class=post_image src=variational-inference-2/generic_derivation.png> </figure> <p class=post_p> This derivation has to be done for each variable of the variational model Î¸<small>i</small> and after that, a statistician could deduce the distribution type of the variational parameter and how to update it. </p> <p class=post_p> Another way to obtain the variational parameters updates is to derive them using the properties of the Exponential Family. To this family belong all the distributions that can be written in the form: </p> <figure class="flex justify-center post_figure"> <img alt="Exponential family" class=post_image src=variational-inference-2/exponential_family.png> </figure> <ul class=post_list> <li>h(x): Base measure.</li> <li>Î·(Î¸): Natural parameters(it just depends on the parameters).</li> <li>t(x): Sufficient statistics(it just depends on the data). Lets know the shape of the distribution. Describe the possible space for the distribution parameters.</li> <li>a(Î·(Î¸)): Cumulant. It is a normalizer.</li> </ul> <p class=post_p> This family allows to establish conjugation relations between distributions. When creating the joint distribution based on two distributions the natural parameters of one allow some simplification in the formulation together with the sufficient statistics of the other one we say that the first distribution is conjugated of the second. The conjugated models, as already mentioned, thanks to these simplifications, allow to calculate the posterior analytically. </p> <h5 class="text-black post_section_title text-xl">Coordinate Ascent Variational Inference algorithm</h5> <figure class="flex justify-center post_figure"> <img alt="Coordinate Ascent Variational Inference Algorithm" class=post_image src=variational-inference-2/CAVI.png> </figure> <p class=post_p> In this version of CAVI algorithm the distinction between updating local and global variables of the model has already been taken into accoun. </p> <h5 class="text-black post_section_title text-xl">Gradient Ascent Variational Inference</h5> <p class=post_p> A more naive inference alternative is Gradient Ascent Variational Inference (GAVI). The difference is how is the variational parameters update. It is not done analytically with derived formulas by a statistician, it is an exploratory process. GAVI is based on the Gradient Descent/Ascent algorithm. </p> <h5 class="text-black post_section_title text-xl">Gradient Ascent</h5> <p class=post_p> Gradient Ascent aims to maximize a cost function C(Î») parameterized by the model parameters, Î». The algorithm optimizes these parameters Î» in the gradient direction (in the case of Gradient Descent, in the opposite direction of the gradient) of the objective function âˆ‡<small>Î»</small>C(Î»). In our case we need Gradient Ascent because we want to maximize ELBO function. Learning rate Î·>0 determines the size of the step in the direction of the local maximum. Gradient Ascent explores latent variables space of the model and moves in the direction of maximum slope (which is indicated by the gradient of the function) until find a local maximum. Variational model parameters are updated as follows: </p> <figure class="flex justify-center post_figure"> <img alt="Gradient application" class=post_image src=variational-inference-2/gradient.png> </figure> <p class=post_p> Over the last years optimizations of this algorithm have been appearing: Momentum, Adagrad, Adadelta, RMSprop, Adam, ... the improvements they offer are based on aspects such as each parameter Î»<small>i</small> has its Â Â Â Â own learning rate Î·<small>i</small> or taking into account the value of previous iterations gradients to calculate the following. </p> <h5 class="text-black post_section_title text-xl">Gradient Ascent Variational Inference algorithm</h5> <figure class="flex justify-center post_figure"> <img alt="Gradient Ascent Variational Inference algorithm" class=post_image src=variational-inference-2/GAVI.png> </figure> <p class=post_p> A problem of this algorithm to approximate the posterior (which causes more inaccurate convergences) is the use of the gradient to optimize variational parameters. The gradient supposes that latent variables space is an Euclidean space. This fact implies the assumption that distance between the distributions is mesured by the Euclidean distance of their parameters. The solution to this problem, to find the real distance between two distributions, is to use the natural gradient. </p> <figure class="flex justify-center post_figure"> <img alt="Natural gradient definition" class=post_image src=variational-inference-2/natural_gradient.png> </figure> <p class=post_p> Natural gradient indicates the direction of maximum slope in other space, the Riemman space, where the real distance between distributions is taken into account. This distance can be calculated premultiplying the normal gradient by the inverse of the matrix Fisher's, G(Î»). </p> <figure class="flex justify-center post_figure"> <img alt="Natural gradient definition" class=post_image src=variational-inference-2/natural_gradient2.png> </figure> <p class=post_p> In the case of CAVI, when the analytical updates of each variational parameter are derived, the shape of the distributions to measure the distance between them is already taking into account. </p> <h5 class="text-black post_section_title text-xl">Efficiency problems</h5> <p class=post_p> Nowadays, in the already known as the information age, algorithms used in machine learning use huge volumes of data. This causes programmers to scale algorithms or design alternatives less computationally expensive. CAVI and GAVI have to pass through all the data for each iteration. This procedure for massive datasets is intractable. In the next post I'm going to explain the measures you can take in this cases and how to solve the scalability problem. </p> <h4 class="text-black post_section_title text-2xl">References</h4> <ul class=post_list> <li> Journal of the American Statistical AssociationGeorge (E. P. Box) </li> <li> Build, Compute, Critique, Repeat: Data Analysis with Latent Variable Models (David M. Blei) </li> <li> Probabilistic graphical models: principles and techniques (Koller, Daphne, and Nir Friedman) </li> <li> Model-based Machine Learning (Christopher M. Bishop) </li> <li> Machine Learning. A probabilistic perspective (Kevin P. Murphy) </li> <li> An overview of gradient descent optimization algorithms (Sebastian Rudes) </li> <li> Variational Bayesian inference (slides) (Kay H. Brodersen) </li> <li> Variational Inference (David M. Blei) </li> </ul> <div class="border-red-400 border-t-2 my-10"></div> <div><h4 class="text-black text-2xl mb-10 text-center">Write a comment! ðŸ˜€</h4> <figure class="flex justify-center"><img alt=Loader class=w-48 src=loader.gif></figure> <form class="w-full mt-16" onsubmit="return false"><div class="flex -mx-3 flex-wrap mb-6"><div class="w-full px-3 md:w-1/2 mb-6 md:mb-0"><label class="block text-gray-700 font-bold mb-2 text-xs tracking-wide uppercase" for=name>Name</label> <input class="block text-gray-700 appearance-none bg-gray-200 border border-gray-200 focus:bg-white focus:border-gray-500 focus:outline-none leading-tight px-4 py-3 rounded w-full"> <p class="text-xs text-red-600"></div> <div class="w-full px-3 md:w-1/2"><label class="block text-gray-700 font-bold mb-2 text-xs tracking-wide uppercase" for=email>Email</label> <input class="block text-gray-700 appearance-none bg-gray-200 border border-gray-200 focus:bg-white focus:border-gray-500 focus:outline-none leading-tight px-4 py-3 rounded w-full"> <p class="text-xs text-red-600"></div></div> <div class="flex -mx-3 flex-wrap mb-6"><div class="w-full px-3"><label class="block text-gray-700 font-bold mb-2 text-xs tracking-wide uppercase" for=comment>Comment</label> <textarea class="block text-gray-700 appearance-none bg-gray-200 border border-gray-200 focus:bg-white focus:border-gray-500 focus:outline-none leading-tight px-4 py-3 rounded w-full" rows=2></textarea> <p class="text-xs text-red-600"></div></div> <div class=w-full><button class="w-full px-4 rounded bg-red-500 border-b-4 border-red-700 font-bold hover:bg-red-400 hover:border-red-500 py-2 text-white">Send </button></div></form></div></div></main></div> <script>__SAPPER__={baseUrl:"",preloaded:[void 0,null,{post:{title:"Variational Inference II ðŸ¤–ðŸ¤–",slug:"variational-inference-2",tags:["#probabilistic-machine-learning","#variational-inference","#statistics"],date:"2017-02-25",html:"\n\u003Cp class=\"post_p\"\u003E\n    In this post I'm going to continue explaining concepts that I introduced in my\n\t\u003Ca href=\"https:\u002F\u002Falbertopou.dev\u002Fblog\u002Fvariational-inference-1\" class=\"post_link\" target=\"_blank\"\u003Eprevious post\u003C\u002Fa\u003E about\n\tVariational Inference (VI). At first I'm going to show that to find the best posterior approach it's necessary\n\tto minimize the Kullback-Leibler divergence (KL) between the variational model q(Î¸|Î») and the probabilistic\n\tmodel p(Î¸|x). Starting from Bayes' rule we have:\n\u003C\u002Fp\u003E\n\u003Cfigure class=\"post_figure flex justify-center\"\u003E\n\t\u003Cimg class=\"post_image\"\n\t\t src=\"variational-inference-2\u002FELBO_derivation.png\"\n\t\t alt=\"ELBO derivation\"\u003E\n\u003C\u002Ffigure\u003E\n\u003Cp class=\"post_p\"\u003E\n\tThis expression gives us a more affordable way to calculate the evidence of the model where each factor is:\n\u003C\u002Fp\u003E\n\u003Cfigure class=\"post_figure flex justify-center\"\u003E\n\t\u003Cimg class=\"post_image\"\n\t\t src=\"variational-inference-2\u002FELBO_explanation.png\"\n\t\t alt=\"ELBO explantation\"\u003E\n\u003C\u002Ffigure\u003E\n\u003Cp class=\"post_p\"\u003E\n\tWith this demonstration we get that minimize KL[q(Î¸|Î»)||p(Î¸|x)] is equivalent to maximize ELBO(q(Î¸|Î»),p(x,Î¸))\n\twhich is easier to evaluate. An intractable integral has been transformed to an expectation of a known distribution.\n\u003C\u002Fp\u003E\n\u003Cfigure class=\"post_figure flex justify-center\"\u003E\n\t\u003Cimg class=\"post_image\"\n\t\t src=\"variational-inference-2\u002Fkullback_leibler.png\"\n\t\t alt=\"ELBO\"\u003E\n\u003C\u002Ffigure\u003E\n\u003Cfigcaption class=\"post_image_figcaption\"\u003E\n\tMaximize ELBO is equivalent to minimize the distance between variational model and probabilistic model.\n\u003C\u002Ffigcaption\u003E\n\u003Cp class=\"post_p\"\u003E\n\tVariational inference uses ELBO as algorithm stop condition. When some iterations, where ELBO value does not\n\tincrease, are executed it means that good values for variational model Î» params have been found. These are values of\n\tÎ» which get closer probabilistic model (the posterior). We can rewrite ELBO as:\n\u003C\u002Fp\u003E\n\u003Cfigure class=\"post_figure flex justify-center\"\u003E\n\t\u003Cimg class=\"post_image\"\n\t\t src=\"variational-inference-2\u002FELBO_reescritura.png\"\n\t\t alt=\"Rewrited ELBO\"\u003E\n\u003C\u002Ffigure\u003E\n\u003Cp class=\"post_p\"\u003E\n\tWhere each factor is:\n\u003C\u002Fp\u003E\n\u003Cfigure class=\"post_figure flex justify-center\"\u003E\n\t\u003Cimg class=\"post_image\"\n\t\t src=\"variational-inference-2\u002FELBO_terminos.png\"\n\t\t alt=\"ELBO factors\"\u003E\n\u003C\u002Ffigure\u003E\n\u003Cp class=\"post_p\"\u003E\n\tAnd now, if we take into account the Mean-Field assumption commented in the\n\t\u003Ca href=\"https:\u002F\u002Falbertopou.dev\u002Fblog\u002Fvariational-inference-1\" class=\"post_link\" target=\"_blank\"\u003Eprevious post\u003C\u002Fa\u003E,\n\tELBO ends as follows:\n\u003C\u002Fp\u003E\n\u003Cfigure class=\"post_figure flex justify-center\"\u003E\n\t\u003Cimg class=\"post_image\" style=\"width: 90%\"\n\t\t src=\"variational-inference-2\u002FELBO_MeanField.png\"\n\t\t alt=\"ELBO taking into account Mean Field assumption\"\u003E\n\u003C\u002Ffigure\u003E\n\u003Ch4 class=\"post_section_title text-2xl text-black\"\u003EVariational Inference algorithm\u003C\u002Fh4\u003E\n\u003Cfigure class=\"post_figure flex justify-center\"\u003E\n\t\u003Cimg class=\"post_image\"\n\t\t src=\"variational-inference-2\u002FVI.png\"\n\t\t alt=\"Variational Inference algorithm\"\u003E\n\u003C\u002Ffigure\u003E\n\u003Cp class=\"post_p\"\u003E\n\tThis algorithm represents the basic idea of VI. In practice more things must be taken into account for its correct\n\toperation. First, variational model could be formed by local and global variables and the updates of these have to be\n\tdone in a concrete way. You also have to choose an inference method: Coordinate Ascent, Gradient Ascent, Sthocastic\n\tGradient Descent, ... Chosen inference method doesn't change the basic structure of the VI algorithm just change\n\tthe method of obtaining the Î» new values of variational model in each iteration.\n\u003C\u002Fp\u003E\n\u003Ch5 class=\"post_section_title text-xl text-black\"\u003ECoordinate Ascent Variational Inference\u003C\u002Fh5\u003E\n\u003Cp class=\"post_p\"\u003E\n\tThe most traditional method for the inference of probabilistic models is: Coordinate Ascent Variational Inference\n\t(CAVI). For the implementation of this kind of inference you require knowledge of Bayesian statistics because\n\tfor the update of each variational model Î» parameter and ELBO some analytical closed formulas have to be derived.\n\tAs previously mentioned when the model is completely conjugated (as Dirichlet-Categorical model or Normal Inverse\n\tWishart-Normal model) posterior can be analytically calculated, that is, without the need to approximate it.\n\tHowever if we have a bast amount of data this analytical calculus is impracticable due to the operations with\n\tvery large matrices in memory. For this reason, in this case and the case of no conjugated models it is a good\n\toption to approximate the posterior using VI.\n\u003C\u002Fp\u003E\n\u003Cp class=\"post_p\"\u003E\n\tThe derivation of the analytical updates for the variational model parameters can be done in two ways: generic\n\tderivation or using the properties of the\n\t\u003Ca href=\"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FExponential_family\" class=\"post_link\" target=\"_blank\"\u003EExponential Family\u003C\u002Fa\u003E.\n\u003C\u002Fp\u003E\n\u003Cp class=\"post_p\"\u003E\n\tGeneric derivarion is based on the following formula (assuming the Mean-Field assumption):\n\u003C\u002Fp\u003E\n\u003Cfigure class=\"post_figure flex justify-center\"\u003E\n\t\u003Cimg class=\"post_image\"\n\t\t src=\"variational-inference-2\u002Fgeneric_derivation.png\"\n\t\t alt=\"Generic derivation\"\u003E\n\u003C\u002Ffigure\u003E\n\u003Cp class=\"post_p\"\u003E\n\tThis derivation has to be done for each variable of the variational model Î¸\u003Csmall\u003Ei\u003C\u002Fsmall\u003E and after that,\n\ta statistician could deduce the distribution type of the variational parameter and how to update it.\n\u003C\u002Fp\u003E\n\u003Cp class=\"post_p\"\u003E\n\tAnother way to obtain the variational parameters updates is to derive them using the properties of the Exponential\n\tFamily. To this family belong all the distributions that can be written in the form:\n\u003C\u002Fp\u003E\n\u003Cfigure class=\"post_figure flex justify-center\"\u003E\n\t\u003Cimg class=\"post_image\"\n\t\t src=\"variational-inference-2\u002Fexponential_family.png\"\n\t\t alt=\"Exponential family\"\u003E\n\u003C\u002Ffigure\u003E\n\u003Cul class=\"post_list\"\u003E\n\t\u003Cli\u003Eh(x): Base measure.\u003C\u002Fli\u003E\n\t\u003Cli\u003EÎ·(Î¸): Natural parameters(it just depends on the parameters).\u003C\u002Fli\u003E\n\t\u003Cli\u003Et(x): Sufficient statistics(it just depends on the data). Lets know the shape of the distribution.\n\t\tDescribe the possible space for the distribution parameters.\u003C\u002Fli\u003E\n\t\u003Cli\u003Ea(Î·(Î¸)): Cumulant. It is a normalizer.\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Cp class=\"post_p\"\u003E\n\tThis family allows to establish conjugation relations between distributions. When creating the joint\n\tdistribution based on two distributions the natural parameters of one allow some simplification in the\n\tformulation together with the sufficient statistics of the other one we say that the first distribution is\n\tconjugated of the second. The conjugated models, as already mentioned, thanks to these simplifications,\n\tallow to calculate the posterior analytically.\n\u003C\u002Fp\u003E\n\u003Ch5 class=\"post_section_title text-xl text-black\"\u003ECoordinate Ascent Variational Inference algorithm\u003C\u002Fh5\u003E\n\u003Cfigure class=\"post_figure flex justify-center\"\u003E\n\t\u003Cimg class=\"post_image\"\n\t\t src=\"variational-inference-2\u002FCAVI.png\"\n\t\t alt=\"Coordinate Ascent Variational Inference Algorithm\"\u003E\n\u003C\u002Ffigure\u003E\n\u003Cp class=\"post_p\"\u003E\n\tIn this version of CAVI algorithm the distinction between updating local and global variables of the model has\n\talready been taken into accoun.\n\u003C\u002Fp\u003E\n\u003Ch5 class=\"post_section_title text-xl text-black\"\u003EGradient Ascent Variational Inference\u003C\u002Fh5\u003E\n\u003Cp class=\"post_p\"\u003E\n\tA more naive inference alternative is Gradient Ascent Variational Inference (GAVI). The difference is how is\n\tthe variational parameters update. It is not done analytically with derived formulas by a statistician,\n\tit is an exploratory process. GAVI is based on the Gradient Descent\u002FAscent algorithm.\n\u003C\u002Fp\u003E\n\u003Ch5 class=\"post_section_title text-xl text-black\"\u003EGradient Ascent\u003C\u002Fh5\u003E\n\u003Cp class=\"post_p\"\u003E\n\tGradient Ascent aims to maximize a cost function C(Î») parameterized by the model parameters, Î». The algorithm\n\toptimizes these parameters Î» in the gradient direction (in the case of Gradient Descent, in the opposite\n\tdirection of the gradient) of the objective function âˆ‡\u003Csmall\u003EÎ»\u003C\u002Fsmall\u003EC(Î»).\n\tIn our case we need Gradient Ascent because we want to maximize ELBO function. Learning rate Î·\u003E0  determines the\n\tsize of the step in the direction of the local maximum. Gradient Ascent explores latent variables space of the\n\tmodel and moves in the direction of maximum slope (which is indicated by the gradient of the function) until find\n\ta local maximum. Variational model parameters are updated as follows:\n\u003C\u002Fp\u003E\n\u003Cfigure class=\"post_figure flex justify-center\"\u003E\n\t\u003Cimg class=\"post_image\"\n\t\t src=\"variational-inference-2\u002Fgradient.png\"\n\t\t alt=\"Gradient application\"\u003E\n\u003C\u002Ffigure\u003E\n\u003Cp class=\"post_p\"\u003E\n\tOver the last years optimizations of this algorithm have been appearing: Momentum, Adagrad, Adadelta, RMSprop,\n\tAdam, ... the improvements they offer are based on aspects such as each parameter Î»\u003Csmall\u003Ei\u003C\u002Fsmall\u003E has its\nÂ Â Â Â own learning rate Î·\u003Csmall\u003Ei\u003C\u002Fsmall\u003E or taking into account the value of previous iterations gradients to calculate\n\tthe following.\n\u003C\u002Fp\u003E\n\u003Ch5 class=\"post_section_title text-xl text-black\"\u003EGradient Ascent Variational Inference algorithm\u003C\u002Fh5\u003E\n\u003Cfigure class=\"post_figure flex justify-center\"\u003E\n\t\u003Cimg class=\"post_image\"\n\t\t src=\"variational-inference-2\u002FGAVI.png\"\n\t\t alt=\"Gradient Ascent Variational Inference algorithm\"\u003E\n\u003C\u002Ffigure\u003E\n\u003Cp class=\"post_p\"\u003E\n\tA problem of this algorithm to approximate the posterior (which causes more inaccurate convergences) is the use of\n\tthe gradient to optimize variational parameters. The gradient supposes that latent variables space is an\n\tEuclidean space. This fact implies the assumption that distance between the distributions is mesured by the\n\tEuclidean distance of their parameters. The solution to this problem, to find the real distance between two\n\tdistributions, is to use the natural gradient.\n\u003C\u002Fp\u003E\n\u003Cfigure class=\"post_figure flex justify-center\"\u003E\n\t\u003Cimg class=\"post_image\"\n\t\t src=\"variational-inference-2\u002Fnatural_gradient.png\"\n\t\t alt=\"Natural gradient definition\"\u003E\n\u003C\u002Ffigure\u003E\n\u003Cp class=\"post_p\"\u003E\n\tNatural gradient indicates the direction of maximum slope in other space, the Riemman space, where the real\n\tdistance between distributions is taken into account. This distance can be calculated premultiplying the normal\n\tgradient by the inverse of the matrix Fisher's, G(Î»).\n\u003C\u002Fp\u003E\n\u003Cfigure class=\"post_figure flex justify-center\"\u003E\n\t\u003Cimg class=\"post_image\"\n\t\t src=\"variational-inference-2\u002Fnatural_gradient2.png\"\n\t\t alt=\"Natural gradient definition\"\u003E\n\u003C\u002Ffigure\u003E\n\u003Cp class=\"post_p\"\u003E\n\tIn the case of CAVI, when the analytical updates of each variational parameter are derived, the shape of the\n\tdistributions to measure the distance between them is already taking into account.\n\u003C\u002Fp\u003E\n\u003Ch5 class=\"post_section_title text-xl text-black\"\u003EEfficiency problems\u003C\u002Fh5\u003E\n\u003Cp class=\"post_p\"\u003E\n\tNowadays, in the already known as the information age, algorithms used in machine learning use huge volumes of\n\tdata. This causes programmers to scale algorithms or design alternatives less computationally expensive. CAVI and\n\tGAVI have to pass through all the data for each iteration. This procedure for massive datasets is intractable. In\n\tthe next post I'm going to explain the measures you can take in this cases and how to solve the scalability problem.\n\u003C\u002Fp\u003E\n\u003Ch4 class=\"post_section_title text-2xl text-black\"\u003EReferences\u003C\u002Fh4\u003E\n\u003Cul class=\"post_list\"\u003E\n\t\u003Cli\u003E\n\t\tJournal of the American Statistical AssociationGeorge\n\t\t(E. P. Box)\n\t\u003C\u002Fli\u003E\n\t\u003Cli\u003E\n\t\tBuild, Compute, Critique, Repeat: Data Analysis with Latent Variable Models\n\t\t(David M. Blei)\n\t\u003C\u002Fli\u003E\n\t\u003Cli\u003E\n\t\tProbabilistic graphical models: principles and techniques\n\t\t(Koller, Daphne, and Nir Friedman)\n\t\u003C\u002Fli\u003E\n\t\u003Cli\u003E\n\t\tModel-based Machine Learning\n\t\t(Christopher M. Bishop)\n\t\u003C\u002Fli\u003E\n\t\u003Cli\u003E\n\t\tMachine Learning. A probabilistic perspective\n\t\t(Kevin P. Murphy)\n\t\u003C\u002Fli\u003E\n\t\u003Cli\u003E\n\t\tAn overview of gradient descent optimization algorithms\n\t\t(Sebastian Rudes)\n\t\u003C\u002Fli\u003E\n\t\u003Cli\u003E\n\t\tVariational Bayesian inference (slides)\n\t\t(Kay H. Brodersen)\n\t\u003C\u002Fli\u003E\n\t\u003Cli\u003E\n\t\tVariational Inference\n\t\t(David M. Blei)\n\t\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\t\t"}}]};if('serviceWorker' in navigator)navigator.serviceWorker.register('/service-worker.js');(function(){try{eval("async function x(){}");var main="/client/client.681a387a.js"}catch(e){main="/client/legacy/client.41044118.js"};var s=document.createElement("script");try{new Function("if(0)import('')")();s.src=main;s.type="module";s.crossOrigin="use-credentials";}catch(e){s.src="/client/shimport@1.0.1.js";s.setAttribute("data-main",main);}document.head.appendChild(s);}());</script> 