{"title":"Distributed matrix product ðŸ”¢","slug":"distributed-matrix-product","tags":["#CUDA","#MPI","#C","#maths"],"date":"2017-02-02","html":"\n<p class=\"post_p\">\n\tNowadays one of the main problems of any algorithm is its facility to scale it, in other words, its facility to\n\tbe executed over multiple cores (parallelization) or nodes (distribution). The amount of data that is available\n\ttoday has led the world of computing to develop technologies with which to parallelize and distribute processes\n\tare done more easily and automatically. Actually, each computationally expensive algorithm works in its\n\tdistributed version. In this way tasks as finding prime numbers, complex simulations or statistical prediction\n\tmodels, which would spend years to get results, can obtain it in a few hours thanks to the joint work of many\n\tcomputers.\n</p>\n<p class=\"post_p\">\n\tPrecisely this is what supercomputing centers like <a href=\"https://www.bsc.es/\" class=\"post_link\" target=\"_blank\">BSC</a> are\n\tengaged. BSC has a supercomputer, called Marenostrum III, which avoids to its users to run computationally\n\texpensive algorithms or algorithms that require a huge amount of data in a distributed way. Specifically,\n\tMarenostrum III has 3108 nodes each with 2 processors of 8 cores (over 49.728 cores).\n</p>\n<figure class=\"post_figure flex justify-center\">\n\t<img src=\"distributed-matrix-product/marenostrum.jpg\"\n\t\t alt=\"Marenostrum III supercomputer\">\n</figure>\n<figcaption class=\"post_image_figcaption\">Marenostrum III</figcaption>\n<p class=\"post_p\">\n\tIn this post I'm going to show, with a simple example, how to distribute a matrix product over several nodes and\n\thow to parallelize at each node. Matrix product is a mathematical operation that, when it is taken to the extreme\n\tusing very large matrices it is become in a very computationally expensive operation.\n</p>\n<figure class=\"post_figure flex justify-center\">\n\t<img src=\"distributed-matrix-product/matrix_product.png\"\n\t\t alt=\"Matrix product\">\n</figure>\n<figcaption class=\"post_image_figcaption\">Matrix product</figcaption>\n<p class=\"post_p\">\n\tSince each element of the resulting matrix does not depend on any other element of it we can distribute without\n\tany restriction. For this example I could use 4 nodes, so I decided that each node would be responsible for\n\tcalculating a quarter of the resulting matrix. A and B matrices will be sent to each node to avoid them to\n\tdo its computations. For the distribution I'm using <a href=\"https://www.open-mpi.org/\" class=\"post_link\" target=\"_blank\">MPI</a>\n\t(Message Passing Interface) technology. Using functions of this library data can be shared over the nodes\n\t(lines 123 and 124) and to specify, using process id, which part of the result matrix each node has to calculate\n\t(lines 116, 117 and 118).\n</p>\n<figure class=\"post_figure flex justify-center\">\n\t<img src=\"distributed-matrix-product/distributed_matrix_product.png\"\n\t\t alt=\"Distributed matrix product\">\n</figure>\n<figcaption class=\"post_image_figcaption\">Matrix product distribution and parallelization</figcaption>\n<p class=\"post_p\">\n\tNowadays, for parallel computation, the power of GPUs is being used. In our case each node of Marenostrum III has a\n\tNvidia K80 graphic card. This graphic cards can be used for parallel computation using Nvidia\n\t<a href=\"https://en.wikipedia.org/wiki/CUDA\" class=\"post_link\" target=\"_blank\">CUDA</a> (Compute Unified Device Architecture)\n\tplatform. GPUs are essentially a big number of simple processors that can be used to speed up some parts of the\n\tcode. For this, the program has to be decomposed in a big number of threads which will be executed concurrently.\n\tIn this example, each thread will be responsible for calculating just an element of the resulting matrix. Using\n\tCUDA it is required to define a block structure (threads group) delimiting each thread data domain using its\n\tthreadId and its blockId. These threads will be executed in one kernel, in other words, a GPU program.\n</p>\n<div class=\"post_code\">\n\t<pre><code>/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */\n/* MULTI-NODE AND PARALLEL MATRIX-MATRIX PRODUCT WITH MPI AND CUDA           */\n/*                                                                           */\n/* File:         mmpmpicuda.cu                                               */\n/* Author:       Alberto Pou QuirÃ³s (Github: bertini36)                      */\n/* Description:  This program performs a matrix product (A * B = C)          */\n/*               distributing the computation between multiple nodes         */\n/*               with MPI technology and parallelizing the computation in    */\n/*               every node with Nvidia CUDA technology                      */\n/* Compilation:  nvcc -I/opt/mpi/bullxmpi/1.2.9.1/include                    */\n/*               -L/opt/mpi/bullxmpi/1.2.9.1/lib -lmpi -ldl -lm -lnuma       */\n/*               -lrt -lnsl -lutil -lm -ldl mmpmpicuda.cu -o mmpmpicuda      */\n/* Strategy:                                                                 */\n/*                  Example 16x16 matrices with 4 nodes:                     */\n/*                   _________________16________________                     */\n/*                   |                                 |                     */\n/*                   |               NODE 1            | 4                   */\n/*                   |_________________________________|                     */\n/*                   |                                 |                     */\n/*                   |               NODE 2            | 4                   */\n/*              C =  |_________________________________|    16               */\n/*                   |                                 |                     */\n/*                   |               NODE 3            | 4                   */\n/*                   |_________________________________|                     */\n/*                   |                                 |                     */\n/*                   |               NODE 4            | 4                   */\n/*                   |_________________________________|                     */\n/*                                                                           */\n/*                  Node 1 computes 4 rows of result matrix:                 */\n/*                   __________________________________                      */\n/*                   |                                 |                     */\n/*                   |         4x16 CUDA block         |                     */\n/*                   |_________________________________|                     */\n/*                                                                           */\n/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */\n\n#include &lt;sys/time.h&gt;\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;assert.h&gt;\n#include &lt;mpi.h&gt;\n\n#define N 1024\n\n#define err(format, ...) do { fprintf(stderr, format, ##__VA_ARGS__); exit(1); } while (0)\n\nstruct timeval start_time, end_time;\n\ninline void checkCuda(cudaError_t e) {\n    if (e != cudaSuccess) {\n        err(\"CUDA Error %d: %s\\n\", e, cudaGetErrorString(e));\n    }\n}\n\n__global__ void matrixProduct(double *matrix_a, double *matrix_b, double *matrix_c, int width, int nrows, int my_rank) {\n    int row = threadIdx.y + blockDim.y * blockIdx.y;\n    int col = threadIdx.x + blockDim.x * blockIdx.x;\n    matrix_c[row * width + col] = 0;\n    for (int k=0; k&lt;width; k++) {\n        matrix_c[row * width + col] += matrix_a[(row * width) + (my_rank * nrows * width) + k] * matrix_b[k * width + col];\n    }\n}\n\nvoid initializeMatrices(double matrix_a[N][N], double matrix_b[N][N]) {\n    int i, j;\n    srand(time(NULL));\n    for (i=0; i&lt;N; i++) {\n        for (j=0; j&lt;N; j++) {\n            matrix_a[i][j] = rand();\n            matrix_b[i][j] = rand();\n        }\n    }\n}\n\nvoid showMatrices(double matrix_a[N][N], double matrix_b[N][N], double matrix_c[N][N]) {\n    int i, j;\n    srand(time(NULL));\n    printf(\"***** MATRIX A ***** \\n\");\n    for (i=0; i&lt;N; i++) {\n        for (j=0; j&lt;N; j++) {\n            (j % N == N-1) ? printf(\"%.1f \\n\", matrix_a[i][j]) : printf(\"%.1f,\", matrix_a[i][j]);\n        }\n    }\n    printf(\"***** MATRIX B ***** \\n\");\n    for (i=0; i&lt;N; i++) {\n        for (j=0; j&lt;N; j++) {\n            (j % N == N-1) ? printf(\"%.1f \\n\", matrix_b[i][j]) : printf(\"%.1f,\", matrix_b[i][j]);\n        }\n    }\n    printf(\"***** RESULT MATRIX ***** \\n\");\n    for (int i=0; i&lt;N; i++) {\n        for (int j=0; j&lt;N; j++) {\n            (j % N == N-1) ? printf(\"%f \\n\", matrix_c[i][j]) : printf(\"%f,\", matrix_c[i][j]);\n        }\n    }\n}\n\nint main(int argc, char *argv[]) {\n\n    double A[N][N], B[N][N], C[N][N];\n    double *d_a, *d_b, *d_c;\n    int my_rank, comm_sz, from, to, nrows;\n\n    // MPI initialization\n    MPI_Init (&argc, &argv);\n    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);    // Process id\n    MPI_Comm_size(MPI_COMM_WORLD, &comm_sz);    // Number of processors\n\n    if (N % comm_sz != 0) {\n        if (my_rank == 0) printf(\"Matrix size not divisible by number of processors \\n\");\n        MPI_Finalize();\n        exit(-1);\n    }\n\n    // Calculate interval lines to compute per node\n    from = my_rank * N / comm_sz;\n    to = (my_rank + 1) * N / comm_sz;\n    nrows = to - from;\n\n    if (my_rank == 0) { initializeMatrices(A, B); }\n\n    // Send A y B to every node\n    MPI_Bcast(A, N*N, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    MPI_Bcast(B, N*N, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    // Allocate memory in the device\n    checkCuda(cudaMalloc((void **) &d_a, N*N*sizeof(double)));\n    checkCuda(cudaMalloc((void **) &d_b, N*N*sizeof(double)));\n    checkCuda(cudaMalloc((void **) &d_c, (N*N/comm_sz)*sizeof(double)));\n\n    // Copy the information in the device\n    checkCuda(cudaMemcpy(d_a, A, N*N*sizeof(double), cudaMemcpyHostToDevice));\n    checkCuda(cudaMemcpy(d_b, B, N*N*sizeof(double), cudaMemcpyHostToDevice));\n\n    // CUDA threads structure definition\n    dim3 dimGrid(1);\n    dim3 dimBlock(N, nrows);\n\n    MPI_Barrier(MPI_COMM_WORLD);\n    if (my_rank == 0) { gettimeofday(&start_time, NULL); }\n\n    // Kernel launch\n    matrixProduct&lt;&lt;&lt;dimGrid, dimBlock&gt;&gt;&gt;(d_a, d_b, d_c, N, nrows, my_rank);\n    checkCuda(cudaDeviceSynchronize());\n    checkCuda(cudaGetLastError());\n\n    // Calculate compute time\n    MPI_Barrier(MPI_COMM_WORLD);\n    if (my_rank == 0) {\n        gettimeofday(&end_time, NULL);\n        printf(\"Compute time: %.1f ms \\n\", (float) (end_time.tv_sec - start_time.tv_sec) * 1000 + (end_time.tv_usec - start_time.tv_usec) / 1000);\n     }\n\n    // Get results from device\n    checkCuda(cudaMemcpy(C[from], d_c, (nrows)*N*sizeof(double), cudaMemcpyDeviceToHost));\n\n    // Unify results from nodes\n    MPI_Gather(C[from], N*N/comm_sz, MPI_DOUBLE, C, N*N/comm_sz, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    // if (my_rank == 0)  { showMatrices(A, B, C); }\n\n    checkCuda(cudaFree(d_a));\n    checkCuda(cudaFree(d_b));\n    checkCuda(cudaFree(d_c));\n\n    MPI_Finalize();\n    return 0;\n\n}</code></pre>\n</div>\n<p class=\"post_p\">\n\tOne of the main problems of CUDA is that to get the most out of it you have to know the technical specifications\n\tof the GPU on which it will run the code. Block size (number of threads) or the use of the memory hierarchy\n\tof the graphic card are aspects that the coder has to take into account when programming with this technology.\n</p>\n<p class=\"post_p\">\n\tAt this <a href=\"https://github.com/bertini36/distributedMatrixProduct\" class=\"post_link\" target=\"_blank\">Github repository</a>\n\tyou will find two versions of matrix product, one just using MPI technology and another just with CUDA. Also,\n\tat config folder, there are a script to know graphic card specifications and a possible CUDA configuration\n\tfor that card.\n</p>\n\t\t"}