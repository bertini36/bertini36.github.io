<!doctype html> <html lang=en> <head> <meta charset=utf-8> <meta content="width=device-width,initial-scale=1" name=viewport> <meta content=#333333 name=theme-color> <base href=/ > <link href=https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css rel=stylesheet> <link href=global.css rel=stylesheet> <link href=manifest.json rel=manifest crossorigin=use-credentials> <link href=black_logo.png rel=icon type=image/png> <link href=client/main.1745966569.css rel=stylesheet> <noscript id=sapper-head-start></noscript><title>Variational Inference III 🤖🎯</title><noscript id=sapper-head-end></noscript> </head> <body> <div id=sapper> <nav class="svelte-71l2nh border-b-2 font-light px-4 py-0"><ul class="svelte-71l2nh m-0 p-0"><li class=float-left><a class="block hover:text-red-500 no-underline px-3 py-4 svelte-71l2nh" href=.>Home</a></li> <li class=float-left><a class="block hover:text-red-500 no-underline px-3 py-4 svelte-71l2nh" href=blog aria-current=page rel=prefetch>Blog</a></li> <li class=float-right><a class="text-black block hover:text-red-500 no-underline px-3 py-4" href=https://github.com/bertini36/blogv2 target=_blank><i class="fa fa-github"></i></a></ul></nav> <main class=svelte-1uhnsl8> <h1 class=text-4xl>Variational Inference III 🤖🎯</h1> <div class="mb-10 content svelte-1lhkq7r"> <p class=post_p> In this post I'm going to continue explaining concepts that I introduced in previous posts (<a class=post_link href=https://albertopou.dev/blog/variational-inference-1/ target=_blank>1</a> and <a class=post_link href=https://albertopou.dev/blog/variational-inference-2/ target=_blank>2</a>) about Variational inference (VI). Algorithms presented till now have scalability problems. For each iteration the algorithm requires to go through all the data and this, for massive volumes of data, is impracticable. An alternative to solve this problem is: Sthocastic Variational Inference. This version is based on using a batch (set of points) of data in each iteration. In this way, after more iterations than the conventional solutions, the solution will tend to a local optimum. The main advantage of this mechanism is that it doesn't require to keep all dataset in memory, solving the bottleneck that could be formed by using VI with very big datasets. </p> <h4 class="text-black post_section_title text-2xl">Sthocastic Variational Inference</h4> <h5 class="text-black post_section_title text-xl">Sthocastic optimization</h5> <p class=post_p> This technique obtains estimations of the real gradient of an objective function. Thus we obtain an algorithm which iterates each batch and adjusts the hidden structure of the model based only on that batch of data. Stochastic optimization finds a function relative maximum or minimum using real gradient estimations. Estimations expectation E(∇λC(λ,x)), being x a batch of data, is equivalent to the real gradient ∇λC(λ,X), being X the full dataset. </p> <figure class="flex justify-center post_figure"> <img alt="Sthocastic Optimization" class=post_image src=variational-inference-3/svi.png> </figure> <p class=post_p> Under ideal conditions these stochastic algorithms converge to a local optimum of the function if ρ meets the Robins-Monro conditions: </p> <figure class="flex justify-center post_figure"> <img alt=Robins-Monro class=post_image src=variational-inference-3/robins.png> </figure> <p class=post_p> The use of this technique results in the algorithm Stochastic Gradient Ascent Variational Inference (SGAVI) and Stochastic Natural Gradient Ascent Variational Inference (SNGAVI) if natural gradients are used to estimate. These algorthims, thanks to the real gradient estimations, can avoid saddle points. </p> <figure class="flex justify-center post_figure"> <img alt="Saddle point" class=post_image src=variational-inference-3/saddle-point.png> </figure> <figcaption class=post_image_figcaption> Saddle point. (Source: Wikipedia) </figcaption> <h5 class="text-black post_section_title text-xl">Algorithm</h5> <p class=post_p> In VI, the function to be optimized is called ELBO. If variational parameters are updated by closed analytical formulas, the algorithm is known as Sthocastic Coordinate Ascent Variational Inference (SCAVI) while if we use sthocastic optimization the algorithm is known as SGAVI. This version uses a corrector term using calculations from previous iterations. The generic algorithm would be as follows: </p> <figure class="flex justify-center post_figure"> <img alt="Sthocastic Variational Inference Algorithm" class=post_image src=variational-inference-3/svi-algorithm.png> </figure> <h4 class="text-black post_section_title text-2xl">Black Box Variational Inference</h4> <p class=post_p> Starting from the ELBO formula that was reached in the <a class=post_link href=https://albertopou.herokuapp.com/#variational_inference_2 target=_blank>previous post</a>: </p> <figure class="flex justify-center post_figure"> <img alt=ELBO class=post_image src=variational-inference-3/elbo2.png> </figure> <p class=post_p> The main idea of Black Box Variational Inference (BBVI) consists of sampling the variational model q(θ|λ) with the purpose of obtain an approach of formula expectations. Such expectations regarding the variational model can be computationally expensive and can be a bottleneck at computer memory level. </p> <h5 class="text-black post_section_title text-xl">Score Gradients</h5> <p class=post_p> Now we apply gradients and some algebraic transformations into the analytic ELBO formula (complete derivation is not shown): </p> <figure class="flex justify-center post_figure"> <img alt=ELBO class=post_image src=variational-inference-3/elbo3.png> </figure> <p class=post_p> After these transformations you could notice that it is not necesary to obtain gradients of complete ELBO, it is enough to derive the variational model (q(θ|λ)). </p> <h5 class="text-black post_section_title text-xl">Monte-Carlo Integration</h5> <p class=post_p> Monte-Carlo integration is a mechanism to approximate integrals. It consists of sampling the variable with respect to which we are integrating and do a summation of function values given those samples. The more samples are taken from the variable more exact will be the approximation. In our case we are going to use this mechanism to approximate ELBO integral, which integrates with respect to the q(θ|λ) distribution. Then a set of samples (θs) obtained from the q(θ|λ) distribution will allow us to get an approximation of the integral. The formula would be as follows: </p> <figure class="flex justify-center post_figure"> <img alt="Monte-Carlo Integration" class=post_image src=variational-inference-3/montecarlo.png> </figure> <h5 class="text-black post_section_title text-xl">Algorithm</h5> <p class=post_p> Below is the BBVI algorithm: </p> <figure class="flex justify-center post_figure"> <img alt="Black Box Variational Inference Algorithm" class=post_image src=variational-inference-3/bbvi.png> </figure> <h4 class="text-black post_section_title text-2xl">Considerations</h4> <p class=post_p> This algorithm is the result of get some measures that can question the convergence of VI algorithm. Starting by supposing q(θ|λ) factorizes (mean field assumption) to approximate ELBO integral using Monte-Carlo. All this causes that this algorithm is subjected to a high variance, and depending on the model, slow convergence. In order to reduce the variance that this method can cause some mechanisms have appeared: </p> <ul class=post_list> <li> <strong>Rao-Blackwellization</strong>. This method reduces the variance of an aleatory variable replacing it by its conditional expectation with respect to a subset of variables. </li> <li> <strong>Control variates</strong>. This method replaces Monte-Carlo expectation by another function with less variance. </li> </ul> <p class=post_p> A positive aspect of this method is that it is not necessary to derive the analytical formulas used to update the variational parameters either analytical ELBO. This permits the access to these algorithms to people who have less statistical knowledge (as me). It exists another approximation called Automatic Differentiation Variational Inference (ADVI). This method also has these advantages and improves the convergence of unconjugated models that can be a challenge for the rest of VI variants. </p> <p class=post_p> Formerly with VI only conjugated models could be inferred since the unconjugated models didn't be easily derivable. The discovery of algorithms like ADVI and BBVI allowed the inference of this kind of models because they changed the analytical calculus for an approximate strategy. </p> <h4 class="text-black post_section_title text-2xl">Variational inference libraries</h4> <p class=post_p> Best-known libraries for the use of VI and Markov Chain Monte-Carlo (MCMC): </p> <ul class=post_list> <li> <a class=post_link href=http://edwardlib.org/ target=_blank>Edward</a>: It uses Tensorflow for gradients computations and has BBVI, reparameterization BBVI and Metropolis-Hastings implementations. </li> <li> <a class=post_link href=http://mc-stan.org/ target=_blank>Stan</a>: It uses C++ Automatic Differentiation reverse mode for gradients computation and has ADVI and HMC implementations. </li> <li> <a class=post_link href=https://github.com/pymc-devs/pymc3 target=_blank>PyMC3</a>: It uses Theano for gradient computations and has ADVI, Gibbs sampling and Metropolis-Hastings implementations. </li> <li> <a class=post_link href=https://www.tensorflow.org/versions/r0.12/api_docs/python/contrib.bayesflow.variational_inference/ target=_blank>Bayesflow</a>: Young Google module for VI. </li> </ul> <p class=post_p> If you want to learn more about Variational Inference and its uses in probabilistic models inference you can take a look to my <a class=post_link href=https://github.com/bertini36/GMM/blob/master/docs/doc.pdf target=_blank> master thesis</a>. It focuses on the use of automatic differentiation tools to apply Variational Inference into a Gaussian Mixture Model (GMM). At the <a class=post_link href=https://github.com/bertini36/GMM target=_blank>repository</a> you can find implementations of Gaussian Mixture Model with different technologies like <a class=post_link href=https://www.tensorflow.org/ target=_blank>Tensorflow</a>, Python, <a class=post_link href=https://github.com/blei-lab/edward target=_blank>Edward</a>, <a class=post_link href=https://github.com/HIPS/autograd target=_blank>Autograd</a>, ... And also other probabilistic models that helped me to learn. </p> <h4 class="text-black post_section_title text-2xl">References</h4> <ul class=post_list> <li> Journal of the American Statistical AssociationGeorge (E. P. Box) </li> <li> Build, Compute, Critique, Repeat: Data Analysis with Latent Variable Models (David M. Blei) </li> <li> Probabilistic graphical models: principles and techniques (Koller, Daphne, and Nir Friedman) </li> <li> Model-based Machine Learning (Christopher M. Bishop) </li> <li> Machine Learning. A probabilistic perspective (Kevin P. Murphy) </li> <li> An overview of gradient descent optimization algorithms (Sebastian Rudes) </li> <li> Variational Bayesian inference (slides) (Kay H. Brodersen) </li> <li> Variational Inference (David M. Blei) </li> <li> Sthocastic Variational Inference (Matthew D. Hoffman, David M. Blei, Chong Wang and John Paisley) </li> <li> Black Box Variational Inference (Rajesh Ranganath, Sean Gerrish and David M. Blei) </li> <li> The Stan Math Library: Reverse-Mode Automatic Differentiation in C++ (Bob Carpenter, Matthew D. Hoffman, Marcus Brubaker, Daniel Lee, Peter Li and Michael Betancourt) </li> <li> Automatic Differentiation Variational Inference (Alp Kucukelbir, Dustin Tran, Rajesh Ranganath, Andrew Gelman and David M. Blei) </li> </ul> <div class="border-red-400 border-t-2 my-10"></div> <div><h4 class="text-black text-2xl mb-10 text-center">Write a comment! 😀</h4> <figure class="flex justify-center"><img alt=Loader class=w-48 src=loader.gif></figure> <form class="w-full mt-16" onsubmit="return false"><div class="flex -mx-3 flex-wrap mb-6"><div class="w-full px-3 md:w-1/2 mb-6 md:mb-0"><label class="block text-gray-700 font-bold mb-2 text-xs tracking-wide uppercase" for=name>Name</label> <input class="block text-gray-700 appearance-none bg-gray-200 border border-gray-200 focus:bg-white focus:border-gray-500 focus:outline-none leading-tight px-4 py-3 rounded w-full"> <p class="text-xs text-red-600"></div> <div class="w-full px-3 md:w-1/2"><label class="block text-gray-700 font-bold mb-2 text-xs tracking-wide uppercase" for=email>Email</label> <input class="block text-gray-700 appearance-none bg-gray-200 border border-gray-200 focus:bg-white focus:border-gray-500 focus:outline-none leading-tight px-4 py-3 rounded w-full"> <p class="text-xs text-red-600"></div></div> <div class="flex -mx-3 flex-wrap mb-6"><div class="w-full px-3"><label class="block text-gray-700 font-bold mb-2 text-xs tracking-wide uppercase" for=comment>Comment</label> <textarea class="block text-gray-700 appearance-none bg-gray-200 border border-gray-200 focus:bg-white focus:border-gray-500 focus:outline-none leading-tight px-4 py-3 rounded w-full" rows=2></textarea> <p class="text-xs text-red-600"></div></div> <div class=w-full><button class="w-full px-4 rounded bg-red-500 border-b-4 border-red-700 font-bold hover:bg-red-400 hover:border-red-500 py-2 text-white">Send </button></div></form></div></div></main></div> <script>__SAPPER__={baseUrl:"",preloaded:[void 0,null,{post:{title:"Variational Inference III 🤖🎯",slug:"variational-inference-3",tags:["#probabilistic-machine-learning","#variational-inference","#statistics"],date:"2017-05-11",html:"\n\u003Cp class=\"post_p\"\u003E\n\tIn this post I'm going to continue explaining concepts that I introduced in previous posts\n\t(\u003Ca href=\"https:\u002F\u002Falbertopou.dev\u002Fblog\u002Fvariational-inference-1\u002F\" class=\"post_link\" target=\"_blank\"\u003E1\u003C\u002Fa\u003E and\n\t\u003Ca href=\"https:\u002F\u002Falbertopou.dev\u002Fblog\u002Fvariational-inference-2\u002F\" class=\"post_link\" target=\"_blank\"\u003E2\u003C\u002Fa\u003E) about Variational\n\tinference (VI). Algorithms presented till now have scalability problems. For each iteration the algorithm\n\trequires to go through all the data and this, for massive volumes of data, is impracticable. An alternative to\n\tsolve this problem is: Sthocastic Variational Inference. This version is based on using a batch (set of points)\n\tof data in each iteration. In this way, after more iterations than the conventional solutions, the solution\n\twill tend to a local optimum. The main advantage of this mechanism is that it doesn't require to keep all dataset in\n\tmemory, solving the bottleneck that could be formed by using VI with very big datasets.\n\u003C\u002Fp\u003E\n\u003Ch4 class=\"post_section_title text-2xl text-black\"\u003ESthocastic Variational Inference\u003C\u002Fh4\u003E\n\u003Ch5 class=\"post_section_title text-xl text-black\"\u003ESthocastic optimization\u003C\u002Fh5\u003E\n\u003Cp class=\"post_p\"\u003E\n\tThis technique obtains estimations of the real gradient of an objective function.\n\tThus we obtain an algorithm which iterates each batch and adjusts the hidden structure of the model based only\n\ton that batch of data. Stochastic optimization finds a function relative maximum or minimum using real gradient\n\testimations. Estimations expectation  E(∇λC(λ,x)), being x a batch of data, is equivalent to the\n\treal gradient ∇λC(λ,X), being X the full dataset.\n\u003C\u002Fp\u003E\n\u003Cfigure class=\"post_figure flex justify-center\"\u003E\n\t\u003Cimg class=\"post_image\"\n\t\t src=\"variational-inference-3\u002Fsvi.png\"\n\t\t alt=\"Sthocastic Optimization\"\u003E\n\u003C\u002Ffigure\u003E\n\u003Cp class=\"post_p\"\u003E\n\tUnder ideal conditions these stochastic algorithms converge to a local optimum of the function\n\tif ρ meets the Robins-Monro conditions:\n\u003C\u002Fp\u003E\n\u003Cfigure class=\"post_figure flex justify-center\"\u003E\n\t\u003Cimg class=\"post_image\"\n\t\t src=\"variational-inference-3\u002Frobins.png\"\n\t\t alt=\"Robins-Monro\"\u003E\n\u003C\u002Ffigure\u003E\n\u003Cp class=\"post_p\"\u003E\n\tThe use of this technique results in the algorithm Stochastic Gradient Ascent Variational Inference (SGAVI)\n\tand Stochastic Natural Gradient Ascent Variational Inference (SNGAVI) if natural gradients are used to estimate.\n\tThese algorthims, thanks to the real gradient estimations, can avoid saddle points.\n\u003C\u002Fp\u003E\n\u003Cfigure class=\"post_figure flex justify-center\"\u003E\n\t\u003Cimg class=\"post_image\"\n\t\t src=\"variational-inference-3\u002Fsaddle-point.png\"\n\t\t alt=\"Saddle point\"\u003E\n\u003C\u002Ffigure\u003E\n\u003Cfigcaption class=\"post_image_figcaption\"\u003E\n\tSaddle point. (Source: Wikipedia)\n\u003C\u002Ffigcaption\u003E\n\u003Ch5 class=\"post_section_title text-xl text-black\"\u003EAlgorithm\u003C\u002Fh5\u003E\n\u003Cp class=\"post_p\"\u003E\n\tIn VI, the function to be optimized is called ELBO. If variational parameters are updated by closed analytical\n\tformulas, the algorithm is known as Sthocastic Coordinate Ascent Variational Inference (SCAVI) while if we use\n\tsthocastic optimization the algorithm is known as SGAVI. This version uses a corrector term using calculations\n\tfrom previous iterations. The generic algorithm would be as follows:\n\u003C\u002Fp\u003E\n\u003Cfigure class=\"post_figure flex justify-center\"\u003E\n\t\u003Cimg class=\"post_image\"\n\t\t src=\"variational-inference-3\u002Fsvi-algorithm.png\"\n\t\t alt=\"Sthocastic Variational Inference Algorithm\"\u003E\n\u003C\u002Ffigure\u003E\n\u003Ch4 class=\"post_section_title text-2xl text-black\"\u003EBlack Box Variational Inference\u003C\u002Fh4\u003E\n\u003Cp class=\"post_p\"\u003E\n\tStarting from the ELBO formula that was reached in the\n\t\u003Ca href=\"https:\u002F\u002Falbertopou.herokuapp.com\u002F#variational_inference_2\" class=\"post_link\" target=\"_blank\"\u003Eprevious post\u003C\u002Fa\u003E:\n\u003C\u002Fp\u003E\n\u003Cfigure class=\"post_figure flex justify-center\"\u003E\n\t\u003Cimg class=\"post_image\"\n\t\t src=\"variational-inference-3\u002Felbo2.png\"\n\t\t alt=\"ELBO\"\u003E\n\u003C\u002Ffigure\u003E\n\u003Cp class=\"post_p\"\u003E\n\tThe main idea of Black Box Variational Inference (BBVI) consists of sampling the variational model q(θ|λ) with\n\tthe purpose of obtain an approach of formula expectations. Such expectations regarding the variational model\n\tcan be computationally expensive and can be a bottleneck at computer memory level.\n\u003C\u002Fp\u003E\n\u003Ch5 class=\"post_section_title text-xl text-black\"\u003EScore Gradients\u003C\u002Fh5\u003E\n\u003Cp class=\"post_p\"\u003E\n\tNow we apply gradients and some algebraic transformations into the analytic ELBO formula (complete derivation\n\tis not shown):\n\u003C\u002Fp\u003E\n\u003Cfigure class=\"post_figure flex justify-center\"\u003E\n\t\u003Cimg class=\"post_image\"\n\t\t src=\"variational-inference-3\u002Felbo3.png\"\n\t\t alt=\"ELBO\"\u003E\n\u003C\u002Ffigure\u003E\n\u003Cp class=\"post_p\"\u003E\n\tAfter these transformations you could notice that it is not necesary to obtain gradients of complete ELBO, it\n\tis enough to derive the variational model (q(θ|λ)).\n\u003C\u002Fp\u003E\n\u003Ch5 class=\"post_section_title text-xl text-black\"\u003EMonte-Carlo Integration\u003C\u002Fh5\u003E\n\u003Cp class=\"post_p\"\u003E\n\tMonte-Carlo integration is a mechanism to approximate integrals. It consists of sampling the variable\n\twith respect to which we are integrating and do a summation of function values given those samples.\n\tThe more samples are taken from the variable more exact will be the approximation. In our case we are going\n\tto use this mechanism to approximate ELBO integral, which integrates with respect to the q(θ|λ) distribution.\n\tThen a set of samples (θs) obtained from the q(θ|λ) distribution will allow us to get an approximation of\n\tthe integral. The formula would be as follows:\n\u003C\u002Fp\u003E\n\u003Cfigure class=\"post_figure flex justify-center\"\u003E\n\t\u003Cimg class=\"post_image\"\n\t\t src=\"variational-inference-3\u002Fmontecarlo.png\"\n\t\t alt=\"Monte-Carlo Integration\"\u003E\n\u003C\u002Ffigure\u003E\n\u003Ch5 class=\"post_section_title text-xl text-black\"\u003EAlgorithm\u003C\u002Fh5\u003E\n\u003Cp class=\"post_p\"\u003E\n\tBelow is the BBVI algorithm:\n\u003C\u002Fp\u003E\n\u003Cfigure class=\"post_figure flex justify-center\"\u003E\n\t\u003Cimg class=\"post_image\"\n\t\t src=\"variational-inference-3\u002Fbbvi.png\"\n\t\t alt=\"Black Box Variational Inference Algorithm\"\u003E\n\u003C\u002Ffigure\u003E\n\u003Ch4 class=\"post_section_title text-2xl text-black\"\u003EConsiderations\u003C\u002Fh4\u003E\n\u003Cp class=\"post_p\"\u003E\n\tThis algorithm is the result of get some measures that can question the convergence of VI algorithm. Starting by\n\tsupposing q(θ|λ) factorizes (mean field assumption) to approximate ELBO integral using Monte-Carlo.\n\tAll this causes that this algorithm is subjected to a high variance, and depending on the model, slow convergence.\n\tIn order to reduce the variance that this method can cause some mechanisms have appeared:\n\u003C\u002Fp\u003E\n\u003Cul class=\"post_list\"\u003E\n\t\u003Cli\u003E\n\t\t\u003Cstrong\u003ERao-Blackwellization\u003C\u002Fstrong\u003E. This method reduces the variance of an aleatory variable replacing it\n\t\tby its conditional expectation with respect to a subset of variables.\n\t\u003C\u002Fli\u003E\n\t\u003Cli\u003E\n\t\t\u003Cstrong\u003EControl variates\u003C\u002Fstrong\u003E. This method replaces Monte-Carlo expectation by another function with\n\t\tless variance.\n\t\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Cp class=\"post_p\"\u003E\n\tA positive aspect of this method is that it is not necessary to derive the analytical formulas used to update\n\tthe variational parameters either analytical ELBO. This permits the access to these algorithms to people who have\n\tless statistical knowledge (as me). It exists another approximation called Automatic Differentiation Variational\n\tInference (ADVI). This method also has these advantages and improves the convergence of unconjugated models\n\tthat can be a challenge for the rest of VI variants.\n\u003C\u002Fp\u003E\n\u003Cp class=\"post_p\"\u003E\n\tFormerly with VI only conjugated models could be inferred since the unconjugated models didn't be easily derivable.\n\tThe discovery of algorithms like ADVI and BBVI allowed the inference of this kind of models because they changed\n\tthe analytical calculus for an approximate strategy.\n\u003C\u002Fp\u003E\n\u003Ch4 class=\"post_section_title text-2xl text-black\"\u003EVariational inference libraries\u003C\u002Fh4\u003E\n\u003Cp class=\"post_p\"\u003E\n\tBest-known libraries for the use of VI and Markov Chain Monte-Carlo (MCMC):\n\u003C\u002Fp\u003E\n\u003Cul class=\"post_list\"\u003E\n\t\u003Cli\u003E\n\t\t\u003Ca href=\"http:\u002F\u002Fedwardlib.org\u002F\" class=\"post_link\" target=\"_blank\"\u003EEdward\u003C\u002Fa\u003E: It uses Tensorflow for gradients computations\n\t\tand has BBVI, reparameterization BBVI and Metropolis-Hastings implementations.\n\t\u003C\u002Fli\u003E\n\t\u003Cli\u003E\n\t\t\u003Ca href=\"http:\u002F\u002Fmc-stan.org\u002F\" class=\"post_link\" target=\"_blank\"\u003EStan\u003C\u002Fa\u003E: It uses C++ Automatic Differentiation\n\t\treverse mode for gradients computation and has ADVI and HMC implementations.\n\t\u003C\u002Fli\u003E\n\t\u003Cli\u003E\n\t\t\u003Ca href=\"https:\u002F\u002Fgithub.com\u002Fpymc-devs\u002Fpymc3\" class=\"post_link\" target=\"_blank\"\u003EPyMC3\u003C\u002Fa\u003E: It uses Theano for gradient computations\n\t\tand has ADVI, Gibbs sampling and Metropolis-Hastings implementations.\n\t\u003C\u002Fli\u003E\n\t\u003Cli\u003E\n\t\t\u003Ca href=\"https:\u002F\u002Fwww.tensorflow.org\u002Fversions\u002Fr0.12\u002Fapi_docs\u002Fpython\u002Fcontrib.bayesflow.variational_inference\u002F\" class=\"post_link\" target=\"_blank\"\u003EBayesflow\u003C\u002Fa\u003E:\n\t\tYoung Google module for VI.\n\t\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Cp class=\"post_p\"\u003E\n\tIf you want to learn more about Variational Inference and its uses in probabilistic models inference you can take\n\ta look to my \u003Ca href=\"https:\u002F\u002Fgithub.com\u002Fbertini36\u002FGMM\u002Fblob\u002Fmaster\u002Fdocs\u002Fdoc.pdf\" class=\"post_link\" target=\"_blank\"\u003E\n\tmaster thesis\u003C\u002Fa\u003E. It focuses on the use of automatic differentiation tools to apply Variational Inference into a\n\tGaussian Mixture Model (GMM). At the \u003Ca href=\"https:\u002F\u002Fgithub.com\u002Fbertini36\u002FGMM\" class=\"post_link\" target=\"_blank\"\u003Erepository\u003C\u002Fa\u003E you can\n\tfind implementations of Gaussian Mixture Model with different technologies like\n\t\u003Ca href=\"https:\u002F\u002Fwww.tensorflow.org\u002F\" class=\"post_link\" target=\"_blank\"\u003ETensorflow\u003C\u002Fa\u003E, Python,\n\t\u003Ca href=\"https:\u002F\u002Fgithub.com\u002Fblei-lab\u002Fedward\" class=\"post_link\" target=\"_blank\"\u003EEdward\u003C\u002Fa\u003E,\n\t\u003Ca href=\"https:\u002F\u002Fgithub.com\u002FHIPS\u002Fautograd\" class=\"post_link\" target=\"_blank\"\u003EAutograd\u003C\u002Fa\u003E, ...\n\tAnd also other probabilistic models that helped me to learn.\n\u003C\u002Fp\u003E\n\u003Ch4 class=\"post_section_title text-2xl text-black\"\u003EReferences\u003C\u002Fh4\u003E\n\u003Cul class=\"post_list\"\u003E\n\t\u003Cli\u003E\n\t\tJournal of the American Statistical AssociationGeorge\n\t\t(E. P. Box)\n\t\u003C\u002Fli\u003E\n\t\u003Cli\u003E\n\t\tBuild, Compute, Critique, Repeat: Data Analysis with Latent Variable Models\n\t\t(David M. Blei)\n\t\u003C\u002Fli\u003E\n\t\u003Cli\u003E\n\t\tProbabilistic graphical models: principles and techniques\n\t\t(Koller, Daphne, and Nir Friedman)\n\t\u003C\u002Fli\u003E\n\t\u003Cli\u003E\n\t\tModel-based Machine Learning\n\t\t(Christopher M. Bishop)\n\t\u003C\u002Fli\u003E\n\t\u003Cli\u003E\n\t\tMachine Learning. A probabilistic perspective\n\t\t(Kevin P. Murphy)\n\t\u003C\u002Fli\u003E\n\t\u003Cli\u003E\n\t\tAn overview of gradient descent optimization algorithms\n\t\t(Sebastian Rudes)\n\t\u003C\u002Fli\u003E\n\t\u003Cli\u003E\n\t\tVariational Bayesian inference (slides)\n\t\t(Kay H. Brodersen)\n\t\u003C\u002Fli\u003E\n\t\u003Cli\u003E\n\t\tVariational Inference\n\t\t(David M. Blei)\n\t\u003C\u002Fli\u003E\n\t\u003Cli\u003E\n\t\tSthocastic Variational Inference\n\t\t(Matthew D. Hoffman, David M. Blei, Chong Wang and John Paisley)\n\t\u003C\u002Fli\u003E\n\t\u003Cli\u003E\n\t\tBlack Box Variational Inference\n\t\t(Rajesh Ranganath, Sean Gerrish and David M. Blei)\n\t\u003C\u002Fli\u003E\n\t\u003Cli\u003E\n\t\tThe Stan Math Library: Reverse-Mode Automatic Differentiation in C++\n\t\t(Bob Carpenter, Matthew D. Hoffman, Marcus Brubaker, Daniel Lee, Peter Li and Michael Betancourt)\n\t\u003C\u002Fli\u003E\n\t\u003Cli\u003E\n\t\tAutomatic Differentiation Variational Inference\n\t\t(Alp Kucukelbir, Dustin Tran, Rajesh Ranganath, Andrew Gelman and David M. Blei)\n\t\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\t\t"},env:"production"}]};if('serviceWorker' in navigator)navigator.serviceWorker.register('/service-worker.js');(function(){try{eval("async function x(){}");var main="/client/client.8e6d9fe5.js"}catch(e){main="/client/legacy/client.f5a415ce.js"};var s=document.createElement("script");try{new Function("if(0)import('')")();s.src=main;s.type="module";s.crossOrigin="use-credentials";}catch(e){s.src="/client/shimport@1.0.1.js";s.setAttribute("data-main",main);}document.head.appendChild(s);}());</script> 