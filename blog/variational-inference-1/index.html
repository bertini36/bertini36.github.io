<!doctype html> <html lang=en> <head> <meta charset=utf-8> <meta content="width=device-width,initial-scale=1" name=viewport> <meta content=#333333 name=theme-color> <base href=/ > <link href=https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css rel=stylesheet> <link href=styles.css rel=stylesheet> <link href=manifest.json rel=manifest crossorigin=use-credentials> <link href=black_logo.png rel=icon type=image/png> <link href=client/main.2075857528.css rel=stylesheet> <noscript id=sapper-head-start></noscript><title>Variational Inference I ðŸ¤–</title><noscript id=sapper-head-end></noscript> </head> <body> <div id=sapper> <nav class="border-b-2 border-red-200 dark:border-b-4 font-light px-4 py-0"><ul class="svelte-i3hbql m-0 p-0"><li class=float-left><a href=. class="block svelte-i3hbql dark:hover:text-red-400 hover:text-red-500 px-3 py-4">Home</a></li> <li class=float-left><a href=blog class="block svelte-i3hbql dark:hover:text-red-400 hover:text-red-500 px-3 py-4 no-underline" aria-current=page rel=prefetch>Blog</a></li> <li class=float-right><a href=https://github.com/bertini36/bertini36.github.io class="block dark:hover:text-red-400 hover:text-red-500 px-3 py-4 no-underline" target=_blank><i class="fa fa-github"></i></a></li> <li class="float-right pt-4"><label class="mr-2 text-xs" for=toggle>Dark mode</label> <div class="mr-2 align-middle duration-200 ease-in inline-block relative select-none transition w-10"><input class="block appearance-none focus:outline-none absolute bg-white border-4 cursor-pointer h-6 rounded-full svelte-i3hbql toggle-checkbox w-6" id=toggle name=toggle type=checkbox> <label class="block svelte-i3hbql cursor-pointer h-6 rounded-full bg-gray-300 overflow-hidden toggle-label" for=toggle></label></div></ul> </nav> <main class=svelte-6aaop8> <h1 class="text-4xl font-bold title">Variational Inference I ðŸ¤–</h1> <div class="mb-10 post"> <p class=post_p> In next posts I'm going to speak about probabilistic machine learning. Last months I was learning about this field, specifically about a type of inference on these models known as Variational Inference, and I reckon can be interesting to write a set of posts summarizing my experience about it thanks to my teacher <a href=https://twitter.com/CapdevilaPujol class=post_link target=_blank>Joan Capdevila</a>. </p> <h4>Probabilistic Machine Learning</h4> <p class=post_p> Last decades studies about machine learning has caused the appearance of a wide variety of algorithms to solve a large set of problems covering areas such as driving autonomous vehicles, medical diagnosis, speech recognition or user ranking for marketing campaigns. These algorithms are mainly based on aa construction of a model which describes data as closely as possible. </p> <p class=post_p> A model is a compact description of a set of data that avoids us to make predictions about future samples. The main difference between a conventional machine learning model and a probabilistic one is that the last avoids to model uncertainty, in other words, it avoids to know how probably is a prediction to be fulfilled. This aspect can be very valuable in areas such as medicine or economy where risk about to take a bad decision can be detrimental to a person's health or lead to financial loss. </p> <figure class="flex justify-center post_figure"> <img alt="Probabilistic Machine Learning situation" class=post_image src=variational-inference-1/schema_situation.png> </figure> <figcaption class=post_image_figcaption> Probabilistic machine learning situation at artificial intelligence area </figcaption> <p class=post_p> This kind of models uses the probability theory to model priori information, in this way the algorithm is not based just on sample data. These models permit the use of different datasets to learn (this is useful when we have a small quantity of data) and to define complex models with a quantity of random variables required. They also support online leaning, you don't need to retrain the full model each time you obtain new data, you just require to update some probabilities. They are also very useful in decision-making, when a robust explanation of a model is required. Finally they are generative models, thanks to the distributions that they infer, they allow to generate new data simulating values â€‹â€‹of any random variable of the model. </p> <p class=post_p> Unlike discriminant models which only model the probability of the variable to be predicted, a generative model is a complete model which uses all the variables (observed and latent), allowing multiple questions to be answered. </p> <p class=post_p> The construction of this type of models with latent variables is done following Box Loop philosophy. This loop was created by the statistician <a href=https://en.wikipedia.org/wiki/George_E._P._Box class=post_link target=_blank>George E. P. Box</a>. This loop is iterated several times during the construction of a probabilistic model. </p> <figure class="flex justify-center post_figure"> <img alt="Box loop" class=post_image src=variational-inference-1/box_loop.png> </figure> <figcaption class=post_image_figcaption> Box loop graphical scheme </figcaption> <ul class=post_list> <li>First, probabilistic model is made based on environment knowledge that we already have.</li> <li>After some patterns are discovered using the model previously defined and with an inference method.</li> <li> Finally the model has to be validated. If it was not good enough we would go back to step 1 unless it would be used to describe or predict new data. </li> </ul> <h4>Bayesian inference</h4> <p class=post_p> Bayesian inference tries to reveal a hidden structure in data that cannot be directly observed. For traditional machine learning methods parameters are values that are determined by optimization algorithms minimizing an error function. The bayesian point of view is a little bit different. For a bayesian all the unknown parameters are described by probability distributions and observation of evidence avoids to update these distrubutions using Bayes rule. </p> <h5>Bayes rule</h5> <p class=post_p> The first thing you need to be clear about to understand Bayesian inference is Bayes rule. Bayes rule indicates how a priori probability about an event has to be updated after observe evidences about it. From a bayesian point of view there are no differences between parameters and observed variables, both are random variables. I'm going to use x to reference observed variables and Î¸ to reference latent variables. </p> <figure class="flex justify-center post_figure"> <img alt="Bayes rule" class="post_image md:w-1/6 sm:w-1/2" src=variational-inference-1/bayes_rule.png> </figure> <p class=post_p> The following explains what is each term of the formula, being x and Î¸ data and model parameters respectively: </p> <ul class=post_list> <li> <strong>Posterior p(Î¸|x)</strong>: It is the probability of data, the probability that the model with Î¸ parameters has generated x data. </li> <li> <strong>Likelihood p(x|Î¸)</strong>: It is the probability of data assuming that are modeled by a parametrized distribution. The way to calculate it depends on the model. Usually it is used to assess the quality of a model. </li> <li> <strong>Prior p(Î¸)</strong>: It is the probability of the parameters. In this factor of the formula is where prior knowledge is reflected, information that we have before to observe any data observation. </li> <li> <strong>Evidence p(x)</strong>: This is the evidence from data. It is calculated as âˆ« p(x,Î¸)dÎ¸. Usually it can't be calculated but, as it is a normalizing constant of the model, it does not affect. When two probabilistic models are compared, important factors are the ones which depend on Î¸ because p(x) will be the same for each model because it just depends on data. </li> </ul> <p class=post_p> Product p(x|Î¸)p(Î¸) is also known as joint probability: p(Î¸,x). </p> <p class=post_p> Use the likelihood to estimate Î¸ parameters is known as Maximum Likelihood Estimation (MLE) while if you take the prior into account then is known as Maximum A Posteriori estimation (MAP). MLE and MAP are equivalents if there is an uniform prior. However these methods only allow estimating a mean, a median and a mode of the posterior and maybe your goal requires to model uncertainty or to generate new data. In theses cases we would need to know posterior distribution. As we will see, methods as Variational Inference (VI) or Markov Chain Monte Carlo (MCMC) allow to infer this distribution. Taking into account the Bayes normalizer constant p(x) is what allows these methods to calculate a posterior distribution. </p> <p class=post_p> A summary of this formula would be: At the beginning we have a belief (prior) about an event Î¸ (p(Î¸)), for example, that the height of Barcelona population is described by a Normal distribution. After we observe evidences (x), a sample of the heights of Barcelona population. With this evidence our belief about Î¸ has to be updated, in other words, Normal distribution which described the height of Barcelona population has to be updated. This change is reflected by the posterior distribution (p(Î¸|x)). In this example we can appreciate the support to online learning that offers this kind of models. </p> <figure class="flex justify-center post_figure"> <img alt="Online learning" class=post_image src=variational-inference-1/online_learning.png> </figure> <p class=post_p> At the end it is an iterative process of updating beliefs (prior) based on evidences (x) where posterior of one iteration will be the prior of next one. </p> <p class=post_p> Posterior inference algorithms avoid to analyze information under certain assumptions (priors) discovering a hidden structure which best fits with our data. When all relations between model random variable are <a href=https://en.wikipedia.org/wiki/Conjugate_prior class=post_link target=_blank>conjugated</a>, this is, when joint distribution has the same form as the prior, posterior can be calculated analytically. This is the case of models like Dirichlet-Categorical o Normal Inverse Wishart-Normal. In the opposite case, the problem of this formula resides in the calculation of the evidence (p(x)). For many models of interest (no conjugated) to calculate a posterior is computationally intractable because the integral over all latent variables of data that requires to calculate the evidence. For these models, another strategy is required to obtain the posterior, for that reason the calculation of the posterior becomes an approximation problem. </p> <h5>Posterior approximation</h5> <p class=post_p> Probabilistic machine learning uses latent variables to describe data hidden structure, some relations between observed and latent variables are modeled using probability distributions and inference algorithms are used to estimate the posterior, that is the conditional distribution about latent variables given the observed variables. Due to the fact that in most cases we are working in spaces with many dimensions, the calculation of posterior expectation, E(p(Î¸|x)), is impossible to obtain analytically and computationally, for this reason some inference methods have been created to approximate this distribution. Bayesian inference concept comes from the set of tools which have been developed to approximate this posterior and it is one of the central problems in bayesian statistics. Nowadays there are 2 algorithmic branches: </p> <ul class=post_list> <li> <a href=https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo class=post_link target=_blank> Markov Chain Monte Carlo (MCMC) </a>: Sampling approximate inference. </li> <li> <a href=https://en.wikipedia.org/wiki/Variational_Bayesian_methods class=post_link target=_blank> Variational Inference (VI) </a>: Structural approximate inference. </li> </ul> <p class=post_p> On the one hand, MCMC is based on the construction of a Markov chain over all latent variables being the posterior its stationary distribution. After the chain is executed till it arrives to its equilibrium point. Finally, results obtained sampling the Markov chain in its stationary section, are the posterior samples. The best known algorithms of this family are <a href=https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm class=post_link target=_blank>Metropolisâ€“Hastings</a> , <a href=https://en.wikipedia.org/wiki/Gibbs_sampling class=post_link target=_blank>Gibbs sampling</a> and <a href=https://en.wikipedia.org/wiki/Hamiltonian_Monte_Carlo>Hamiltonian Monte Carlo</a>. </p> <p class=post_p> On the other hand, Variational Inference approximates the posterior creating an analytical approximation, the variational model, which is adjusted in order to reduce the distance with the posterior. In this family the problem is transformed from an approximation one to an optimization one. </p> <h5>Probabilistic Graphical Models</h5> <p class=post_p> In the bayesian field, a model represents a joint probability over all random variables of the problem. </p> <figure class="flex justify-center post_figure"> <img alt="Joint distribution" class="post_image md:w-1/5 sm:w-4/5" src=variational-inference-1/joint_dist.png> </figure> <p class=post_p> A very practical way of representing these models is using <a href=https://en.wikipedia.org/wiki/Graphical_model class=post_link target=_blank>probabilistic graphical models</a>. A probabilistic graphical model is a directed graph where nodes are random variables and edges are dependency relations between those variables. For example, joint distribution probabilistic graphical model is: </p> <figure class="flex justify-center post_figure"> <img alt="Joint distribution" class="post_image md:w-1/4 sm:w-64" src=variational-inference-1/joint_dist2.png> </figure> <figure class="flex justify-center post_figure"> <img alt="Joint distribution" class=post_image src=variational-inference-1/graphical_model.png> </figure> <p class=post_p> At this context, p(x|y) represents the conditional probability in which <i>x</i> variable depends on the value of y. At this kind of diagrams also exists another components called plates. </p> <figure class="flex justify-center post_figure"> <img alt=Plate class=post_image src=variational-inference-1/plate.png> </figure> <p class=post_p> This notation indicates a vector of n random variables <i>x</i>. </p> <h5>Local and global variables</h5> <p class=post_p> In a probabilistic model two types of random variables can be distinguished: globals and locals. A global variable is the one which is shared between all dataset examples while a local variable is owned by each example. For example, the following probabilistic graphical model, <i>y</i> variable is local while <i>z</i> variable is a global one. When a node appears obscured it means that it is an observed variable. </p> <figure class="flex justify-center post_figure"> <img alt="Model example" class=post_image src=variational-inference-1/global_local_variables.png> </figure> <h4>Variational inference</h4> <p class=post_p> In next posts we will focus on the different variational strategies for posterior approach. </p> <h5>Strategy</h5> <p class=post_p> As already mentioned, Variational Inference consists in defining a distribution, q(Î¸|Î»), whose parameters Î» will be optimized in order to reduce the differences with the posterior p(Î¸|x). This new distribution is known as variational model and the posterior as probabilistic model. To summarize, VI goal is to optimize variational model Î» parameters in order to reduce the distance with probabilistic model p(Î¸|x). Î» parameters are known also as variational parameters. </p> <figure class="flex justify-center post_figure"> <img alt="Variational inference" class=post_image src=variational-inference-1/variational_inference.jpg> </figure> <h5>Kullback-Leibler divergence</h5> <p class=post_p> To calculate Euclidean distance between distribution parameters to establish the similarity between both is an imperfect measure since we are comparing distributions and not points. Imagine a Normal distribution with 0 mean and 5 variance, N(0, 5), and another one with 5 mean and 5 variance, N(5, 5). These two distributions are very similar but they are separated by an Euclidean distance of 5. If now we compare first defined Normal distribution N(0, 5) with another Normal distribution with 2 mean and 7 variance, N(2, 7), looks like they are more different but Euclidean distance that separates them is 4. For this reason we have to use another measure: Kullback-Leibler divergence (KL). </p> <p class=post_p> KL is a divergence, in other words, a non-simmetric distance, it isn't the same to calculate KL[p(Î¸|x)||q(Î¸|Î»)] (forwards KL) than KL[q(Î¸|Î»)||p(Î¸|x)] (reverse KL). The fact of use one or another rises to different algorithms: <a href=https://tminka.github.io/papers/ep/minka-ep-uai.pdf class=post_link target=_blank>Expectation Propagation</a> uses forwards KL while VI uses reverse KL. In general terms, Expectation Propagation is more computationally expensive. KL quantifies loss information when you approximate one distribution with another. It is based on the concept of entropy. Entropy measures the quantity of information that own data and is defined as follows: </p> <figure class="flex justify-center post_figure"> <img alt=Entropy class=post_image src=variational-inference-1/entropy.png> </figure> <p class=post_p> KL definition consists in modify entropy formula to take q distribution into account. </p> <figure class="flex justify-center post_figure"> <img alt="Kullback-Leibler divergence" class=post_image src=variational-inference-1/kl1.png> </figure> <p class=post_p> If we adapt this form to the VI problem and we apply logarithms properties we have: </p> <figure class="flex justify-center post_figure"> <img alt="Kullback-Leibler definition" class="post_image md:w-1/3 sm:4/5" src=variational-inference-1/kl_rule.png> </figure> <p class=post_p> This divergence avoids us to find the real similarity between two probability distributions and it is the measure that is minimized at VI algorithm. </p> <figure class="flex justify-center post_figure"> <img alt="Kullback-Leibler divergence" class=post_image src=variational-inference-1/kullback-leibler1.png> </figure> <figcaption class=post_image_figcaption> Forwards and reverse KL comparison to approximate a bimodal distribution. Blue part represents distribution to be approximated and red one the approximation. <strong>a</strong> image is an approximation with forwards KL and <strong>b</strong> and <strong>c</strong> are approximation with reverse KL. </figcaption> <figure class="flex justify-center post_figure"> <img alt="Kullback-Leibler visualization" class=post_image src=variational-inference-1/kullback-leibler2.png> </figure> <figcaption class=post_image_figcaption> Forwards and reverse KL comparison to approximate a unimodal distribution. Blue part represents distribution to be approximated and red one the approximation. <strong>a</strong> image is an approximation with forwards KL and <strong>b</strong> is an approximation with reverse KL. </figcaption> <h5>Mean-Field assumption</h5> <p class=post_p> In order to define a treatable distribution over all latent variables to approximate the posterior we can simplify variational model optimization assuming that it is a factorized model. It is to suppose that q(Î¸|Î») is composed by a set of distributions q<sub>i</sub>(Î¸<sub>i</sub>|Î»<sub>i</sub>) (of the <a href=https://en.wikipedia.org/wiki/Exponential_family class=post_link target=_blank>Exponential Family</a>). Each one of these distributions has its own parameters Î»<sub>i</sub> which could be optimized individually. </p> <figure class="flex justify-center post_figure"> <img alt="Mean-Field assumption" class="post_image md:w-1/5 sm:w-4/5" src=variational-inference-1/mean_field.png> </figure> <p class=post_p> Main goal of this post is not more than to present the basic idea of Variational Inference and its main players. In following posts we will go deeper into these algorithms and programming tools to code this models will be presented. </p> <h4>References</h4> <ul class=post_list> <li> Journal of the American Statistical AssociationGeorge (E. P. Box) </li> <li> Build, Compute, Critique, Repeat: Data Analysis with Latent Variable Models (David M. Blei) </li> <li> Probabilistic graphical models: principles and techniques (Koller, Daphne, and Nir Friedman) </li> <li> Model-based Machine Learning (Christopher M. Bishop) </li> <li> Machine Learning. A probabilistic perspective (Kevin P. Murphy) </li> </ul> <div class="border-red-400 border-t-2 dark:border-white my-10"></div> <div><h4 class="mb-10 text-2xl text-black text-center">Write a comment! ðŸ˜€</h4> <figure class="flex justify-center"><img alt=Loader class=w-48 src=loader.gif></figure> <form class="w-full mt-16" onsubmit="return false"><div class="flex -mx-3 flex-wrap mb-6"><div class="w-full px-3 md:w-1/2 mb-6 md:mb-0"><label class="block dark:text-white text-gray-700 font-bold mb-2 text-xs tracking-wide uppercase" for=name>Name</label> <input class="block dark:text-white text-gray-700 appearance-none bg-gray-200 border border-gray-200 focus:bg-white focus:border-gray-500 focus:outline-none leading-tight px-4 py-3 rounded w-full"> <p class="dark:text-white text-xs text-red-600"></div> <div class="w-full px-3 md:w-1/2"><label class="block dark:text-white text-gray-700 font-bold mb-2 text-xs tracking-wide uppercase" for=email>Email</label> <input class="block dark:text-white text-gray-700 appearance-none bg-gray-200 border border-gray-200 focus:bg-white focus:border-gray-500 focus:outline-none leading-tight px-4 py-3 rounded w-full"> <p class="dark:text-white text-xs text-red-600"></div></div> <div class="flex -mx-3 flex-wrap mb-6"><div class="w-full px-3"><label class="block dark:text-white text-gray-700 font-bold mb-2 text-xs tracking-wide uppercase" for=comment>Comment</label> <textarea class="block dark:text-white text-gray-700 appearance-none bg-gray-200 border border-gray-200 focus:bg-white focus:border-gray-500 focus:outline-none leading-tight px-4 py-3 rounded w-full" rows=2></textarea> <p class="dark:text-white text-xs text-red-600"></div></div> <div class=w-full><button class="w-full px-4 rounded bg-red-500 border-b-4 border-red-700 dark:bg-red-400 dark:border-red-500 dark:hover:bg-red-300 dark:hover:border-red-400 font-bold hover:bg-red-400 hover:border-red-500 py-2 text-white">Send </button></div></form></div></div> </main></div> <script>__SAPPER__={baseUrl:"",preloaded:[void 0,null,{post:{title:"Variational Inference I ðŸ¤–",slug:"variational-inference-1",tags:["#probabilistic-machine-learning","#variational-inference","#statistics"],date:"2017-02-09",html:"\n\u003Cp class=\"post_p\"\u003E\n\tIn next posts I'm going to speak about probabilistic machine learning. Last months I was learning about this\n\tfield, specifically about a type of inference on these models known as Variational Inference, and I reckon can be\n\tinteresting to write a set of posts summarizing my experience about it thanks to my teacher\n\t\u003Ca href=\"https:\u002F\u002Ftwitter.com\u002FCapdevilaPujol\" class=\"post_link\" target=\"_blank\"\u003EJoan Capdevila\u003C\u002Fa\u003E.\n\u003C\u002Fp\u003E\n\u003Ch4\u003EProbabilistic Machine Learning\u003C\u002Fh4\u003E\n\u003Cp class=\"post_p\"\u003E\n\tLast decades studies about machine learning has caused the appearance of a wide variety of algorithms to solve\n\ta large set of problems covering areas such as driving autonomous vehicles, medical diagnosis, speech recognition\n\tor user ranking for marketing campaigns. These algorithms are mainly based on aa construction of a model which\n\tdescribes data as closely as possible.\n\u003C\u002Fp\u003E\n\u003Cp class=\"post_p\"\u003E\n\tA model is a compact description of a set of data that avoids us to make predictions about future samples. The main\n\tdifference between a conventional machine learning model and a probabilistic one is that the last avoids to model\n\tuncertainty, in other words, it avoids to know how probably is a prediction to be fulfilled. This aspect can\n\tbe very valuable in areas such as medicine or economy where risk about to take a bad decision can be detrimental to\n\ta person's health or lead to financial loss.\n\u003C\u002Fp\u003E\n\u003Cfigure class=\"post_figure flex justify-center\"\u003E\n\t\u003Cimg class=\"post_image\"\n\t\t src=\"variational-inference-1\u002Fschema_situation.png\"\n\t\t alt=\"Probabilistic Machine Learning situation\"\u003E\n\u003C\u002Ffigure\u003E\n\u003Cfigcaption class=\"post_image_figcaption\"\u003E\n\tProbabilistic machine learning situation at artificial intelligence area\n\u003C\u002Ffigcaption\u003E\n\u003Cp class=\"post_p\"\u003E\n\tThis kind of models uses the probability theory to model priori information, in this way the algorithm is not\n\tbased just on sample data. These models permit the use of different datasets to learn (this is useful when\n\twe have a small quantity of data) and to define complex models with a quantity of random variables required.\n\tThey also support online leaning, you don't need to retrain the full model each time you obtain new data, you just\n\trequire to update some probabilities. They are also very useful in decision-making, when a robust explanation of a\n\tmodel is required. Finally they are generative models, thanks to the distributions that they infer, they allow\n\tto generate new data simulating values â€‹â€‹of any random variable of the model.\n\u003C\u002Fp\u003E\n\u003Cp class=\"post_p\"\u003E\n\tUnlike discriminant models which only model the probability of the variable to be predicted, a generative model is\n\ta complete model which uses all the variables (observed and latent), allowing multiple questions to be\n\tanswered.\n\u003C\u002Fp\u003E\n\u003Cp class=\"post_p\"\u003E\n\tThe construction of this type of models with latent variables is done following Box Loop philosophy. This loop\n\twas created by the statistician\n\t\u003Ca href=\"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FGeorge_E._P._Box\" class=\"post_link\" target=\"_blank\"\u003EGeorge E. P. Box\u003C\u002Fa\u003E. This loop is\n\titerated several times during the construction of a probabilistic model.\n\u003C\u002Fp\u003E\n\u003Cfigure class=\"post_figure flex justify-center\"\u003E\n\t\u003Cimg class=\"post_image\"\n\t\t src=\"variational-inference-1\u002Fbox_loop.png\"\n\t\t alt=\"Box loop\"\u003E\n\u003C\u002Ffigure\u003E\n\u003Cfigcaption class=\"post_image_figcaption\"\u003E\n\tBox loop graphical scheme\n\u003C\u002Ffigcaption\u003E\n\u003Cul class=\"post_list\"\u003E\n\t\u003Cli\u003EFirst, probabilistic model is made based on environment knowledge that we already have.\u003C\u002Fli\u003E\n\t\u003Cli\u003EAfter some patterns are discovered using the model previously defined and with an inference method.\u003C\u002Fli\u003E\n\t\u003Cli\u003E\n\t\tFinally the model has to be validated. If it was not good enough we would go back to step 1 unless it would be\n\t\tused to describe or predict new data.\n\t\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Ch4\u003EBayesian inference\u003C\u002Fh4\u003E\n\u003Cp class=\"post_p\"\u003E\n\tBayesian inference tries to reveal a hidden structure in data that cannot be directly observed. For\n\ttraditional machine learning methods parameters are values that are determined by optimization algorithms\n\tminimizing an error function. The bayesian point of view is a little bit different. For a bayesian all the\n\tunknown parameters are described by probability distributions and observation of evidence avoids to update\n\tthese distrubutions using Bayes rule.\n\u003C\u002Fp\u003E\n\u003Ch5\u003EBayes rule\u003C\u002Fh5\u003E\n\u003Cp class=\"post_p\"\u003E\n\tThe first thing you need to be clear about to understand Bayesian inference is Bayes rule. Bayes rule indicates\n\thow a priori probability about an event has to be updated after observe evidences about it. From a bayesian point\n\tof view there are no differences between parameters and observed variables, both are random variables. I'm going to\n\tuse x to reference observed variables and Î¸ to reference latent variables.\n\u003C\u002Fp\u003E\n\u003Cfigure class=\"post_figure flex justify-center\"\u003E\n\t\u003Cimg class=\"post_image md:w-1\u002F6 sm:w-1\u002F2\"\n\t\t src=\"variational-inference-1\u002Fbayes_rule.png\"\n\t\t alt=\"Bayes rule\"\u003E\n\u003C\u002Ffigure\u003E\n\u003Cp class=\"post_p\"\u003E\n\tThe following explains what is each term of the formula, being x and &theta; data and model\n\tparameters respectively:\n\u003C\u002Fp\u003E\n\u003Cul class=\"post_list\"\u003E\n\t\u003Cli\u003E\n\t\t\u003Cstrong\u003EPosterior p(&theta;|x)\u003C\u002Fstrong\u003E: It is the probability of data, the probability that the model with &theta;\n\t\tparameters has generated x data.\n\t\u003C\u002Fli\u003E\n\t\u003Cli\u003E\n\t\t\u003Cstrong\u003ELikelihood p(x|&theta;)\u003C\u002Fstrong\u003E: It is the probability of data assuming that are modeled by a\n\t\tparametrized distribution. The way to calculate it depends on the model. Usually it is used to assess\n\t\tthe quality of a model.\n\t\u003C\u002Fli\u003E\n\t\u003Cli\u003E\n\t\t\u003Cstrong\u003EPrior p(&theta;)\u003C\u002Fstrong\u003E: It is the probability of the parameters. In this factor of the formula\n\t\tis where prior knowledge is reflected, information that we have before to observe any data\n\t\tobservation.\n\t\u003C\u002Fli\u003E\n\t\u003Cli\u003E\n\t\t\u003Cstrong\u003EEvidence p(x)\u003C\u002Fstrong\u003E: This is the evidence from data. It is calculated as &int; p(x,&theta;)d&theta;. Usually it\n\t\tcan't be calculated but, as it is a normalizing constant of the model, it does not affect. When two\n\t\tprobabilistic models are compared, important factors are the ones which depend on Î¸ because p(x) will be\n\t\tthe same for each model because it just depends on data.\n\t\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Cp class=\"post_p\"\u003E\n\tProduct p(x|Î¸)p(Î¸) is also known as joint probability: p(Î¸,x).\n\u003C\u002Fp\u003E\n\u003Cp class=\"post_p\"\u003E\n\tUse the likelihood to estimate Î¸ parameters is known as Maximum Likelihood Estimation (MLE) while if you take\n\tthe prior into account then is known as Maximum A Posteriori estimation (MAP). MLE and MAP are equivalents if\n\tthere is an uniform prior. However these methods only allow estimating a mean, a median and a mode of the\n\tposterior and maybe your goal requires to model uncertainty or to generate new data. In theses cases we would need\n\tto know posterior distribution. As we will see, methods as Variational Inference (VI) or Markov Chain Monte Carlo\n\t(MCMC) allow to infer this distribution. Taking into account the Bayes normalizer constant p(x) is what allows\n\tthese methods to calculate a posterior distribution.\n\u003C\u002Fp\u003E\n\u003Cp class=\"post_p\"\u003E\n\tA summary of this formula would be: At the beginning we have a belief (prior) about an event Î¸ (p(Î¸)), for example,\n\tthat the height of Barcelona population is described by a Normal distribution. After we observe evidences (x),\n\ta sample of the heights of Barcelona population. With this evidence our belief about Î¸ has to be updated, in\n\tother words, Normal distribution which described the height of Barcelona population has to be updated. This\n\tchange is reflected by the posterior distribution (p(Î¸|x)). In this example we can appreciate the support to\n\tonline learning that offers this kind of models.\n\u003C\u002Fp\u003E\n\u003Cfigure class=\"post_figure flex justify-center\"\u003E\n\t\u003Cimg class=\"post_image\"\n\t\t src=\"variational-inference-1\u002Fonline_learning.png\"\n\t\t alt=\"Online learning\"\u003E\n\u003C\u002Ffigure\u003E\n\u003Cp class=\"post_p\"\u003E\n\tAt the end it is an iterative process of updating beliefs (prior) based on evidences (x) where posterior of one\n\titeration will be the prior of next one.\n\u003C\u002Fp\u003E\n\u003Cp class=\"post_p\"\u003E\n\tPosterior inference algorithms avoid to analyze information under certain assumptions (priors) discovering a\n\thidden structure which best fits with our data. When all relations between model random variable are\n\t\u003Ca href=\"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FConjugate_prior\" class=\"post_link\" target=\"_blank\"\u003Econjugated\u003C\u002Fa\u003E, this is, when joint\n\tdistribution has the same form as the prior, posterior can be calculated analytically. This is the\n\tcase of models like Dirichlet-Categorical o Normal Inverse Wishart-Normal. In the opposite case, the problem of\n\tthis formula resides in the calculation of the evidence (p(x)). For many models of interest (no conjugated) to\n\tcalculate a posterior is computationally intractable because the integral over all latent variables of data\n\tthat requires to calculate the evidence. For these models, another strategy is required to obtain the posterior,\n\tfor that reason the calculation of the posterior becomes an approximation problem.\n\u003C\u002Fp\u003E\n\u003Ch5\u003EPosterior approximation\u003C\u002Fh5\u003E\n\u003Cp class=\"post_p\"\u003E\n\tProbabilistic machine learning uses latent variables to describe data hidden structure, some relations\n\tbetween observed and latent variables are modeled using probability distributions and inference algorithms are used\n\tto estimate the posterior, that is the conditional distribution about latent variables given the observed\n\tvariables. Due to the fact that in most cases we are working in spaces with many dimensions, the calculation of\n\tposterior expectation, E(p(Î¸|x)), is impossible to obtain analytically and computationally, for this reason some\n\tinference methods have been created to approximate this distribution. Bayesian inference concept comes from the\n\tset of tools which have been developed to approximate this posterior and it is one of the central problems in\n\tbayesian statistics. Nowadays there are 2 algorithmic branches:\n\u003C\u002Fp\u003E\n\u003Cul class=\"post_list\"\u003E\n\t\u003Cli\u003E\n\t\t\u003Ca href=\"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FMarkov_chain_Monte_Carlo\" class=\"post_link\" target=\"_blank\"\u003E\n\t\t\tMarkov Chain Monte Carlo (MCMC)\n\t\t\u003C\u002Fa\u003E:\n\t\tSampling approximate inference.\n\t\u003C\u002Fli\u003E\n\t\u003Cli\u003E\n\t\t\u003Ca href=\"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FVariational_Bayesian_methods\" class=\"post_link\" target=\"_blank\"\u003E\n\t\t\tVariational Inference (VI)\n\t\t\u003C\u002Fa\u003E:\n\t\tStructural approximate inference.\n\t\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Cp class=\"post_p\"\u003E\n\tOn the one hand, MCMC is based on the construction of a Markov chain over all latent variables being the posterior\n\tits stationary distribution. After the chain is executed till it arrives to its equilibrium point.\n\tFinally, results obtained sampling the Markov chain in its stationary section, are the posterior samples. The\n\tbest known algorithms of this family are\n\t\u003Ca href=\"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FMetropolis%E2%80%93Hastings_algorithm\" class=\"post_link\" target=\"_blank\"\u003EMetropolisâ€“Hastings\u003C\u002Fa\u003E\n\t, \u003Ca href=\"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FGibbs_sampling\" class=\"post_link\" target=\"_blank\"\u003EGibbs sampling\u003C\u002Fa\u003E and\n\t\u003Ca href=\"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FHamiltonian_Monte_Carlo\"\u003EHamiltonian Monte Carlo\u003C\u002Fa\u003E.\n\u003C\u002Fp\u003E\n\u003Cp class=\"post_p\"\u003E\n\tOn the other hand, Variational Inference approximates the posterior creating an analytical approximation, the\n\tvariational model, which is adjusted in order to reduce the distance with the posterior. In this family the\n\tproblem is transformed from an approximation one to an optimization one.\n\u003C\u002Fp\u003E\n\u003Ch5\u003EProbabilistic Graphical Models\u003C\u002Fh5\u003E\n\u003Cp class=\"post_p\"\u003E\n\tIn the bayesian field, a model represents a joint probability over all random variables of the problem.\n\u003C\u002Fp\u003E\n\u003Cfigure class=\"post_figure flex justify-center\"\u003E\n\t\u003Cimg class=\"post_image md:w-1\u002F5 sm:w-4\u002F5\"\n\t\t src=\"variational-inference-1\u002Fjoint_dist.png\"\n\t\t alt=\"Joint distribution\"\u003E\n\u003C\u002Ffigure\u003E\n\u003Cp class=\"post_p\"\u003E\n\tA very practical way of representing these models is using\n\t\u003Ca href=\"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FGraphical_model\" class=\"post_link\" target=\"_blank\"\u003Eprobabilistic graphical models\u003C\u002Fa\u003E. A\n\tprobabilistic graphical model is a directed graph where nodes are random variables and edges are dependency\n\trelations between those variables. For example, joint distribution probabilistic graphical model is:\n\u003C\u002Fp\u003E\n\u003Cfigure class=\"post_figure flex justify-center\"\u003E\n\t\u003Cimg class=\"post_image md:w-1\u002F4 sm:w-64\"\n\t\t src=\"variational-inference-1\u002Fjoint_dist2.png\"\n\t\t alt=\"Joint distribution\"\u003E\n\u003C\u002Ffigure\u003E\n\u003Cfigure class=\"post_figure flex justify-center\"\u003E\n\t\u003Cimg class=\"post_image\"\n\t\t src=\"variational-inference-1\u002Fgraphical_model.png\"\n\t\t alt=\"Joint distribution\"\u003E\n\u003C\u002Ffigure\u003E\n\u003Cp class=\"post_p\"\u003E\n\tAt this context, p(x|y) represents the conditional probability in which \u003Ci\u003Ex\u003C\u002Fi\u003E variable depends on the\n\tvalue of y. At this kind of diagrams also exists another components called plates.\n\u003C\u002Fp\u003E\n\u003Cfigure class=\"post_figure flex justify-center\"\u003E\n\t\u003Cimg class=\"post_image\"\n\t\t src=\"variational-inference-1\u002Fplate.png\"\n\t\t alt=\"Plate\"\u003E\n\u003C\u002Ffigure\u003E\n\u003Cp class=\"post_p\"\u003E\n\tThis notation indicates a vector of n random variables \u003Ci\u003Ex\u003C\u002Fi\u003E.\n\u003C\u002Fp\u003E\n\u003Ch5\u003ELocal and global variables\u003C\u002Fh5\u003E\n\u003Cp class=\"post_p\"\u003E\n\tIn a probabilistic model two types of random variables can be distinguished: globals and locals. A global variable\n\tis the one which is shared between all dataset examples while a local variable is owned by each example. For\n\texample, the following probabilistic graphical model, \u003Ci\u003Ey\u003C\u002Fi\u003E variable is local while \u003Ci\u003Ez\u003C\u002Fi\u003E variable is a\n\tglobal one. When a node appears obscured it means that it is an observed variable.\n\u003C\u002Fp\u003E\n\u003Cfigure class=\"post_figure flex justify-center\"\u003E\n\t\u003Cimg class=\"post_image\"\n\t\t src=\"variational-inference-1\u002Fglobal_local_variables.png\"\n\t\t alt=\"Model example\"\u003E\n\u003C\u002Ffigure\u003E\n\u003Ch4\u003EVariational inference\u003C\u002Fh4\u003E\n\u003Cp class=\"post_p\"\u003E\n\tIn next posts we will focus on the different variational strategies for posterior approach.\n\u003C\u002Fp\u003E\n\u003Ch5\u003EStrategy\u003C\u002Fh5\u003E\n\u003Cp class=\"post_p\"\u003E\n\tAs already mentioned, Variational Inference consists in defining a distribution, q(&theta;|&lambda;), whose parameters &lambda;\n\twill be optimized in order to reduce the differences with the posterior p(&theta;|x). This new distribution is known as\n\tvariational model and the posterior as probabilistic model. To summarize, VI goal is to optimize variational model\n\t&lambda; parameters in order to reduce the distance with probabilistic model p(&theta;|x). &lambda; parameters are known also as\n\tvariational parameters.\n\u003C\u002Fp\u003E\n\u003Cfigure class=\"post_figure flex justify-center\"\u003E\n\t\u003Cimg class=\"post_image\"\n\t\t src=\"variational-inference-1\u002Fvariational_inference.jpg\"\n\t\t alt=\"Variational inference\"\u003E\n\u003C\u002Ffigure\u003E\n\u003Ch5\u003EKullback-Leibler divergence\u003C\u002Fh5\u003E\n\u003Cp class=\"post_p\"\u003E\n\tTo calculate Euclidean distance between distribution parameters to establish the similarity between both is an\n\timperfect measure since we are comparing distributions and not points. Imagine a Normal distribution with 0 mean\n\tand 5 variance, N(0, 5), and another one with 5 mean and 5 variance, N(5, 5). These two distributions are very\n\tsimilar but they are separated by an Euclidean distance of 5. If now we compare first defined Normal distribution\n\tN(0, 5) with another Normal distribution with 2 mean and 7 variance, N(2, 7), looks like they are more different\n\tbut Euclidean distance that separates them is 4. For this reason we have to use another measure: Kullback-Leibler\n\tdivergence (KL).\n\u003C\u002Fp\u003E\n\u003Cp class=\"post_p\"\u003E\n\tKL is a divergence, in other words, a non-simmetric distance, it isn't the same to calculate KL[p(Î¸|x)||q(Î¸|Î»)]\n\t(forwards KL) than KL[q(Î¸|Î»)||p(Î¸|x)] (reverse KL). The fact of use one or another rises to different algorithms:\n\t \u003Ca href=\"https:\u002F\u002Ftminka.github.io\u002Fpapers\u002Fep\u002Fminka-ep-uai.pdf\" class=\"post_link\" target=\"_blank\"\u003EExpectation Propagation\u003C\u002Fa\u003E uses\n\tforwards KL while VI uses reverse KL. In general terms, Expectation Propagation is more computationally expensive.\n\tKL quantifies loss information when you approximate one distribution with another. It is based on the concept of\n\tentropy. Entropy measures the quantity of information that own data and is defined as follows:\n\u003C\u002Fp\u003E\n\u003Cfigure class=\"post_figure flex justify-center\"\u003E\n\t\u003Cimg class=\"post_image\"\n\t\t src=\"variational-inference-1\u002Fentropy.png\"\n\t\t alt=\"Entropy\"\u003E\n\u003C\u002Ffigure\u003E\n\u003Cp class=\"post_p\"\u003E\n\tKL definition consists in modify entropy formula to take q distribution into account.\n\u003C\u002Fp\u003E\n\u003Cfigure class=\"post_figure flex justify-center\"\u003E\n\t\u003Cimg class=\"post_image\"\n\t\t src=\"variational-inference-1\u002Fkl1.png\"\n\t\t alt=\"Kullback-Leibler divergence\"\u003E\n\u003C\u002Ffigure\u003E\n\u003Cp class=\"post_p\"\u003E\n\tIf we adapt this form to the VI problem and we apply logarithms properties we have:\n\u003C\u002Fp\u003E\n\u003Cfigure class=\"post_figure flex justify-center\"\u003E\n\t\u003Cimg class=\"post_image md:w-1\u002F3 sm:4\u002F5\"\n\t\t src=\"variational-inference-1\u002Fkl_rule.png\"\n\t\t alt=\"Kullback-Leibler definition\"\u003E\n\u003C\u002Ffigure\u003E\n\u003Cp class=\"post_p\"\u003E\n\tThis divergence avoids us to find the real similarity between two probability distributions and it is the measure\n\tthat is minimized at VI algorithm.\n\u003C\u002Fp\u003E\n\u003Cfigure class=\"post_figure flex justify-center\"\u003E\n\t\u003Cimg class=\"post_image\"\n\t\t src=\"variational-inference-1\u002Fkullback-leibler1.png\"\n\t\t alt=\"Kullback-Leibler divergence\"\u003E\n\u003C\u002Ffigure\u003E\n\u003Cfigcaption class=\"post_image_figcaption\"\u003E\n\tForwards and reverse KL comparison to approximate a bimodal distribution. Blue part represents distribution\n\tto be approximated and red one the approximation. \u003Cstrong\u003Ea\u003C\u002Fstrong\u003E image is an approximation\n\twith forwards KL and \u003Cstrong\u003Eb\u003C\u002Fstrong\u003E and \u003Cstrong\u003Ec\u003C\u002Fstrong\u003E are approximation with reverse KL.\n\u003C\u002Ffigcaption\u003E\n\u003Cfigure class=\"post_figure flex justify-center\"\u003E\n\t\u003Cimg class=\"post_image\"\n\t\t src=\"variational-inference-1\u002Fkullback-leibler2.png\"\n\t\t alt=\"Kullback-Leibler visualization\"\u003E\n\u003C\u002Ffigure\u003E\n\u003Cfigcaption class=\"post_image_figcaption\"\u003E\n\tForwards and reverse KL comparison to approximate a unimodal distribution. Blue part represents distribution\n\tto be approximated and red one the approximation. \u003Cstrong\u003Ea\u003C\u002Fstrong\u003E image is an approximation\n\twith forwards KL and \u003Cstrong\u003Eb\u003C\u002Fstrong\u003E is an approximation with reverse KL.\n\u003C\u002Ffigcaption\u003E\n\u003Ch5\u003EMean-Field assumption\u003C\u002Fh5\u003E\n\u003Cp class=\"post_p\"\u003E\n\tIn order to define a treatable distribution over all latent variables to approximate the posterior we can\n\tsimplify variational model optimization assuming that it is a factorized model. It is to suppose that q(&theta;|&lambda;) is\n\tcomposed by a set of distributions q\u003Csub\u003Ei\u003C\u002Fsub\u003E(&theta;\u003Csub\u003Ei\u003C\u002Fsub\u003E|&lambda;\u003Csub\u003Ei\u003C\u002Fsub\u003E) (of the\n\t\u003Ca href=\"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FExponential_family\" class=\"post_link\" target=\"_blank\"\u003EExponential Family\u003C\u002Fa\u003E). Each one of\n\tthese distributions has its own parameters &lambda;\u003Csub\u003Ei\u003C\u002Fsub\u003E which could be optimized individually.\n\u003C\u002Fp\u003E\n\u003Cfigure class=\"post_figure flex justify-center\"\u003E\n\t\u003Cimg class=\"post_image md:w-1\u002F5 sm:w-4\u002F5\"\n\t\t src=\"variational-inference-1\u002Fmean_field.png\"\n\t\t alt=\"Mean-Field assumption\"\u003E\n\u003C\u002Ffigure\u003E\n\u003Cp class=\"post_p\"\u003E\n\tMain goal of this post is not more than to present the basic idea of Variational Inference and its main players.\n\tIn following posts we will go deeper into these algorithms and programming tools to code this models will be\n\tpresented.\n\u003C\u002Fp\u003E\n\u003Ch4\u003EReferences\u003C\u002Fh4\u003E\n\u003Cul class=\"post_list\"\u003E\n\t\u003Cli\u003E\n\t\tJournal of the American Statistical AssociationGeorge (E. P. Box)\n\t\u003C\u002Fli\u003E\n\t\u003Cli\u003E\n\t\tBuild, Compute, Critique, Repeat: Data Analysis with Latent Variable Models\n\t\t(David M. Blei)\n\t\u003C\u002Fli\u003E\n\t\u003Cli\u003E\n\t\tProbabilistic graphical models: principles and techniques\n\t\t(Koller, Daphne, and Nir Friedman)\n\t\u003C\u002Fli\u003E\n\t\u003Cli\u003E\n\t\tModel-based Machine Learning\n\t\t(Christopher M. Bishop)\n\t\u003C\u002Fli\u003E\n\t\u003Cli\u003E\n\t\tMachine Learning. A probabilistic perspective\n\t\t(Kevin P. Murphy)\n\t\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\t\t"},comments_url:"https:\u002F\u002Fydp2l1f32a.execute-api.eu-west-1.amazonaws.com\u002Fprod\u002Fcomments\u002Fvariational-inference-1"}]};if('serviceWorker' in navigator)navigator.serviceWorker.register('/service-worker.js');(function(){try{eval("async function x(){}");var main="/client/client.0f72c0be.js"}catch(e){main="/client/legacy/client.c03197aa.js"};var s=document.createElement("script");try{new Function("if(0)import('')")();s.src=main;s.type="module";s.crossOrigin="use-credentials";}catch(e){s.src="/client/shimport@1.0.1.js";s.setAttribute("data-main",main);}document.head.appendChild(s);}());</script> 