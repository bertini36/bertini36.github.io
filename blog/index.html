<!doctype html> <html lang=en> <head> <meta charset=utf-8> <meta content="width=device-width,initial-scale=1" name=viewport> <meta content=#333333 name=theme-color> <base href=/ > <link href=https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css rel=stylesheet> <link href=global.css rel=stylesheet> <link href=manifest.json rel=manifest crossorigin=use-credentials> <link href=black_logo.png rel=icon type=image/png> <link href=client/main.419280361.css rel=stylesheet> <noscript id=sapper-head-start></noscript><title>Alberto Pou - Blog</title><meta content="All blog posts of Alberto Pou" name=description><noscript id=sapper-head-end></noscript> </head> <body> <div id=sapper> <nav class="svelte-71l2nh border-b-2 font-light px-4 py-0"><ul class="svelte-71l2nh m-0 p-0"><li class=float-left><a href=. class="hover:text-red-500 px-3 block no-underline py-4 svelte-71l2nh">Home</a></li> <li class=float-left><a href=blog class="hover:text-red-500 px-3 block no-underline py-4 svelte-71l2nh" rel=prefetch aria-current=page>Blog</a></li> <li class=float-right><a href=https://github.com/bertini36/blogv2 class="hover:text-red-500 px-3 block no-underline py-4 text-black" target=_blank><i class="fa fa-github"></i></a></ul></nav> <main class=svelte-1uhnsl8> <div class=posts><div class=mb-8><div class=mb-4><span class="text-base text-gray-600">#svelte</span> <span class="text-base text-gray-600">#sapper</span> <span class="text-base text-gray-600">#serverless</span> <span class="text-base text-gray-600">#python</span> <span class="text-base text-gray-600">#javascript</span> <h1 class="hover:text-red-500 mb-0 pb-0 text-3xl"><a href=blog/recoding-my-blog rel=prefetch hreflang=en>Recoding my blog ⛩</a></h1> <span class="text-base text-gray-600">a month ago</span></div> <p> <p class=post_p> During quarantine I had more free time than usual so I decided it was time to modernize my <a href=https://albertopou.dev/ class=post_link target=_blank>blog</a>. I coded the first version 5 years ago choosing technologies simply because I knew them. That resulted in a <a href=https://www.djangoproject.com/ class=post_link target=_blank>Django</a> app with...<p></p> <div class=mt-2><a href=blog/recoding-my-blog class="hover:text-red-500 px-3 border border-black hover:border-red-500 py-1" rel=prefetch hreflang=en>Continue reading »</a></div> </div><div class=mb-8><div class=mb-4><span class="text-base text-gray-600">#python</span> <span class="text-base text-gray-600">#twitter</span> <span class="text-base text-gray-600">#text-mining</span> <span class="text-base text-gray-600">#machine-learning</span> <span class="text-base text-gray-600">#lda</span> <h1 class="hover:text-red-500 mb-0 pb-0 text-3xl"><a href=blog/profiling-on-social-networks rel=prefetch hreflang=en>Profiling on social networks 🤹‍♂️</a></h1> <span class="text-base text-gray-600">a year ago</span></div> <p> <p class=post_p> Last weekend I was coding an application for Twitter timelines analysis which I called <a href=https://github.com/bertini36/profiler class=post_link target=_blank>Profiler</a> (I was just bored). Some years ago I was working on probabilistic models and there was one which got my attention: ...<p></p> <div class=mt-2><a href=blog/profiling-on-social-networks class="hover:text-red-500 px-3 border border-black hover:border-red-500 py-1" rel=prefetch hreflang=en>Continue reading »</a></div> </div><div class=mb-8><div class=mb-4><span class="text-base text-gray-600">#python</span> <span class="text-base text-gray-600">#tensorflow</span> <span class="text-base text-gray-600">#autograd</span> <span class="text-base text-gray-600">#linear-regression</span> <h1 class="hover:text-red-500 mb-0 pb-0 text-3xl"><a href=blog/automatic-differentiation rel=prefetch hreflang=en>Automatic differentiation 📈</a></h1> <span class="text-base text-gray-600">3 years ago</span></div> <p> <p class=post_p> Derivatives, specifically gradients (derivatives in more than one dimension spaces) and hesians (second derivatives), have become in a fundamental of machine learning. Gradient is a vector which indicates the maximum slope direction of a function at the evaluated point. This is important to move around the function space to find relative minimums or maximums (to...<p></p> <div class=mt-2><a href=blog/automatic-differentiation class="hover:text-red-500 px-3 border border-black hover:border-red-500 py-1" rel=prefetch hreflang=en>Continue reading »</a></div> </div><div class=mb-8><div class=mb-4><span class="text-base text-gray-600">#CUDA</span> <span class="text-base text-gray-600">#MPI</span> <span class="text-base text-gray-600">#C</span> <span class="text-base text-gray-600">#maths</span> <h1 class="hover:text-red-500 mb-0 pb-0 text-3xl"><a href=blog/distributed-matrix-product rel=prefetch hreflang=en>Distributed matrix product 🔢</a></h1> <span class="text-base text-gray-600">4 years ago</span></div> <p> <p class=post_p> Nowadays one of the main problems of any algorithm is its facility to scale it, in other words, its facility to be executed over multiple cores (parallelization) or nodes (distribution). The amount of data that is available today has led the world of computing to develop technologies with which to parallelize and distribute processes are done more easily and...<p></p> <div class=mt-2><a href=blog/distributed-matrix-product class="hover:text-red-500 px-3 border border-black hover:border-red-500 py-1" rel=prefetch hreflang=en>Continue reading »</a></div> </div><div class=mb-8><div class=mb-4><span class="text-base text-gray-600">#probabilistic-machine-learning</span> <span class="text-base text-gray-600">#variational-inference</span> <span class="text-base text-gray-600">#statistics</span> <h1 class="hover:text-red-500 mb-0 pb-0 text-3xl"><a href=blog/variational-inference-3 rel=prefetch hreflang=en>Variational Inference III 🤖🎯</a></h1> <span class="text-base text-gray-600">3 years ago</span></div> <p> <p class=post_p> In this post I'm going to continue explaining concepts that I introduced in previous posts (<a href=https://albertopou.dev/blog/variational-inference-1/ class=post_link target=_blank>1</a> and <a href=https://albertopou.dev/blog/variational-inference-2/ class=post_link target=_blank>2</a>) about Variational inference (VI). Algorithms presented till now have...<p></p> <div class=mt-2><a href=blog/variational-inference-3 class="hover:text-red-500 px-3 border border-black hover:border-red-500 py-1" rel=prefetch hreflang=en>Continue reading »</a></div> </div><div class=mb-8><div class=mb-4><span class="text-base text-gray-600">#probabilistic-machine-learning</span> <span class="text-base text-gray-600">#variational-inference</span> <span class="text-base text-gray-600">#statistics</span> <h1 class="hover:text-red-500 mb-0 pb-0 text-3xl"><a href=blog/variational-inference-2 rel=prefetch hreflang=en>Variational Inference II 🤖🎯</a></h1> <span class="text-base text-gray-600">3 years ago</span></div> <p> <p class=post_p> In this post I'm going to continue explaining concepts that I introduced in my <a href=https://albertopou.dev/blog/variational-inference-1 class=post_link target=_blank>previous post</a> about Variational Inference (VI). At first I'm going to show that to find the best posterior approach it's necessary to minimize the Kullback-Leibler divergence (KL) between...<p></p> <div class=mt-2><a href=blog/variational-inference-2 class="hover:text-red-500 px-3 border border-black hover:border-red-500 py-1" rel=prefetch hreflang=en>Continue reading »</a></div> </div><div class=mb-8><div class=mb-4><span class="text-base text-gray-600">#probabilistic-machine-learning</span> <span class="text-base text-gray-600">#variational-inference</span> <span class="text-base text-gray-600">#statistics</span> <h1 class="hover:text-red-500 mb-0 pb-0 text-3xl"><a href=blog/variational-inference-1 rel=prefetch hreflang=en>Variational Inference I 🤖🎯</a></h1> <span class="text-base text-gray-600">4 years ago</span></div> <p> <p class=post_p> In next posts I'm going to speak about probabilistic machine learning. Last months I was learning about this field, specifically about a type of inference on these models known as Variational Inference, and I reckon can be interesting to write a set of posts summarizing my experience about it thanks to my teacher ...<p></p> <div class=mt-2><a href=blog/variational-inference-1 class="hover:text-red-500 px-3 border border-black hover:border-red-500 py-1" rel=prefetch hreflang=en>Continue reading »</a></div> </div></div></main></div> <script>__SAPPER__={baseUrl:"",preloaded:[void 0,(function(a,b,c,d){return {posts:[{title:"Recoding my blog ⛩",slug:"recoding-my-blog",tags:["#svelte","#sapper","#serverless",a,"#javascript"],date:"2020-07-12",html:"\n\u003Cp class=\"post_p\"\u003E\n\tDuring quarantine I had more free time than usual so I decided it was time\n\tto modernize my \u003Ca href=\"https:\u002F\u002Falbertopou.dev\u002F\" class=\"post_link\" target=\"_blank\"\u003Eblog\u003C\u002Fa\u003E.\n\tI coded the first version 5 years ago choosing technologies\n\tsimply because I knew them. That resulted in a \u003Ca href=\"https:\u002F\u002Fwww.djangoproject.com\u002F\" class=\"post_link\" target=\"_blank\"\u003EDjango\u003C\u002Fa\u003E\n\t app with a \u003Ca href=\"https:\u002F\u002Fwww.postgresql.org\u002F\" class=\"post_link\" target=\"_blank\"\u003EPostgres\u003C\u002Fa\u003E database \n\t and an outdated design using \u003Ca href=\"https:\u002F\u002Fgetbootstrap.com\u002F\" class=\"post_link\" target=\"_blank\"\u003EBootstrap\u003C\u002Fa\u003E.  \n\tAfter the years I used to use my blog to test new technologies so I had versions \n\tof my blog using \u003Ca href=\"https:\u002F\u002Fjquery.com\u002F\" class=\"post_link\" target=\"_blank\"\u003EJQuery\u003C\u002Fa\u003E,\n\t\u003Ca href=\"https:\u002F\u002Fvuejs.org\u002F\" class=\"post_link\" target=\"_blank\"\u003EVue\u003C\u002Fa\u003E, Django, Django 2.0 \n\tusing a DDD approach, ... and it was deployed on \u003Ca href=\"https:\u002F\u002Fwww.digitalocean.com\u002F\" class=\"post_link\" target=\"_blank\"\u003EDigital Ocean\u003C\u002Fa\u003E,\n\t\u003Ca href=\"https:\u002F\u002Faws.amazon.com\u002F\" class=\"post_link\" target=\"_blank\"\u003EAWS\u003C\u002Fa\u003E,\n\t\u003Ca href=\"https:\u002F\u002Fwww.heroku.com\u002F\" class=\"post_link\" target=\"_blank\"\u003EHeroku\u003C\u002Fa\u003E, \n\tHeroku using \u003Ca href=\"https:\u002F\u002Fwww.terraform.io\u002F\" class=\"post_link\" target=\"_blank\"\u003ETerraform\u003C\u002Fa\u003E, ... \n\tAnyone who saw the code could say that a blog should never be so over-engineer, and I agree. For this \n\treason I decided to check which technologies were being used nowadays to make simple blogs.  \n\u003C\u002Fp\u003E\n\u003Cp class=\"post_p\"\u003E\n\tMy goal was to make a simple, fast and modern blog which allows me to write posts easily, \n\tdeploy them on \u003Ca href=\"https:\u002F\u002Fpages.github.com\u002F\" class=\"post_link\" target=\"_blank\"\u003EGithub Pages\u003C\u002Fa\u003E \n\tand, of course, spend as little time as possible on maintenance. After check several technologies \n\ta co-worker told me about \u003Ca href=\"https:\u002F\u002Fsvelte.dev\u002F\" class=\"post_link\" target=\"_blank\"\u003ESvelte\u003C\u002Fa\u003E\n\tand I loved it. Svelte and its server-rendering backend \u003Ca href=\"https:\u002F\u002Fsapper.svelte.dev\u002F\" class=\"post_link\" target=\"_blank\"\u003ESapper\u003C\u002Fa\u003E\n\twere what I was looking for and there were a lot of examples of blogs made with these technologies.  \n\tAfter that I just required a new design. I was tired about Bootstrap designs so I was looking \n\tfor something new, and then I found \u003Ca href=\"https:\u002F\u002Ftailwindcss.com\u002F\" class=\"post_link\" target=\"_blank\"\u003ETailwind\u003C\u002Fa\u003E. \n\tTailwind had all that I needed. A framework which, after memorize some easy classes, allows you not to write CSS. Maybe for \n\tyou this is not a plus feature but for me, a backend coder, it was perfect. \n\u003C\u002Fp\u003E\n\u003Ch4 class=\"post_section_title text-2xl text-bold\"\u003EThe good parts of this architecture\u003C\u002Fh4\u003E\n\u003Cp class=\"post_p\"\u003E\n\tAfter this introduction I want to list the advantages that I found making my \n\tblog with Svelte, Sapper and Tailwind. Remember that this benefits I found exist in \n\tthe case of study of a simple blog. \n\u003C\u002Fp\u003E\n\u003Cul class=\"post_list\"\u003E\n\t\u003Cli\u003E\n\t\t\u003Cstrong\u003EEasy installation\u003C\u002Fstrong\u003E. Sapper has a great initial template and several pre-configured commands\n\t\twhich allows you to focus on the important parts of your project. \n\t\u003C\u002Fli\u003E\n\t\u003Cli\u003E\n\t\t\u003Cstrong\u003EGreat development environment\u003C\u002Fstrong\u003E. Live reload! When you modify some file, the navigator\n\t\treloads the page automatically. Maybe I'm overestimating this feature but I work everyday in an outdated\n\t\tfrontend and this feature drove me crazy!\n\t\u003C\u002Fli\u003E\n\t\u003Cli\u003E\n\t\t\u003Cstrong\u003EStatic page\u003C\u002Fstrong\u003E. Sapper allows you to export your project as a static site. \n\t\tThen it can be hosted and served as static files, which allows it to be deployed on hosting\n\t\tenvironments such as Github Pages. To serve the page as static files also improves\n\t\tthe speed notoriously, obtaining better results in web positioning. You can use apps as \n\t\t\u003Ca href=\"https:\u002F\u002Fdevelopers.google.com\u002Fweb\u002Ftools\u002Flighthouse\" class=\"post_link\" target=\"_blank\"\u003ELighthouse\u003C\u002Fa\u003E to check this.\n\t\u003C\u002Fli\u003E\n\t\u003Cli\u003E\n\t\t\u003Cstrong\u003EElegant syntax\u003C\u002Fstrong\u003E. I worked with different frontend technologies such as JQuery, \n\t\t\u003Ca href=\"https:\u002F\u002Fknockoutjs.com\u002F\" class=\"post_link\" target=\"_blank\"\u003EKnockout\u003C\u002Fa\u003E, \n\t\t \u003Ca href=\"https:\u002F\u002Fangular.io\u002F\" class=\"post_link\" target=\"_blank\"\u003EAngular\u003C\u002Fa\u003E or Vue \n\t\tand Svelte syntax looks perfect for me. It's easy to understand, simple and elegant. In the following section you'll \n\t\tbe able to check it in a easy example I'll show you.  \n\t\u003C\u002Fli\u003E\n\t\u003Cli\u003E\n\t\t\u003Cstrong\u003EComponent structure\u003C\u002Fstrong\u003E. One thing normally I don't like in a frontend based project is \n\t\tthe way how developers structure the code. Last years I was working in projects where component HTML, Javascript and\n\t\tspecific CSS were in different folders and then developers loose to much time searching the \n\t\tJavascript or de CSS associated to a particular template. Svelte joins all in one file (.svelte) and\n\t\tin my opinion is a great solution to improve the components reusability.\n\t\u003C\u002Fli\u003E\n\t\u003Cli\u003E\n\t\t\u003Cstrong\u003ESpeed\u003C\u002Fstrong\u003E. Svelte is based on reactivity. To understand easily which reasons \n\t\tallows Svelte to get such good results in terms of speed compared to other frameworks you can watch this \n\t\t\u003Ca href=\"https:\u002F\u002Fwww.youtube.com\u002Fwatch?v=gJ2P6hGwcgo\" class=\"post_link\" target=\"_blank\"\u003Egreat conference\u003C\u002Fa\u003E\n\t\tof its creator Rich Harris.\n\t\u003C\u002Fli\u003E\n\t\u003Cli\u003E\n\t\t\u003Cstrong\u003ENo CSS\u003C\u002Fstrong\u003E. With Tailwind you don't need to write CSS code. With its pre-defined classes\n\t\tyou can make pretty much anything.\n\t\u003C\u002Fli\u003E\n\t\u003Cli\u003E\n\t\t\u003Cstrong\u003EEasy personalization\u003C\u002Fstrong\u003E. At Tailwind config file you can configure stuff as main \n\t\tcolors, default spacing, text font, ... and using @apply CSS sentence you can create your own classes easily.\n\t\u003C\u002Fli\u003E\n\t\u003Cli\u003E\n\t\t\u003Cstrong\u003E0 costs\u003C\u002Fstrong\u003E. With this architecture I was able to have my blog with 0 costs (I just have to pay the domain). \n\t\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Cp class=\"post_p\"\u003E\n\tNow I'm going to show\n\tyou how I made the comments engine using \u003Ca href=\"https:\u002F\u002Fwww.serverless.com\u002F\" class=\"post_link\" target=\"_blank\"\u003EServerless\u003C\u002Fa\u003E and Svelte\n\tas an example of use of these frameworks.\n\u003C\u002Fp\u003E\n\u003Ch4 class=\"post_section_title text-2xl text-bold\"\u003EPersistent storage for comments\u003C\u002Fh4\u003E\n\u003Cp class=\"post_p\"\u003E\n\tA comments engine requires a persistent storage so I needed to add something else to the blog in order to allow users to write comments.\n\tServerless is a framework that makes easier to mount a serverless infrastructure with AWS. In this case I decided\n\tto use \u003Ca href=\"https:\u002F\u002Faws.amazon.com\u002Fdynamodb\u002F\" class=\"post_link\" target=\"_blank\"\u003EDynamo DB\u003C\u002Fa\u003E\n\tto store post user comments and \u003Ca href=\"https:\u002F\u002Faws.amazon.com\u002Flambda\u002F\" class=\"post_link\" target=\"_blank\"\u003ELambda\u003C\u002Fa\u003E to get and publish comments.\n\tThis functions are not all time waiting for requests in a server (for this reason is so cheap).\n\tAWS Lambda deploys them very fast just when they are invoked by different events such as queue events, S3 changes, HTTP requests, .... \n\tIn this case the event was going to \n\tbe an HTTP request to the \u003Ca href=\"https:\u002F\u002Faws.amazon.com\u002Fapi-gateway\u002F\" class=\"post_link\" target=\"_blank\"\u003EAWS API Gateway\u003C\u002Fa\u003E.\n\u003C\u002Fp\u003E\n\u003Cp class=\"post_p\"\u003E\n\tAll this could seem very complex but with a framework as Serverless is done with a simple config file. \n\t\u003Ca href=\"https:\u002F\u002Fgithub.com\u002Fbertini36\u002Fbertini36.github.io\u002Fblob\u002Fdevelop\u002Flambdas\u002Fserverless.yml\" class=\"post_link\" target=\"_blank\"\u003EHere\u003C\u002Fa\u003E\n\tyou have the config file I wrote to have 2 lambda functions (at 2 different endpoints), one for \n\tget post comments and another to publish new ones. At this config file you can configure several things, from\n\tlanguage used and access management till requests rate limits and database resources. In this case the configuration is very simple, \n\tthe 2 lambda functions are 2 views of a simple \u003Ca href=\"https:\u002F\u002Fflask.palletsprojects.com\u002Fen\u002F1.1.x\u002F\" class=\"post_link\" target=\"_blank\"\u003EFlask\u003C\u002Fa\u003E application. \n\tThis views just get the comments or stores a new one in the database (in this case Dynamo DB, you can check the databse repository \n\t\u003Ca href=\"https:\u002F\u002Fgithub.com\u002Fbertini36\u002Fbertini36.github.io\u002Fblob\u002Fdevelop\u002Flambdas\u002Frepository.py\" class=\"post_link\" target=\"_blank\"\u003Ehere\u003C\u002Fa\u003E).\n\u003C\u002Fp\u003E\n\u003Cdiv class=\"post_code\"\u003E\n\t\u003Cpre\u003E\u003Ccode\u003E\nfrom flask import Flask, jsonify, request\nfrom flask_cors import CORS, cross_origin\n    \nfrom exceptions import RepositoryException\nfrom repository import comments_repository\n\napp = Flask(__name__)\ncors = CORS(app)\napp.config['CORS_HEADERS'] = 'Content-Type'\n    \n    \n@app.route('\u002Fcomments\u002F\u003Cstring:post_slug\u003E', methods=['GET'])\n@cross_origin()\ndef get_comments(post_slug):\n    try:\n        comments = comments_repository.get_comments(post_slug)\n        if comments:\n            return jsonify(comments), 200\n        return jsonify({'error': 'Comments not found'}), 404\n    except RepositoryException as e:\n        return jsonify({'error': str(e)}), 500\n\n    \n@app.route('\u002Fcomments\u002F\u003Cstring:post_slug\u003E', methods=['POST'])\n@cross_origin()\ndef add_comment(post_slug):\n    try:\n        comments_repository.add_comment(post_slug, request.get_json())\n        return jsonify({}), 200\n    except RepositoryException as e:\n        return jsonify({'error': str(e)}), 500\n\t\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003C\u002Fdiv\u003E\n\u003Cp class=\"post_p\"\u003E\n\tIf you need you can check the rest of the \n\t\u003Ca href=\"https:\u002F\u002Fgithub.com\u002Fbertini36\u002Fbertini36.github.io\u002Ftree\u002Fdevelop\u002Flambdas\" class=\"post_link\" target=\"_blank\"\u003Ecode\u003C\u002Fa\u003E.\n\u003C\u002Fp\u003E\n\u003Cp class=\"post_p\"\u003E\n\tNow we need a Svelte component that gets the comments of a post and publish new ones using the endpoints created with Serverless. In the following\n\tcode you can revise the component part required to get and show the comments of a post (.svelte file).\n\u003C\u002Fp\u003E\n\u003Cdiv class=\"post_code\"\u003E\n\t\u003Cpre\u003E\u003Ccode\u003E\n&lt;script&gt;\n   export let slug;\n   const comments_url = `dummy.com\u002Fprod\u002Fcomments\u002F${slug}`;\n   let comments = getComments();\n    \n   async function getComments() {\n      const response = await fetch(comments_url);\n      if (!response.ok) throw new Error(text);\n      return await response.json();\n    }\n&lt;\u002Fscript&gt;\n\n{#await comments}\n   &lt;figure class=\"flex justify-center\"\u003E&lt;img class=\"w-48\" src=\"loader.gif\" alt=\"Loader\"&gt;&lt;\u002Ffigure&gt;\n{:then comments}\n   {#each comments as comment}\n      &lt;div class=\"shadow bg-white rounded-md mb-1\" role=\"alert\"&gt;\n         &lt;div class=\"p-4 flex\"&gt;\n            &lt;div class=\"pl-2\"&gt;\n               &lt;p class=\"font-bold text-gray-700\"&gt;{comment.text}&lt;\u002Fp&gt;\n               &lt;p class=\"text-gray-600\"&gt;\n                  {comment.name} &lt;span class=\"text-gray-500 text-sm\"&gt;{comment.date}&lt;\u002Fspan&gt;\n                &lt;\u002Fp&gt;\n            &lt;\u002Fdiv&gt;\n         &lt;\u002Fdiv&gt;\n      &lt;\u002Fdiv&gt;\n   {\u002Feach}\n{:catch error}\n   &lt;p\u003ENo comments yet&lt;\u002Fp&gt;\n{\u002Fawait}\n\t\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003C\u002Fdiv\u003E\n\u003Cp class=\"post_p\"\u003E\n\tI'm using {#await} Svelte declarative to wait the fetch promise of the comments request. Svelte\n\tis going to manage the comments as soon as the promise ends and then comments will be rendered\n\tusing {#each} declarative. All classes used to give styles are Tailwind classes. If you want to see\n\tthe rest of the code to publish new comments \n\t\u003Ca href=\"https:\u002F\u002Fgithub.com\u002Fbertini36\u002Fbertini36.github.io\u002Fblob\u002Fdevelop\u002Fsrc\u002Fcomponents\u002FComments.svelte\" class=\"post_link\" target=\"_blank\"\u003Ehere\u003C\u002Fa\u003E \n\tyou have the complete component and the result online is just below!\n\u003C\u002Fp\u003E\n\u003Cp class=\"post_p\"\u003E\n\tFeel free to ask anything in the comments section!\n\u003C\u002Fp\u003E\n\t\t"},{title:"Profiling on social networks 🤹‍♂️",slug:"profiling-on-social-networks",tags:[a,"#twitter","#text-mining","#machine-learning","#lda"],date:"2019-06-22",html:"\n\u003Cp class=\"post_p\"\u003E\n\tLast weekend I was coding an application for Twitter timelines analysis which I called\n\t\u003Ca href=\"https:\u002F\u002Fgithub.com\u002Fbertini36\u002Fprofiler\" class=\"post_link\" target=\"_blank\"\u003EProfiler\u003C\u002Fa\u003E (I was just bored). Some years ago I\n\twas working on probabilistic models and there was one which got my attention:\n\t\u003Ca href=\"http:\u002F\u002Fwww.jmlr.org\u002Fpapers\u002Fvolume3\u002Fblei03a\u002Fblei03a.pdf\" class=\"post_link\" target=\"_blank\"\u003ELatent Dirichlet Allocation\u003C\u002Fa\u003E.\n\u003C\u002Fp\u003E\n\u003Cp class=\"post_p\"\u003E\n\tThis model was developed by David Blei, Andrew Ng y Michael I. Jordan and tries to find topics in document collections.\n\tIn other words, it groups text documents into topics that the model itself discovers. In this post I'm not going\n\tto explain the model structure and its inference, for this what better than to read the\n\t\u003Ca href=\"http:\u002F\u002Fwww.jmlr.org\u002Fpapers\u002Fvolume3\u002Fblei03a\u002Fblei03a.pdf\" class=\"post_link\" target=\"_blank\"\u003Epaper\u003C\u002Fa\u003E itself.\n\u003C\u002Fp\u003E\n\u003Cp class=\"post_p\"\u003E\n\tThis model is based on estimating  \u003Ca href=\"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FDirichlet_distribution\" class=\"post_link\" target=\"_blank\"\u003E\n\tDirichlet distributions\u003C\u002Fa\u003E. This kind of distributions model the probability of membership to a set of classes.\n\tSpecifically the model builds (using an iterative procedure) Dirichlet distributions to model the\n\tprobability of a word referring to a concrete topic and the probability of membership from each document to\n\teach of the topics. The parameters estimation of these distributions can be done using different types of\n\tBayesian inference like \u003Ca href=\"https:\u002F\u002Falbertopou.herokuapp.com\u002F#variational_inference_1\" class=\"post_link\" target=\"_blank\"\u003EVariational Inference\u003C\u002Fa\u003E\n\tor sampling methods as \u003Ca href=\"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FMarkov_chain_Monte_Carlo\" class=\"post_link\" target=\"_blank\"\u003EMarkov Chain Monte Carlo\u003C\u002Fa\u003E.\n\u003C\u002Fp\u003E\n\u003Cp class=\"post_p\"\u003E\n\tProfiler uses this probabilistic model to identify, given a Twitter timeline (set of user tweets), several\n\ttopics that this user writes about. From this idea I coded a Python application that downloads all these tweets of\n\ta user, stores them, preprocesses them and finally looks for its main topics.\n\u003C\u002Fp\u003E\n\u003Ch4 class=\"post_section_title text-2xl text-black\"\u003ETechnological stack\u003C\u002Fh4\u003E\n\u003Cul class=\"post_list\"\u003E\n\t\u003Cli\u003E\n\t\tTwitter data is downloaded using \u003Ca href=\"https:\u002F\u002Fgithub.com\u002Ftweepy\u002Ftweepy\" class=\"post_link\" target=\"_blank\"\u003ETweepy\u003C\u002Fa\u003E.\n\t\u003C\u002Fli\u003E\n\t\u003Cli\u003E\n\t\tTo store the timelines I used a \u003Ca href=\"https:\u002F\u002Fwww.mongodb.com\u002F\" class=\"post_link\" target=\"_blank\"\u003EMongoDB\u003C\u002Fa\u003E database with\n\t\twhich I interacted using \u003Ca href=\"https:\u002F\u002Fgithub.com\u002Fmongodb\u002Fmongo-python-driver\" class=\"post_link\" target=\"_blank\"\u003EPymongo\u003C\u002Fa\u003E.\n\t\u003C\u002Fli\u003E\n\t\u003Cli\u003E\n\t\tTextual data preprocessing was done using libraries as  \u003Ca href=\"https:\u002F\u002Fgithub.com\u002Fpandas-dev\u002Fpandas\" class=\"post_link\" target=\"_blank\"\u003EPandas\u003C\u002Fa\u003E\n\t\tand \u003Ca href=\"https:\u002F\u002Fgithub.com\u002Fnltk\u002Fnltk\" class=\"post_link\" target=\"_blank\"\u003ENLTK\u003C\u002Fa\u003E. This preprocessing consisted on cleaning\n\t\ttweets to avoid deviations (delete emoticons, capital letters, symbols, digits, ...)\n\t\u003C\u002Fli\u003E\n\t\u003Cli\u003E\n\t\tTo infer the model I used  \u003Ca href=\"https:\u002F\u002Fgithub.com\u002FRaRe-Technologies\u002Fgensim\" class=\"post_link\" target=\"_blank\"\u003EGensim\u003C\u002Fa\u003E\n\t\tlibrary. This library has a concurrent LDA implementation.\n\t\u003C\u002Fli\u003E\n\t\u003Cli\u003E\n\t\tTo plot obtained results I used \u003Ca href=\"https:\u002F\u002Fgithub.com\u002Fbmabey\u002FpyLDAvis\" class=\"post_link\" target=\"_blank\"\u003EpyLDAvis\u003C\u002Fa\u003E\n\t\tlibrary which generates interactive HTML that avoids to explore the results and check which words\n\t\tare the most important in each topic.\n\t\u003C\u002Fli\u003E\n\t\u003Cli\u003E\n\t\tI coded also a command interface to ease interact with the library. To do this I used Google\n\t\t\u003Ca href=\"https:\u002F\u002Fgithub.com\u002Fgoogle\u002Fpython-fire\" class=\"post_link\" target=\"_blank\"\u003EFire\u003C\u002Fa\u003E library.\n\t\u003C\u002Fli\u003E\n\t\u003Cli\u003E\n\t\tI configured the development environment using\n\t\t\u003Ca href=\"https:\u002F\u002Fwww.docker.com\u002F\" class=\"post_link\" target=\"_blank\"\u003EDocker\u003C\u002Fa\u003E,\n\t\t\u003Ca href=\"https:\u002F\u002Fdocs.docker.com\u002Fcompose\u002F\" class=\"post_link\" target=\"_blank\"\u003EDocker compose\u003C\u002Fa\u003E and\n\t\t\u003Ca href=\"https:\u002F\u002Ftravis-ci.org\u002F\" class=\"post_link\" target=\"_blank\"\u003ETravis\u003C\u002Fa\u003E to execute automatic tests.\n\t\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Ch4 class=\"post_section_title text-2xl text-black\"\u003ECommand interface\u003C\u002Fh4\u003E\n\u003Cp class=\"post_p\"\u003E\n\tProfiler has a command interface. With the next command you could analize some Spain politicians:\n\u003C\u002Fp\u003E\n\u003Cdiv class=\"post_code\"\u003E\n\t\u003Cpre\u003E\u003Ccode\u003Emake run_all timelines=Albert_Rivera,sanchezcastejon,Pablo_Iglesias_,pablocasado_ n_topics=5\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003C\u002Fdiv\u003E\n\u003Cp class=\"post_p\"\u003E\n\tYou can find Profiler installation steps at the\n\t\u003Ca href=\"https:\u002F\u002Fgithub.com\u002Fbertini36\u002Fprofiler\u002Fblob\u002Fmaster\u002FREADME.md\" class=\"post_link\" target=\"_blank\"\u003Erepository\u003C\u002Fa\u003E.\n\tIn this \u003Ca href=\"https:\u002F\u002Fgithub.com\u002Fbertini36\u002Fprofiler\u002Fblob\u002Fmaster\u002Fsrc\u002Fsettings.py\" class=\"post_link\" target=\"_blank\"\u003Efile\u003C\u002Fa\u003E you\n\tcan configure some application behaviours about the data preprocessing and about the model.\n\u003C\u002Fp\u003E\n\u003Ch4 class=\"post_section_title text-2xl text-black\"\u003EExample of obtained results\u003C\u002Fh4\u003E\n\u003Cfigure class=\"post_figure flex justify-center\"\u003E\n\t\u003Cimg class=\"md:w-3\u002F4\" src=\"profiling-on-social-networks\u002Fprofiler1.png\" alt=\"Results\"\u003E\n\u003C\u002Ffigure\u003E\n\u003Cfigcaption class=\"post_image_figcaption\"\u003E\n\tGroup 1\n\u003C\u002Ffigcaption\u003E\n\u003Cfigure class=\"post_figure flex justify-center\"\u003E\n\t\u003Cimg class=\"md:w-3\u002F4\" src=\"profiling-on-social-networks\u002Fprofiler2.png\" alt=\"Results\"\u003E\n\u003C\u002Ffigure\u003E\n\u003Cfigcaption class=\"post_image_figcaption\"\u003E\n\tGroup 4\n\u003C\u002Ffigcaption\u003E\n\u003Cp class=\"post_p\"\u003E\n\tObtained results using tweets are not as good as obtained using posts or article news. This is because tweets\n\tare short text documents. This reduced length causes a much smaller vocabulary and therefore a difficulty\n\tfor the model to identify differentiated topics. However in this example we can see some interesting groups.\n\tThese results are from Pedro Sánchez timeline (president of Spain). At group 1 we could see that it refers to\n\ttweets about last campaign and at group 4 we see that refers to the issue of sexist violence.\n\u003C\u002Fp\u003E\n\t\t"},{title:"Automatic differentiation 📈",slug:"automatic-differentiation",tags:[a,"#tensorflow","#autograd","#linear-regression"],date:"2017-05-03",html:"\n\u003Cp class=\"post_p\"\u003E\n\tDerivatives, specifically gradients (derivatives in more than one dimension spaces) and hesians (second\n\tderivatives), have become in a fundamental of machine learning. Gradient is a vector which indicates the maximum\n\tslope direction of a function at the evaluated point. This is important to move around the function space to find\n\trelative minimums or maximums (to minimize or maximize the function). Hesian gives information about the concavity\n\tand convexity of the function. Some algorithms use it to improve the exploratory movement over the function space\n\tand to find minimums and maximums faster. Automatic Differentiation (AD) is a efficiently and accurately\n\tprocedure of derivatives calculation of numerical functions, represented as computer programs. As already mentioned,\n\tthe use of derivatives is very important for model optimization. From a mathematical point of view a model is\n\tnothing more than a function that you want to minimize or maximize and derivatives are the tool to do it.\n\u003C\u002Fp\u003E\n\u003Cp class=\"post_p\"\u003E\n\tThere are several methods of derivative calculation with a computer:\n\u003C\u002Fp\u003E\n\u003Cul class=\"post_list\"\u003E\n\t\u003Cli\u003E\n\t\t\u003Cstrong\u003ENumerical differentiation.\u003C\u002Fstrong\u003E\n\t\tThis method uses derivative definition to approximate using samples of the original function. In this way\n\t\twe can approximate the gradient ∇f as:\n\t\t\u003Cfigure class=\"post_figure flex justify-center\"\u003E\n\t\t\t\u003Cimg class=\"md:w-1\u002F2\" src=\"automatic-differentiation\u002Fderivative_deffinition.png\" alt=\"Derivative deffinition\"\u003E\n\t\t\u003C\u002Ffigure\u003E\n\t\twhere e\u003Csmall\u003Ei\u003C\u002Fsmall\u003E is i\u003Csmall\u003Eth\u003C\u002Fsmall\u003E unitary vector and h\u003E0 is the step size for the approximation.\n\t\u003C\u002Fli\u003E\n\t\u003Cli\u003E\n\t\t\u003Cstrong\u003ESymbolic differentiation.\u003C\u002Fstrong\u003E\n\t\tIt consists on automatic manipulation of the mathematical expressions to obtain the derivatives (similar to\n\t\twhat we did at school). It requires to implement derivative rules. The problem of this kind of derivation is\n\t\tthat it can produce long symbolic expressions which are difficult to evaluate.\n\t\u003C\u002Fli\u003E\n\t\u003Cli\u003E\n\t\t\u003Cstrong\u003EAutomatic Differentiation\u003C\u002Fstrong\u003E.\n\t\tIt is based on the fact that all functions can be decomposed into a finite number of operations which derivative\n\t\tis known. Combining these derivatives the derivative of the original function can be computed. Applying the\n\t\tchain rule to each elementary operation of the function we obtain the trace for the calculation of the\n\t\treal function derivative.\n\t\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Cp class=\"post_p\"\u003E\n\tNext image shows the differences between the three methods.\n\u003C\u002Fp\u003E\n\u003Cfigure class=\"post_figure flex justify-center\"\u003E\n\t\u003Cimg src=\"automatic-differentiation\u002Fderivative_strategies.png\"\n\t\t alt=\"Derivative strategies\"\u003E\n\u003C\u002Ffigure\u003E\n\u003Cfigcaption class=\"post_image_figcaption\"\u003E\n\tMethodologies for calculating derivatives.\n\u003C\u002Ffigcaption\u003E\n\u003Cp class=\"post_p\"\u003E\n\tThere are two types of Automatic Differentiation (AD): forward mode and reverse mode. On the one hand forward mode\n\tevaluates the different parts of the function forward and then it does the same for each part of the derivative till\n\treal function derivative is obtained. On the other hand reverse mode evaluates the different parts of the function\n\tforward but after, from the derivative of the function, it obtains the partial derivatives.\n\tThis is how the backpropagation method works in neural networks, which needs the partial\n\tderivatives to update the weights of each of the neural network layers. This method avoids to reuse calculus\n\talready computed and to calculate derivatives in a very efficient way.\n\u003C\u002Fp\u003E\n\u003Cfigure class=\"post_figure flex justify-center\"\u003E\n\t\u003Cimg src=\"automatic-differentiation\u002Fforward_AD.png\"\n\t\t alt=\"Automatic differentiation forward mode\"\u003E\n\u003C\u002Ffigure\u003E\n\u003Cfigcaption class=\"post_image_figcaption\"\u003E\n\tAD forward mode schema.\n\u003C\u002Ffigcaption\u003E\n\u003Cfigure class=\"post_figure flex justify-center\"\u003E\n\t\u003Cimg src=\"automatic-differentiation\u002Fbackward_AD.png\"\n\t\t alt=\"Automatic differentiation reverse mode\"\u003E\n\u003C\u002Ffigure\u003E\n\u003Cfigcaption class=\"post_image_figcaption\"\u003E\n\tAD reverse mode schema.\n\u003C\u002Ffigcaption\u003E\n\u003Ch4 class=\"post_section_title text-2xl text-black\"\u003EDerivatives tools\u003C\u002Fh4\u003E\n\u003Cp class=\"post_p\"\u003E\n\tIn this post we will be focus on function parameters optimization using Automatic Differentiation. Here are listed\n\tsome software packages to calculate derivatives and gradients:\n\u003C\u002Fp\u003E\n\u003Cul class=\"post_list\"\u003E\n\t\u003Cli\u003E\u003Ca href=\"https:\u002F\u002Fwww.tensorflow.org\u002F\" class=\"post_link\" target=\"_blank\"\u003ETensorflow\u003C\u002Fa\u003E: It uses AD reverse mode.\u003C\u002Fli\u003E\n\t\u003Cli\u003E\u003Ca href=\"http:\u002F\u002Fdeeplearning.net\u002Fsoftware\u002Ftheano\u002F\" class=\"post_link\" target=\"_blank\"\u003ETheano\u003C\u002Fa\u003E: It uses symbolic differentiation.\u003C\u002Fli\u003E\n\t\u003Cli\u003E\u003Ca href=\"https:\u002F\u002Fwww.wolfram.com\u002Fmathematica\u002F\" class=\"post_link\" target=\"_blank\"\u003EMathematica\u003C\u002Fa\u003E: It uses symbolic differentiation.\u003C\u002Fli\u003E\n\t\u003Cli\u003E\u003Ca href=\"https:\u002F\u002Fgithub.com\u002FHIPS\u002Fautograd\" class=\"post_link\" target=\"_blank\"\u003EAutograd\u003C\u002Fa\u003E: It uses AD reverse mode.\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Ch4 class=\"post_section_title text-2xl text-black\"\u003ETensorflow\u003C\u002Fh4\u003E\n\u003Cp class=\"post_p\"\u003E\n\tIt is an open source library developed by Google for numerical computation using flow graphs. Before to execute\n\ta program, Tensorflow makes a flow graph where nodes represent mathematical operations and edges represent\n\tmultidimensional data vectors also called tensors. The construction of this graph avoids to obtain the most profit\n\tof system CPUs and GPUs where the program is executed. Then, completely transparent to the programmer, Tensorflow\n\tparallels everything it can among the resources it dispose.\n\u003C\u002Fp\u003E\n\u003Cp class=\"post_p\"\u003E\n\tThis library was originally designed for \u003Ca href=\"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FDeep_learning\" class=\"post_link\" target=\"_blank\"\u003Edeep learning\u003C\u002Fa\u003E,\n\tthe machine learning branch that studies neural networks. Tensorflow avoids, in a easy way, to implement Deep Neural\n\tNetworks (DNN), Convulational Neural Networks (CNN) and Recurrent Neural Networks (RNN). However last versions\n\thave focused on satisfying the rest of machine learning community by trying to convert the\n    library in a standard for programming models of all branches. Specifically they have developed a module called\n\t\u003Ca href=\"https:\u002F\u002Fgithub.com\u002Ftensorflow\u002Ftensorflow\u002Fblob\u002Fmaster\u002Ftensorflow\u002Fcontrib\u002Flearn\u002Fpython\u002Flearn\u002FREADME.md\" class=\"post_link\" target=\"_blank\"\u003ETFLearn\u003C\u002Fa\u003E\n\twhich has a set of models ready to use and also have updated its syntax with the intention to be more close to\n\t\u003Ca href=\"http:\u002F\u002Fscikit-learn.org\u002Fstable\u002F\" class=\"post_link\" target=\"_blank\"\u003EScikit-learn\u003C\u002Fa\u003E syntax which is one of the most popular and important\n\tmachine learning libraries.\n\u003C\u002Fp\u003E\n\u003Cp class=\"post_p\"\u003E\n\tOne of the most interesting aspects of this library is that it implements AD reverse model in a very elegant way.\n\tThe coder defines a model indicating its parameters as variables and practically automatically, after specify the\n\tinference algorithm, Tensorflow is in charge of calculating gradients and apply them in optimization procedures.\n\u003C\u002Fp\u003E\n\u003Ch4 class=\"post_section_title text-2xl text-black\"\u003EUsage examples\u003C\u002Fh4\u003E\n\u003Cp class=\"post_p\"\u003E\n\tBelow is the code to optimize the parameters of a lineal regression model with Tensorflow and with Autograd (both\n\tuse AD reverse model to get gradients). A lineal regression model is defined by the equation:\n\u003C\u002Fp\u003E\n\u003Cfigure class=\"post_figure flex justify-center\"\u003E\n\t\u003Cimg src=\"automatic-differentiation\u002Flinear_regression.png\"\n\t\t alt=\"Lineal regression equation\"\u003E\n\u003C\u002Ffigure\u003E\n\u003Cp class=\"post_p\"\u003E\n\tWhere w represents the weight and b the bias. AD will find values for these parameters and these values\n\twill minimize Mean Squared Error.\n\u003C\u002Fp\u003E\n\u003Cp class=\"post_p\"\u003E\n\tThe model is defined as code in the following way. In Tensorflow parameters to be optimized of a function are\n\tdefined as variables. Later a cost function is defined based on these parameters,\n\t\u003Ca href=\"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FMean_squared_error\" class=\"post_link\" target=\"_blank\"\u003EMean Squared Error\u003C\u002Fa\u003E. Then optimization\n\talgorithm is specified, in this case \u003Ca href=\"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FGradient_descent\" class=\"post_link\" target=\"_blank\"\u003EGradient Descent\u003C\u002Fa\u003E.\n\tAnd finally we write the code to train the model (last lines). This loop, in each iteration, get a sample from\n\tthe dataset and derive the cost function to obtain the direction (gradient vector) of the local minimum, in\n\tother words, the direction that reduces Mean Squared Error. With this gradient vector weight and bias parameters\n\twill be updated (transparently to the programmer). In this way, when a sufficient number of iterations have been\n\tmade, values for the parameters that minimize the cost function will have been obtained (a local minimum will\n\thave been found).\n\u003C\u002Fp\u003E\n\u003Cdiv class=\"post_code\"\u003E\n\t\u003Cpre\u003E\u003Ccode\u003Eimport matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow as tf\n\nrng = np.random\n\n# Parameters\nlearning_rate = 0.01\ntraining_epochs = 100\n\n# Training data\ntrain_X = np.asarray([3.3, 4.4, 5.5, 6.71, 6.93, 4.168, 9.779, 6.182, 7.59,\n\t  2.167, 7.042, 10.791, 5.313, 7.997, 5.654, 9.27, 3.1])\ntrain_Y = np.asarray([1.7, 2.76, 2.09, 3.19, 1.694, 1.573, 3.366, 2.596, 2.53,\n\t  1.221, 2.827, 3.465, 1.65, 2.904, 2.42, 2.94, 1.3])\nn_samples = train_X.shape[0]\n\n# Graph input data\nX = tf.placeholder('float')\nY = tf.placeholder('float')\n\n# Optimizable parameters with random initialization\nweight = tf.Variable(rng.randn(), name='weight')\nbias = tf.Variable(rng.randn(), name='bias')\n\n# Linear model\npredictions = (X * weight) + bias\n\n# Loss function: Mean Squared Error\nloss = tf.reduce_sum(tf.pow(predictions-Y, 2))\u002F(2*n_samples)\n\n# Gradient descent optimizer\noptimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n\n# Initializing the variables\ninit = tf.global_variables_initializer()\n\n# Launch the graph\nwith tf.Session() as sess:\nsess.run(init)\nfor epoch in range(training_epochs):\n\tfor (x, y) in zip(train_X, train_Y):\n\t\tsess.run(optimizer, feed_dict={X: x, Y: y})\ntrain_error = sess.run(loss, feed_dict={X: train_X, Y: train_Y})\nprint('Train error={}'.format(train_error))\n\n# Test error\ntest_X = np.asarray([6.83, 4.668, 8.9, 7.91, 5.7, 8.7, 3.1, 2.1])\ntest_Y = np.asarray([1.84, 2.273, 3.2, 2.831, 2.92, 3.24, 1.35, 1.03])\ntest_error = sess.run(\n\ttf.reduce_sum(tf.pow(predictions - Y, 2)) \u002F (2 * test_X.shape[0]),\n\tfeed_dict={X: test_X, Y: test_Y})\nprint('Test error={}'.format(test_error))\n\nprint('Weight={} Bias={}'.format(sess.run(weight), sess.run(bias)))\n\n# Graphic display\nplt.plot(train_X, train_Y, 'ro', label='Original data')\nplt.plot(train_X, sess.run(weight) * train_X\n\t\t + sess.run(bias), label='Fitted line')\nplt.legend()\nplt.show()\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003C\u002Fdiv\u003E\n\u003Cdiv class=\"text-center\"\u003E\n\t\u003Csmall class=\"text-gray-900 leading-relaxed text-sm\"\u003E\n\t\tLearning weight and bias parameters of a linear regression model with Tensorflow.\n\t\u003C\u002Fsmall\u003E\n\u003C\u002Fdiv\u003E\n\u003Cfigure class=\"post_figure flex justify-center\"\u003E\n\t\u003Cimg src=\"automatic-differentiation\u002Flinear_regression_tf_results.png\"\n\t\t alt=\"Linear regression with Tensoflow\"\u003E\n\u003C\u002Ffigure\u003E\n\u003Cfigcaption class=\"post_image_figcaption\"\u003E\n\tOptimization results of model parameters of a lineal regression using Tensorflow.\n\u003C\u002Ffigcaption\u003E\n\u003Cp class=\"post_p\"\u003E\n\tUsing Autograd all is more visible than in Tensorflow. A cost function is defined with the model parameters\n\tand then get gradients in each iteration to update weight and bias parameters.\n\u003C\u002Fp\u003E\n\u003Cdiv class=\"post_code\"\u003E\n\t\u003Cpre\u003E\u003Ccode\u003Eimport autograd.numpy as np\nimport matplotlib.pyplot as plt\nfrom autograd import elementwise_grad\n\nrng = np.random\n\n# Parameters\nlearning_rate = 0.01\ntraining_epochs = 100\n\n# Training data\ntrain_X = np.array([3.3, 4.4, 5.5, 6.71, 6.93, 4.168, 9.779, 6.182, 7.59,\n\t2.167, 7.042, 10.791, 5.313, 7.997, 5.654, 9.27, 3.1])\ntrain_Y = np.array([1.7, 2.76, 2.09, 3.19, 1.694, 1.573, 3.366, 2.596, 2.53,\n\t1.221, 2.827, 3.465, 1.65, 2.904, 2.42, 2.94, 1.3])\nn_samples = train_X.shape[0]\n\n\ndef loss((weight, bias)):\n\"\"\" Loss function: Mean Squared Error \"\"\"\npredictions = (train_X * weight) + bias\nreturn np.sum(np.power(predictions - train_Y, 2) \u002F (2 * n_samples))\n\n# Function that returns gradients of loss function\ngradient_fun = elementwise_grad(loss)\n\n# Optimizable parameters with random initialization\nweight = rng.randn()\nbias = rng.randn()\n\nfor epoch in range(training_epochs):\ngradients = gradient_fun((weight, bias))\nweight -= gradients[0] * learning_rate\nbias -= gradients[1] * learning_rate\nprint('Train error={}'.format(loss((weight, bias))))\n\n# Test error\ntest_X = np.array([6.83, 4.668, 8.9, 7.91, 5.7, 8.7, 3.1, 2.1])\ntest_Y = np.array([1.84, 2.273, 3.2, 2.831, 2.92, 3.24, 1.35, 1.03])\npredictions = (test_X * weight) + bias\nprint('Test error={}'.format(\nnp.sum(np.power(predictions - test_Y, 2) \u002F (2 * n_samples))))\n\nprint('Weight={} Bias={}'.format(weight, bias))\n\n# Graphic display\nplt.plot(train_X, train_Y, 'ro', label='Original data')\nplt.plot(train_X, weight * train_X + bias, label='Fitted line')\nplt.legend()\nplt.show()\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003C\u002Fdiv\u003E\n\u003Cdiv class=\"text-center\"\u003E\n\t\u003Csmall class=\"text-gray-900 leading-relaxed text-sm\"\u003E\n\t\tLearning weight and bias parameters of a linear regression model with Autograd.\n\t\u003C\u002Fsmall\u003E\n\u003C\u002Fdiv\u003E\n\u003Cfigure class=\"post_figure flex justify-center\"\u003E\n\t\u003Cimg src=\"automatic-differentiation\u002Flinear_regression_ag_results.png\"\n\t\t alt=\"Linear regression with Tensoflow\"\u003E\n\u003C\u002Ffigure\u003E\n\u003Cfigcaption class=\"post_image_figcaption\"\u003E\n\tOptimization results of model parameters of a lineal regression using Autograd.\n\u003C\u002Ffigcaption\u003E\n\u003Cp class=\"post_p\"\u003E\n\tThe main objective of this post was to uncover a bit the black box that involves the optimization of models using\n\ttools as Tensorflow, Theano, \u003Ca href=\"http:\u002F\u002Fpytorch.org\u002F\" class=\"post_link\" target=\"_blank\"\u003EPytorch\u003C\u002Fa\u003E, ...\n\u003C\u002Fp\u003E\n\u003Ch4 class=\"post_section_title text-2xl text-black\"\u003EReferences\u003C\u002Fh4\u003E\n\u003Cul class=\"post_list\"\u003E\n\t\u003Cli\u003E\n\t\tAutomatic differentiation in machine learning: a survey \u003Cbr\u003E\n\t\tAtilim Gunes Baydin, Barak A. Pearlmutter, Alexey Andreyevich Radul, Jeffrey Mark Siskind\n\t\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\t\t"},{title:"Distributed matrix product 🔢",slug:"distributed-matrix-product",tags:["#CUDA","#MPI","#C","#maths"],date:"2017-02-02",html:"\n\u003Cp class=\"post_p\"\u003E\n\tNowadays one of the main problems of any algorithm is its facility to scale it, in other words, its facility to\n\tbe executed over multiple cores (parallelization) or nodes (distribution). The amount of data that is available\n\ttoday has led the world of computing to develop technologies with which to parallelize and distribute processes\n\tare done more easily and automatically. Actually, each computationally expensive algorithm works in its\n\tdistributed version. In this way tasks as finding prime numbers, complex simulations or statistical prediction\n\tmodels, which would spend years to get results, can obtain it in a few hours thanks to the joint work of many\n\tcomputers.\n\u003C\u002Fp\u003E\n\u003Cp class=\"post_p\"\u003E\n\tPrecisely this is what supercomputing centers like \u003Ca href=\"https:\u002F\u002Fwww.bsc.es\u002F\" class=\"post_link\" target=\"_blank\"\u003EBSC\u003C\u002Fa\u003E are\n\tengaged. BSC has a supercomputer, called Marenostrum III, which avoids to its users to run computationally\n\texpensive algorithms or algorithms that require a huge amount of data in a distributed way. Specifically,\n\tMarenostrum III has 3108 nodes each with 2 processors of 8 cores (over 49.728 cores).\n\u003C\u002Fp\u003E\n\u003Cfigure class=\"post_figure flex justify-center\"\u003E\n\t\u003Cimg src=\"distributed-matrix-product\u002Fmarenostrum.jpg\"\n\t\t alt=\"Marenostrum III supercomputer\"\u003E\n\u003C\u002Ffigure\u003E\n\u003Cfigcaption class=\"post_image_figcaption\"\u003EMarenostrum III\u003C\u002Ffigcaption\u003E\n\u003Cp class=\"post_p\"\u003E\n\tIn this post I'm going to show, with a simple example, how to distribute a matrix product over several nodes and\n\thow to parallelize at each node. Matrix product is a mathematical operation that, when it is taken to the extreme\n\tusing very large matrices it is become in a very computationally expensive operation.\n\u003C\u002Fp\u003E\n\u003Cfigure class=\"post_figure flex justify-center\"\u003E\n\t\u003Cimg src=\"distributed-matrix-product\u002Fmatrix_product.png\"\n\t\t alt=\"Matrix product\"\u003E\n\u003C\u002Ffigure\u003E\n\u003Cfigcaption class=\"post_image_figcaption\"\u003EMatrix product\u003C\u002Ffigcaption\u003E\n\u003Cp class=\"post_p\"\u003E\n\tSince each element of the resulting matrix does not depend on any other element of it we can distribute without\n\tany restriction. For this example I could use 4 nodes, so I decided that each node would be responsible for\n\tcalculating a quarter of the resulting matrix. A and B matrices will be sent to each node to avoid them to\n\tdo its computations. For the distribution I'm using \u003Ca href=\"https:\u002F\u002Fwww.open-mpi.org\u002F\" class=\"post_link\" target=\"_blank\"\u003EMPI\u003C\u002Fa\u003E\n\t(Message Passing Interface) technology. Using functions of this library data can be shared over the nodes\n\t(lines 123 and 124) and to specify, using process id, which part of the result matrix each node has to calculate\n\t(lines 116, 117 and 118).\n\u003C\u002Fp\u003E\n\u003Cfigure class=\"post_figure flex justify-center\"\u003E\n\t\u003Cimg src=\"distributed-matrix-product\u002Fdistributed_matrix_product.png\"\n\t\t alt=\"Distributed matrix product\"\u003E\n\u003C\u002Ffigure\u003E\n\u003Cfigcaption class=\"post_image_figcaption\"\u003EMatrix product distribution and parallelization\u003C\u002Ffigcaption\u003E\n\u003Cp class=\"post_p\"\u003E\n\tNowadays, for parallel computation, the power of GPUs is being used. In our case each node of Marenostrum III has a\n\tNvidia K80 graphic card. This graphic cards can be used for parallel computation using Nvidia\n\t\u003Ca href=\"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FCUDA\" class=\"post_link\" target=\"_blank\"\u003ECUDA\u003C\u002Fa\u003E (Compute Unified Device Architecture)\n\tplatform. GPUs are essentially a big number of simple processors that can be used to speed up some parts of the\n\tcode. For this, the program has to be decomposed in a big number of threads which will be executed concurrently.\n\tIn this example, each thread will be responsible for calculating just an element of the resulting matrix. Using\n\tCUDA it is required to define a block structure (threads group) delimiting each thread data domain using its\n\tthreadId and its blockId. These threads will be executed in one kernel, in other words, a GPU program.\n\u003C\u002Fp\u003E\n\u003Cdiv class=\"post_code\"\u003E\n\t\u003Cpre\u003E\u003Ccode\u003E\u002F* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\u002F\n\u002F* MULTI-NODE AND PARALLEL MATRIX-MATRIX PRODUCT WITH MPI AND CUDA           *\u002F\n\u002F*                                                                           *\u002F\n\u002F* File:         mmpmpicuda.cu                                               *\u002F\n\u002F* Author:       Alberto Pou Quirós (Github: bertini36)                      *\u002F \n\u002F* Description:  This program performs a matrix product (A * B = C)          *\u002F\n\u002F*               distributing the computation between multiple nodes         *\u002F\n\u002F*               with MPI technology and parallelizing the computation in    *\u002F\n\u002F*               every node with Nvidia CUDA technology                      *\u002F\n\u002F* Compilation:  nvcc -I\u002Fopt\u002Fmpi\u002Fbullxmpi\u002F1.2.9.1\u002Finclude                    *\u002F  \n\u002F*               -L\u002Fopt\u002Fmpi\u002Fbullxmpi\u002F1.2.9.1\u002Flib -lmpi -ldl -lm -lnuma       *\u002F\n\u002F*               -lrt -lnsl -lutil -lm -ldl mmpmpicuda.cu -o mmpmpicuda      *\u002F\n\u002F* Strategy:                                                                 *\u002F\n\u002F*                  Example 16x16 matrices with 4 nodes:                     *\u002F\n\u002F*                   _________________16________________                     *\u002F\n\u002F*                   |                                 |                     *\u002F\n\u002F*                   |               NODE 1            | 4                   *\u002F\n\u002F*                   |_________________________________|                     *\u002F\n\u002F*                   |                                 |                     *\u002F\n\u002F*                   |               NODE 2            | 4                   *\u002F\n\u002F*              C =  |_________________________________|    16               *\u002F \n\u002F*                   |                                 |                     *\u002F\n\u002F*                   |               NODE 3            | 4                   *\u002F\n\u002F*                   |_________________________________|                     *\u002F\n\u002F*                   |                                 |                     *\u002F \n\u002F*                   |               NODE 4            | 4                   *\u002F\n\u002F*                   |_________________________________|                     *\u002F\n\u002F*                                                                           *\u002F\n\u002F*                  Node 1 computes 4 rows of result matrix:                 *\u002F\n\u002F*                   __________________________________                      *\u002F\n\u002F*                   |                                 |                     *\u002F\n\u002F*                   |         4x16 CUDA block         |                     *\u002F\n\u002F*                   |_________________________________|                     *\u002F\n\u002F*                                                                           *\u002F\n\u002F* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *\u002F\n\n#include &lt;sys\u002Ftime.h&gt;\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;assert.h&gt;\n#include &lt;mpi.h&gt;\n\n#define N 1024\n\n#define err(format, ...) do { fprintf(stderr, format, ##__VA_ARGS__); exit(1); } while (0)\n\nstruct timeval start_time, end_time;\n\ninline void checkCuda(cudaError_t e) {\n    if (e != cudaSuccess) {\n        err(\"CUDA Error %d: %s\\n\", e, cudaGetErrorString(e));\n    }\n}\n\n__global__ void matrixProduct(double *matrix_a, double *matrix_b, double *matrix_c, int width, int nrows, int my_rank) {\n    int row = threadIdx.y + blockDim.y * blockIdx.y;\n    int col = threadIdx.x + blockDim.x * blockIdx.x; \n    matrix_c[row * width + col] = 0;\n    for (int k=0; k&lt;width; k++) {\n        matrix_c[row * width + col] += matrix_a[(row * width) + (my_rank * nrows * width) + k] * matrix_b[k * width + col];\n    }\n}\n\nvoid initializeMatrices(double matrix_a[N][N], double matrix_b[N][N]) {\n    int i, j;\n    srand(time(NULL));\n    for (i=0; i&lt;N; i++) {\n        for (j=0; j&lt;N; j++) {\n            matrix_a[i][j] = rand();\n            matrix_b[i][j] = rand();\n        }\n    }\n}\n\nvoid showMatrices(double matrix_a[N][N], double matrix_b[N][N], double matrix_c[N][N]) {\n    int i, j;\n    srand(time(NULL));\n    printf(\"***** MATRIX A ***** \\n\");\n    for (i=0; i&lt;N; i++) {\n        for (j=0; j&lt;N; j++) {\n            (j % N == N-1) ? printf(\"%.1f \\n\", matrix_a[i][j]) : printf(\"%.1f,\", matrix_a[i][j]);\n        }\n    }\n    printf(\"***** MATRIX B ***** \\n\");\n    for (i=0; i&lt;N; i++) {\n        for (j=0; j&lt;N; j++) {\n            (j % N == N-1) ? printf(\"%.1f \\n\", matrix_b[i][j]) : printf(\"%.1f,\", matrix_b[i][j]);\n        }\n    }\n    printf(\"***** RESULT MATRIX ***** \\n\");\n    for (int i=0; i&lt;N; i++) {\n        for (int j=0; j&lt;N; j++) {\n            (j % N == N-1) ? printf(\"%f \\n\", matrix_c[i][j]) : printf(\"%f,\", matrix_c[i][j]);\n        }\n    }\n}\n\nint main(int argc, char *argv[]) {\n\n    double A[N][N], B[N][N], C[N][N];\n    double *d_a, *d_b, *d_c;\n    int my_rank, comm_sz, from, to, nrows;\n  \n    \u002F\u002F MPI initialization\n    MPI_Init (&argc, &argv);\n    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);    \u002F\u002F Process id \n    MPI_Comm_size(MPI_COMM_WORLD, &comm_sz);    \u002F\u002F Number of processors \n\n    if (N % comm_sz != 0) {\n        if (my_rank == 0) printf(\"Matrix size not divisible by number of processors \\n\");\n        MPI_Finalize();\n        exit(-1);\n    }\n\n    \u002F\u002F Calculate interval lines to compute per node\n    from = my_rank * N \u002F comm_sz;\n    to = (my_rank + 1) * N \u002F comm_sz;\n    nrows = to - from;\n\n    if (my_rank == 0) { initializeMatrices(A, B); }\n\n    \u002F\u002F Send A y B to every node\n    MPI_Bcast(A, N*N, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    MPI_Bcast(B, N*N, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    \u002F\u002F Allocate memory in the device\n    checkCuda(cudaMalloc((void **) &d_a, N*N*sizeof(double)));\n    checkCuda(cudaMalloc((void **) &d_b, N*N*sizeof(double)));\n    checkCuda(cudaMalloc((void **) &d_c, (N*N\u002Fcomm_sz)*sizeof(double)));\n\n    \u002F\u002F Copy the information in the device\n    checkCuda(cudaMemcpy(d_a, A, N*N*sizeof(double), cudaMemcpyHostToDevice));\n    checkCuda(cudaMemcpy(d_b, B, N*N*sizeof(double), cudaMemcpyHostToDevice));\n\n    \u002F\u002F CUDA threads structure definition\n    dim3 dimGrid(1);\n    dim3 dimBlock(N, nrows);    \n\n    MPI_Barrier(MPI_COMM_WORLD);\n    if (my_rank == 0) { gettimeofday(&start_time, NULL); }\n\n    \u002F\u002F Kernel launch\n    matrixProduct&lt;&lt;&lt;dimGrid, dimBlock&gt;&gt;&gt;(d_a, d_b, d_c, N, nrows, my_rank);\n    checkCuda(cudaDeviceSynchronize());\n    checkCuda(cudaGetLastError());\n\n    \u002F\u002F Calculate compute time\n    MPI_Barrier(MPI_COMM_WORLD);\n    if (my_rank == 0) { \n        gettimeofday(&end_time, NULL);\n        printf(\"Compute time: %.1f ms \\n\", (float) (end_time.tv_sec - start_time.tv_sec) * 1000 + (end_time.tv_usec - start_time.tv_usec) \u002F 1000);\n     }\n\n    \u002F\u002F Get results from device\n    checkCuda(cudaMemcpy(C[from], d_c, (nrows)*N*sizeof(double), cudaMemcpyDeviceToHost));\n\n    \u002F\u002F Unify results from nodes\n    MPI_Gather(C[from], N*N\u002Fcomm_sz, MPI_DOUBLE, C, N*N\u002Fcomm_sz, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    \u002F\u002F if (my_rank == 0)  { showMatrices(A, B, C); }\n\n    checkCuda(cudaFree(d_a));\n    checkCuda(cudaFree(d_b));\n    checkCuda(cudaFree(d_c));\n    \n    MPI_Finalize();\n    return 0;\n\n}\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003C\u002Fdiv\u003E\n\u003Cp class=\"post_p\"\u003E\n\tOne of the main problems of CUDA is that to get the most out of it you have to know the technical specifications\n\tof the GPU on which it will run the code. Block size (number of threads) or the use of the memory hierarchy\n\tof the graphic card are aspects that the coder has to take into account when programming with this technology.\n\u003C\u002Fp\u003E\n\u003Cp class=\"post_p\"\u003E\n\tAt this \u003Ca href=\"https:\u002F\u002Fgithub.com\u002Fbertini36\u002FdistributedMatrixProduct\" class=\"post_link\" target=\"_blank\"\u003EGithub repository\u003C\u002Fa\u003E\n\tyou will find two versions of matrix product, one just using MPI technology and another just with CUDA. Also,\n\tat config folder, there are a script to know graphic card specifications and a possible CUDA configuration\n\tfor that card.\n\u003C\u002Fp\u003E\n\t\t"},{title:"Variational Inference III 🤖🎯",slug:"variational-inference-3",tags:[b,c,d],date:"2017-05-11",html:"\n\u003Cp class=\"post_p\"\u003E\n\tIn this post I'm going to continue explaining concepts that I introduced in previous posts\n\t(\u003Ca href=\"https:\u002F\u002Falbertopou.dev\u002Fblog\u002Fvariational-inference-1\u002F\" class=\"post_link\" target=\"_blank\"\u003E1\u003C\u002Fa\u003E and\n\t\u003Ca href=\"https:\u002F\u002Falbertopou.dev\u002Fblog\u002Fvariational-inference-2\u002F\" class=\"post_link\" target=\"_blank\"\u003E2\u003C\u002Fa\u003E) about Variational\n\tinference (VI). Algorithms presented till now have scalability problems. For each iteration the algorithm\n\trequires to go through all the data and this, for massive volumes of data, is impracticable. An alternative to\n\tsolve this problem is: Sthocastic Variational Inference. This version is based on using a batch (set of points)\n\tof data in each iteration. In this way, after more iterations than the conventional solutions, the solution\n\twill tend to a local optimum. The main advantage of this mechanism is that it doesn't require to keep all dataset in\n\tmemory, solving the bottleneck that could be formed by using VI with very big datasets.\n\u003C\u002Fp\u003E\n\u003Ch4 class=\"post_section_title text-2xl text-black\"\u003ESthocastic Variational Inference\u003C\u002Fh4\u003E\n\u003Ch5 class=\"post_section_title text-xl text-black\"\u003ESthocastic optimization\u003C\u002Fh5\u003E\n\u003Cp class=\"post_p\"\u003E\n\tThis technique obtains estimations of the real gradient of an objective function.\n\tThus we obtain an algorithm which iterates each batch and adjusts the hidden structure of the model based only\n\ton that batch of data. Stochastic optimization finds a function relative maximum or minimum using real gradient\n\testimations. Estimations expectation  E(∇λC(λ,x)), being x a batch of data, is equivalent to the\n\treal gradient ∇λC(λ,X), being X the full dataset.\n\u003C\u002Fp\u003E\n\u003Cfigure class=\"post_figure flex justify-center\"\u003E\n\t\u003Cimg class=\"post_image\"\n\t\t src=\"variational-inference-3\u002Fsvi.png\"\n\t\t alt=\"Sthocastic Optimization\"\u003E\n\u003C\u002Ffigure\u003E\n\u003Cp class=\"post_p\"\u003E\n\tUnder ideal conditions these stochastic algorithms converge to a local optimum of the function\n\tif ρ meets the Robins-Monro conditions:\n\u003C\u002Fp\u003E\n\u003Cfigure class=\"post_figure flex justify-center\"\u003E\n\t\u003Cimg class=\"post_image\"\n\t\t src=\"variational-inference-3\u002Frobins.png\"\n\t\t alt=\"Robins-Monro\"\u003E\n\u003C\u002Ffigure\u003E\n\u003Cp class=\"post_p\"\u003E\n\tThe use of this technique results in the algorithm Stochastic Gradient Ascent Variational Inference (SGAVI)\n\tand Stochastic Natural Gradient Ascent Variational Inference (SNGAVI) if natural gradients are used to estimate.\n\tThese algorthims, thanks to the real gradient estimations, can avoid saddle points.\n\u003C\u002Fp\u003E\n\u003Cfigure class=\"post_figure flex justify-center\"\u003E\n\t\u003Cimg class=\"post_image\"\n\t\t src=\"variational-inference-3\u002Fsaddle-point.png\"\n\t\t alt=\"Saddle point\"\u003E\n\u003C\u002Ffigure\u003E\n\u003Cfigcaption class=\"post_image_figcaption\"\u003E\n\tSaddle point. (Source: Wikipedia)\n\u003C\u002Ffigcaption\u003E\n\u003Ch5 class=\"post_section_title text-xl text-black\"\u003EAlgorithm\u003C\u002Fh5\u003E\n\u003Cp class=\"post_p\"\u003E\n\tIn VI, the function to be optimized is called ELBO. If variational parameters are updated by closed analytical\n\tformulas, the algorithm is known as Sthocastic Coordinate Ascent Variational Inference (SCAVI) while if we use\n\tsthocastic optimization the algorithm is known as SGAVI. This version uses a corrector term using calculations\n\tfrom previous iterations. The generic algorithm would be as follows:\n\u003C\u002Fp\u003E\n\u003Cfigure class=\"post_figure flex justify-center\"\u003E\n\t\u003Cimg class=\"post_image\"\n\t\t src=\"variational-inference-3\u002Fsvi-algorithm.png\"\n\t\t alt=\"Sthocastic Variational Inference Algorithm\"\u003E\n\u003C\u002Ffigure\u003E\n\u003Ch4 class=\"post_section_title text-2xl text-black\"\u003EBlack Box Variational Inference\u003C\u002Fh4\u003E\n\u003Cp class=\"post_p\"\u003E\n\tStarting from the ELBO formula that was reached in the\n\t\u003Ca href=\"https:\u002F\u002Falbertopou.herokuapp.com\u002F#variational_inference_2\" class=\"post_link\" target=\"_blank\"\u003Eprevious post\u003C\u002Fa\u003E:\n\u003C\u002Fp\u003E\n\u003Cfigure class=\"post_figure flex justify-center\"\u003E\n\t\u003Cimg class=\"post_image\"\n\t\t src=\"variational-inference-3\u002Felbo2.png\"\n\t\t alt=\"ELBO\"\u003E\n\u003C\u002Ffigure\u003E\n\u003Cp class=\"post_p\"\u003E\n\tThe main idea of Black Box Variational Inference (BBVI) consists of sampling the variational model q(θ|λ) with\n\tthe purpose of obtain an approach of formula expectations. Such expectations regarding the variational model\n\tcan be computationally expensive and can be a bottleneck at computer memory level.\n\u003C\u002Fp\u003E\n\u003Ch5 class=\"post_section_title text-xl text-black\"\u003EScore Gradients\u003C\u002Fh5\u003E\n\u003Cp class=\"post_p\"\u003E\n\tNow we apply gradients and some algebraic transformations into the analytic ELBO formula (complete derivation\n\tis not shown):\n\u003C\u002Fp\u003E\n\u003Cfigure class=\"post_figure flex justify-center\"\u003E\n\t\u003Cimg class=\"post_image\"\n\t\t src=\"variational-inference-3\u002Felbo3.png\"\n\t\t alt=\"ELBO\"\u003E\n\u003C\u002Ffigure\u003E\n\u003Cp class=\"post_p\"\u003E\n\tAfter these transformations you could notice that it is not necesary to obtain gradients of complete ELBO, it\n\tis enough to derive the variational model (q(θ|λ)).\n\u003C\u002Fp\u003E\n\u003Ch5 class=\"post_section_title text-xl text-black\"\u003EMonte-Carlo Integration\u003C\u002Fh5\u003E\n\u003Cp class=\"post_p\"\u003E\n\tMonte-Carlo integration is a mechanism to approximate integrals. It consists of sampling the variable\n\twith respect to which we are integrating and do a summation of function values given those samples.\n\tThe more samples are taken from the variable more exact will be the approximation. In our case we are going\n\tto use this mechanism to approximate ELBO integral, which integrates with respect to the q(θ|λ) distribution.\n\tThen a set of samples (θs) obtained from the q(θ|λ) distribution will allow us to get an approximation of\n\tthe integral. The formula would be as follows:\n\u003C\u002Fp\u003E\n\u003Cfigure class=\"post_figure flex justify-center\"\u003E\n\t\u003Cimg class=\"post_image\"\n\t\t src=\"variational-inference-3\u002Fmontecarlo.png\"\n\t\t alt=\"Monte-Carlo Integration\"\u003E\n\u003C\u002Ffigure\u003E\n\u003Ch5 class=\"post_section_title text-xl text-black\"\u003EAlgorithm\u003C\u002Fh5\u003E\n\u003Cp class=\"post_p\"\u003E\n\tBelow is the BBVI algorithm:\n\u003C\u002Fp\u003E\n\u003Cfigure class=\"post_figure flex justify-center\"\u003E\n\t\u003Cimg class=\"post_image\"\n\t\t src=\"variational-inference-3\u002Fbbvi.png\"\n\t\t alt=\"Black Box Variational Inference Algorithm\"\u003E\n\u003C\u002Ffigure\u003E\n\u003Ch4 class=\"post_section_title text-2xl text-black\"\u003EConsiderations\u003C\u002Fh4\u003E\n\u003Cp class=\"post_p\"\u003E\n\tThis algorithm is the result of get some measures that can question the convergence of VI algorithm. Starting by\n\tsupposing q(θ|λ) factorizes (mean field assumption) to approximate ELBO integral using Monte-Carlo.\n\tAll this causes that this algorithm is subjected to a high variance, and depending on the model, slow convergence.\n\tIn order to reduce the variance that this method can cause some mechanisms have appeared:\n\u003C\u002Fp\u003E\n\u003Cul class=\"post_list\"\u003E\n\t\u003Cli\u003E\n\t\t\u003Cstrong\u003ERao-Blackwellization\u003C\u002Fstrong\u003E. This method reduces the variance of an aleatory variable replacing it\n\t\tby its conditional expectation with respect to a subset of variables.\n\t\u003C\u002Fli\u003E\n\t\u003Cli\u003E\n\t\t\u003Cstrong\u003EControl variates\u003C\u002Fstrong\u003E. This method replaces Monte-Carlo expectation by another function with\n\t\tless variance.\n\t\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Cp class=\"post_p\"\u003E\n\tA positive aspect of this method is that it is not necessary to derive the analytical formulas used to update\n\tthe variational parameters either analytical ELBO. This permits the access to these algorithms to people who have\n\tless statistical knowledge (as me). It exists another approximation called Automatic Differentiation Variational\n\tInference (ADVI). This method also has these advantages and improves the convergence of unconjugated models\n\tthat can be a challenge for the rest of VI variants.\n\u003C\u002Fp\u003E\n\u003Cp class=\"post_p\"\u003E\n\tFormerly with VI only conjugated models could be inferred since the unconjugated models didn't be easily derivable.\n\tThe discovery of algorithms like ADVI and BBVI allowed the inference of this kind of models because they changed\n\tthe analytical calculus for an approximate strategy.\n\u003C\u002Fp\u003E\n\u003Ch4 class=\"post_section_title text-2xl text-black\"\u003EVariational inference libraries\u003C\u002Fh4\u003E\n\u003Cp class=\"post_p\"\u003E\n\tBest-known libraries for the use of VI and Markov Chain Monte-Carlo (MCMC):\n\u003C\u002Fp\u003E\n\u003Cul class=\"post_list\"\u003E\n\t\u003Cli\u003E\n\t\t\u003Ca href=\"http:\u002F\u002Fedwardlib.org\u002F\" class=\"post_link\" target=\"_blank\"\u003EEdward\u003C\u002Fa\u003E: It uses Tensorflow for gradients computations\n\t\tand has BBVI, reparameterization BBVI and Metropolis-Hastings implementations.\n\t\u003C\u002Fli\u003E\n\t\u003Cli\u003E\n\t\t\u003Ca href=\"http:\u002F\u002Fmc-stan.org\u002F\" class=\"post_link\" target=\"_blank\"\u003EStan\u003C\u002Fa\u003E: It uses C++ Automatic Differentiation\n\t\treverse mode for gradients computation and has ADVI and HMC implementations.\n\t\u003C\u002Fli\u003E\n\t\u003Cli\u003E\n\t\t\u003Ca href=\"https:\u002F\u002Fgithub.com\u002Fpymc-devs\u002Fpymc3\" class=\"post_link\" target=\"_blank\"\u003EPyMC3\u003C\u002Fa\u003E: It uses Theano for gradient computations\n\t\tand has ADVI, Gibbs sampling and Metropolis-Hastings implementations.\n\t\u003C\u002Fli\u003E\n\t\u003Cli\u003E\n\t\t\u003Ca href=\"https:\u002F\u002Fwww.tensorflow.org\u002Fversions\u002Fr0.12\u002Fapi_docs\u002Fpython\u002Fcontrib.bayesflow.variational_inference\u002F\" class=\"post_link\" target=\"_blank\"\u003EBayesflow\u003C\u002Fa\u003E:\n\t\tYoung Google module for VI.\n\t\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Cp class=\"post_p\"\u003E\n\tIf you want to learn more about Variational Inference and its uses in probabilistic models inference you can take\n\ta look to my \u003Ca href=\"https:\u002F\u002Fgithub.com\u002Fbertini36\u002FGMM\u002Fblob\u002Fmaster\u002Fdocs\u002Fdoc.pdf\" class=\"post_link\" target=\"_blank\"\u003E\n\tmaster thesis\u003C\u002Fa\u003E. It focuses on the use of automatic differentiation tools to apply Variational Inference into a\n\tGaussian Mixture Model (GMM). At the \u003Ca href=\"https:\u002F\u002Fgithub.com\u002Fbertini36\u002FGMM\" class=\"post_link\" target=\"_blank\"\u003Erepository\u003C\u002Fa\u003E you can\n\tfind implementations of Gaussian Mixture Model with different technologies like\n\t\u003Ca href=\"https:\u002F\u002Fwww.tensorflow.org\u002F\" class=\"post_link\" target=\"_blank\"\u003ETensorflow\u003C\u002Fa\u003E, Python,\n\t\u003Ca href=\"https:\u002F\u002Fgithub.com\u002Fblei-lab\u002Fedward\" class=\"post_link\" target=\"_blank\"\u003EEdward\u003C\u002Fa\u003E,\n\t\u003Ca href=\"https:\u002F\u002Fgithub.com\u002FHIPS\u002Fautograd\" class=\"post_link\" target=\"_blank\"\u003EAutograd\u003C\u002Fa\u003E, ...\n\tAnd also other probabilistic models that helped me to learn.\n\u003C\u002Fp\u003E\n\u003Ch4 class=\"post_section_title text-2xl text-black\"\u003EReferences\u003C\u002Fh4\u003E\n\u003Cul class=\"post_list\"\u003E\n\t\u003Cli\u003E\n\t\tJournal of the American Statistical AssociationGeorge\n\t\t(E. P. Box)\n\t\u003C\u002Fli\u003E\n\t\u003Cli\u003E\n\t\tBuild, Compute, Critique, Repeat: Data Analysis with Latent Variable Models\n\t\t(David M. Blei)\n\t\u003C\u002Fli\u003E\n\t\u003Cli\u003E\n\t\tProbabilistic graphical models: principles and techniques\n\t\t(Koller, Daphne, and Nir Friedman)\n\t\u003C\u002Fli\u003E\n\t\u003Cli\u003E\n\t\tModel-based Machine Learning\n\t\t(Christopher M. Bishop)\n\t\u003C\u002Fli\u003E\n\t\u003Cli\u003E\n\t\tMachine Learning. A probabilistic perspective\n\t\t(Kevin P. Murphy)\n\t\u003C\u002Fli\u003E\n\t\u003Cli\u003E\n\t\tAn overview of gradient descent optimization algorithms\n\t\t(Sebastian Rudes)\n\t\u003C\u002Fli\u003E\n\t\u003Cli\u003E\n\t\tVariational Bayesian inference (slides)\n\t\t(Kay H. Brodersen)\n\t\u003C\u002Fli\u003E\n\t\u003Cli\u003E\n\t\tVariational Inference\n\t\t(David M. Blei)\n\t\u003C\u002Fli\u003E\n\t\u003Cli\u003E\n\t\tSthocastic Variational Inference\n\t\t(Matthew D. Hoffman, David M. Blei, Chong Wang and John Paisley)\n\t\u003C\u002Fli\u003E\n\t\u003Cli\u003E\n\t\tBlack Box Variational Inference\n\t\t(Rajesh Ranganath, Sean Gerrish and David M. Blei)\n\t\u003C\u002Fli\u003E\n\t\u003Cli\u003E\n\t\tThe Stan Math Library: Reverse-Mode Automatic Differentiation in C++\n\t\t(Bob Carpenter, Matthew D. Hoffman, Marcus Brubaker, Daniel Lee, Peter Li and Michael Betancourt)\n\t\u003C\u002Fli\u003E\n\t\u003Cli\u003E\n\t\tAutomatic Differentiation Variational Inference\n\t\t(Alp Kucukelbir, Dustin Tran, Rajesh Ranganath, Andrew Gelman and David M. Blei)\n\t\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\t\t"},{title:"Variational Inference II 🤖🎯",slug:"variational-inference-2",tags:[b,c,d],date:"2017-02-25",html:"\n\u003Cp class=\"post_p\"\u003E\n    In this post I'm going to continue explaining concepts that I introduced in my\n\t\u003Ca href=\"https:\u002F\u002Falbertopou.dev\u002Fblog\u002Fvariational-inference-1\" class=\"post_link\" target=\"_blank\"\u003Eprevious post\u003C\u002Fa\u003E about\n\tVariational Inference (VI). At first I'm going to show that to find the best posterior approach it's necessary\n\tto minimize the Kullback-Leibler divergence (KL) between the variational model q(θ|λ) and the probabilistic\n\tmodel p(θ|x). Starting from Bayes' rule we have:\n\u003C\u002Fp\u003E\n\u003Cfigure class=\"post_figure flex justify-center\"\u003E\n\t\u003Cimg class=\"post_image\"\n\t\t src=\"variational-inference-2\u002FELBO_derivation.png\"\n\t\t alt=\"ELBO derivation\"\u003E\n\u003C\u002Ffigure\u003E\n\u003Cp class=\"post_p\"\u003E\n\tThis expression gives us a more affordable way to calculate the evidence of the model where each factor is:\n\u003C\u002Fp\u003E\n\u003Cfigure class=\"post_figure flex justify-center\"\u003E\n\t\u003Cimg class=\"post_image\"\n\t\t src=\"variational-inference-2\u002FELBO_explanation.png\"\n\t\t alt=\"ELBO explantation\"\u003E\n\u003C\u002Ffigure\u003E\n\u003Cp class=\"post_p\"\u003E\n\tWith this demonstration we get that minimize KL[q(θ|λ)||p(θ|x)] is equivalent to maximize ELBO(q(θ|λ),p(x,θ))\n\twhich is easier to evaluate. An intractable integral has been transformed to an expectation of a known distribution.\n\u003C\u002Fp\u003E\n\u003Cfigure class=\"post_figure flex justify-center\"\u003E\n\t\u003Cimg class=\"post_image\"\n\t\t src=\"variational-inference-2\u002Fkullback_leibler.png\"\n\t\t alt=\"ELBO\"\u003E\n\u003C\u002Ffigure\u003E\n\u003Cfigcaption class=\"post_image_figcaption\"\u003E\n\tMaximize ELBO is equivalent to minimize the distance between variational model and probabilistic model.\n\u003C\u002Ffigcaption\u003E\n\u003Cp class=\"post_p\"\u003E\n\tVariational inference uses ELBO as algorithm stop condition. When some iterations, where ELBO value does not\n\tincrease, are executed it means that good values for variational model λ params have been found. These are values of\n\tλ which get closer probabilistic model (the posterior). We can rewrite ELBO as:\n\u003C\u002Fp\u003E\n\u003Cfigure class=\"post_figure flex justify-center\"\u003E\n\t\u003Cimg class=\"post_image\"\n\t\t src=\"variational-inference-2\u002FELBO_reescritura.png\"\n\t\t alt=\"Rewrited ELBO\"\u003E\n\u003C\u002Ffigure\u003E\n\u003Cp class=\"post_p\"\u003E\n\tWhere each factor is:\n\u003C\u002Fp\u003E\n\u003Cfigure class=\"post_figure flex justify-center\"\u003E\n\t\u003Cimg class=\"post_image\"\n\t\t src=\"variational-inference-2\u002FELBO_terminos.png\"\n\t\t alt=\"ELBO factors\"\u003E\n\u003C\u002Ffigure\u003E\n\u003Cp class=\"post_p\"\u003E\n\tAnd now, if we take into account the Mean-Field assumption commented in the\n\t\u003Ca href=\"https:\u002F\u002Falbertopou.dev\u002Fblog\u002Fvariational-inference-1\" class=\"post_link\" target=\"_blank\"\u003Eprevious post\u003C\u002Fa\u003E,\n\tELBO ends as follows:\n\u003C\u002Fp\u003E\n\u003Cfigure class=\"post_figure flex justify-center\"\u003E\n\t\u003Cimg class=\"post_image\" style=\"width: 90%\"\n\t\t src=\"variational-inference-2\u002FELBO_MeanField.png\"\n\t\t alt=\"ELBO taking into account Mean Field assumption\"\u003E\n\u003C\u002Ffigure\u003E\n\u003Ch4 class=\"post_section_title text-2xl text-black\"\u003EVariational Inference algorithm\u003C\u002Fh4\u003E\n\u003Cfigure class=\"post_figure flex justify-center\"\u003E\n\t\u003Cimg class=\"post_image\"\n\t\t src=\"variational-inference-2\u002FVI.png\"\n\t\t alt=\"Variational Inference algorithm\"\u003E\n\u003C\u002Ffigure\u003E\n\u003Cp class=\"post_p\"\u003E\n\tThis algorithm represents the basic idea of VI. In practice more things must be taken into account for its correct\n\toperation. First, variational model could be formed by local and global variables and the updates of these have to be\n\tdone in a concrete way. You also have to choose an inference method: Coordinate Ascent, Gradient Ascent, Sthocastic\n\tGradient Descent, ... Chosen inference method doesn't change the basic structure of the VI algorithm just change\n\tthe method of obtaining the λ new values of variational model in each iteration.\n\u003C\u002Fp\u003E\n\u003Ch5 class=\"post_section_title text-xl text-black\"\u003ECoordinate Ascent Variational Inference\u003C\u002Fh5\u003E\n\u003Cp class=\"post_p\"\u003E\n\tThe most traditional method for the inference of probabilistic models is: Coordinate Ascent Variational Inference\n\t(CAVI). For the implementation of this kind of inference you require knowledge of Bayesian statistics because\n\tfor the update of each variational model λ parameter and ELBO some analytical closed formulas have to be derived.\n\tAs previously mentioned when the model is completely conjugated (as Dirichlet-Categorical model or Normal Inverse\n\tWishart-Normal model) posterior can be analytically calculated, that is, without the need to approximate it.\n\tHowever if we have a bast amount of data this analytical calculus is impracticable due to the operations with\n\tvery large matrices in memory. For this reason, in this case and the case of no conjugated models it is a good\n\toption to approximate the posterior using VI.\n\u003C\u002Fp\u003E\n\u003Cp class=\"post_p\"\u003E\n\tThe derivation of the analytical updates for the variational model parameters can be done in two ways: generic\n\tderivation or using the properties of the\n\t\u003Ca href=\"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FExponential_family\" class=\"post_link\" target=\"_blank\"\u003EExponential Family\u003C\u002Fa\u003E.\n\u003C\u002Fp\u003E\n\u003Cp class=\"post_p\"\u003E\n\tGeneric derivarion is based on the following formula (assuming the Mean-Field assumption):\n\u003C\u002Fp\u003E\n\u003Cfigure class=\"post_figure flex justify-center\"\u003E\n\t\u003Cimg class=\"post_image\"\n\t\t src=\"variational-inference-2\u002Fgeneric_derivation.png\"\n\t\t alt=\"Generic derivation\"\u003E\n\u003C\u002Ffigure\u003E\n\u003Cp class=\"post_p\"\u003E\n\tThis derivation has to be done for each variable of the variational model θ\u003Csmall\u003Ei\u003C\u002Fsmall\u003E and after that,\n\ta statistician could deduce the distribution type of the variational parameter and how to update it.\n\u003C\u002Fp\u003E\n\u003Cp class=\"post_p\"\u003E\n\tAnother way to obtain the variational parameters updates is to derive them using the properties of the Exponential\n\tFamily. To this family belong all the distributions that can be written in the form:\n\u003C\u002Fp\u003E\n\u003Cfigure class=\"post_figure flex justify-center\"\u003E\n\t\u003Cimg class=\"post_image\"\n\t\t src=\"variational-inference-2\u002Fexponential_family.png\"\n\t\t alt=\"Exponential family\"\u003E\n\u003C\u002Ffigure\u003E\n\u003Cul class=\"post_list\"\u003E\n\t\u003Cli\u003Eh(x): Base measure.\u003C\u002Fli\u003E\n\t\u003Cli\u003Eη(θ): Natural parameters(it just depends on the parameters).\u003C\u002Fli\u003E\n\t\u003Cli\u003Et(x): Sufficient statistics(it just depends on the data). Lets know the shape of the distribution.\n\t\tDescribe the possible space for the distribution parameters.\u003C\u002Fli\u003E\n\t\u003Cli\u003Ea(η(θ)): Cumulant. It is a normalizer.\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Cp class=\"post_p\"\u003E\n\tThis family allows to establish conjugation relations between distributions. When creating the joint\n\tdistribution based on two distributions the natural parameters of one allow some simplification in the\n\tformulation together with the sufficient statistics of the other one we say that the first distribution is\n\tconjugated of the second. The conjugated models, as already mentioned, thanks to these simplifications,\n\tallow to calculate the posterior analytically.\n\u003C\u002Fp\u003E\n\u003Ch5 class=\"post_section_title text-xl text-black\"\u003ECoordinate Ascent Variational Inference algorithm\u003C\u002Fh5\u003E\n\u003Cfigure class=\"post_figure flex justify-center\"\u003E\n\t\u003Cimg class=\"post_image\"\n\t\t src=\"variational-inference-2\u002FCAVI.png\"\n\t\t alt=\"Coordinate Ascent Variational Inference Algorithm\"\u003E\n\u003C\u002Ffigure\u003E\n\u003Cp class=\"post_p\"\u003E\n\tIn this version of CAVI algorithm the distinction between updating local and global variables of the model has\n\talready been taken into accoun.\n\u003C\u002Fp\u003E\n\u003Ch5 class=\"post_section_title text-xl text-black\"\u003EGradient Ascent Variational Inference\u003C\u002Fh5\u003E\n\u003Cp class=\"post_p\"\u003E\n\tA more naive inference alternative is Gradient Ascent Variational Inference (GAVI). The difference is how is\n\tthe variational parameters update. It is not done analytically with derived formulas by a statistician,\n\tit is an exploratory process. GAVI is based on the Gradient Descent\u002FAscent algorithm.\n\u003C\u002Fp\u003E\n\u003Ch5 class=\"post_section_title text-xl text-black\"\u003EGradient Ascent\u003C\u002Fh5\u003E\n\u003Cp class=\"post_p\"\u003E\n\tGradient Ascent aims to maximize a cost function C(λ) parameterized by the model parameters, λ. The algorithm\n\toptimizes these parameters λ in the gradient direction (in the case of Gradient Descent, in the opposite\n\tdirection of the gradient) of the objective function ∇\u003Csmall\u003Eλ\u003C\u002Fsmall\u003EC(λ).\n\tIn our case we need Gradient Ascent because we want to maximize ELBO function. Learning rate η\u003E0  determines the\n\tsize of the step in the direction of the local maximum. Gradient Ascent explores latent variables space of the\n\tmodel and moves in the direction of maximum slope (which is indicated by the gradient of the function) until find\n\ta local maximum. Variational model parameters are updated as follows:\n\u003C\u002Fp\u003E\n\u003Cfigure class=\"post_figure flex justify-center\"\u003E\n\t\u003Cimg class=\"post_image\"\n\t\t src=\"variational-inference-2\u002Fgradient.png\"\n\t\t alt=\"Gradient application\"\u003E\n\u003C\u002Ffigure\u003E\n\u003Cp class=\"post_p\"\u003E\n\tOver the last years optimizations of this algorithm have been appearing: Momentum, Adagrad, Adadelta, RMSprop,\n\tAdam, ... the improvements they offer are based on aspects such as each parameter λ\u003Csmall\u003Ei\u003C\u002Fsmall\u003E has its\n    own learning rate η\u003Csmall\u003Ei\u003C\u002Fsmall\u003E or taking into account the value of previous iterations gradients to calculate\n\tthe following.\n\u003C\u002Fp\u003E\n\u003Ch5 class=\"post_section_title text-xl text-black\"\u003EGradient Ascent Variational Inference algorithm\u003C\u002Fh5\u003E\n\u003Cfigure class=\"post_figure flex justify-center\"\u003E\n\t\u003Cimg class=\"post_image\"\n\t\t src=\"variational-inference-2\u002FGAVI.png\"\n\t\t alt=\"Gradient Ascent Variational Inference algorithm\"\u003E\n\u003C\u002Ffigure\u003E\n\u003Cp class=\"post_p\"\u003E\n\tA problem of this algorithm to approximate the posterior (which causes more inaccurate convergences) is the use of\n\tthe gradient to optimize variational parameters. The gradient supposes that latent variables space is an\n\tEuclidean space. This fact implies the assumption that distance between the distributions is mesured by the\n\tEuclidean distance of their parameters. The solution to this problem, to find the real distance between two\n\tdistributions, is to use the natural gradient.\n\u003C\u002Fp\u003E\n\u003Cfigure class=\"post_figure flex justify-center\"\u003E\n\t\u003Cimg class=\"post_image\"\n\t\t src=\"variational-inference-2\u002Fnatural_gradient.png\"\n\t\t alt=\"Natural gradient definition\"\u003E\n\u003C\u002Ffigure\u003E\n\u003Cp class=\"post_p\"\u003E\n\tNatural gradient indicates the direction of maximum slope in other space, the Riemman space, where the real\n\tdistance between distributions is taken into account. This distance can be calculated premultiplying the normal\n\tgradient by the inverse of the matrix Fisher's, G(λ).\n\u003C\u002Fp\u003E\n\u003Cfigure class=\"post_figure flex justify-center\"\u003E\n\t\u003Cimg class=\"post_image\"\n\t\t src=\"variational-inference-2\u002Fnatural_gradient2.png\"\n\t\t alt=\"Natural gradient definition\"\u003E\n\u003C\u002Ffigure\u003E\n\u003Cp class=\"post_p\"\u003E\n\tIn the case of CAVI, when the analytical updates of each variational parameter are derived, the shape of the\n\tdistributions to measure the distance between them is already taking into account.\n\u003C\u002Fp\u003E\n\u003Ch5 class=\"post_section_title text-xl text-black\"\u003EEfficiency problems\u003C\u002Fh5\u003E\n\u003Cp class=\"post_p\"\u003E\n\tNowadays, in the already known as the information age, algorithms used in machine learning use huge volumes of\n\tdata. This causes programmers to scale algorithms or design alternatives less computationally expensive. CAVI and\n\tGAVI have to pass through all the data for each iteration. This procedure for massive datasets is intractable. In\n\tthe next post I'm going to explain the measures you can take in this cases and how to solve the scalability problem.\n\u003C\u002Fp\u003E\n\u003Ch4 class=\"post_section_title text-2xl text-black\"\u003EReferences\u003C\u002Fh4\u003E\n\u003Cul class=\"post_list\"\u003E\n\t\u003Cli\u003E\n\t\tJournal of the American Statistical AssociationGeorge\n\t\t(E. P. Box)\n\t\u003C\u002Fli\u003E\n\t\u003Cli\u003E\n\t\tBuild, Compute, Critique, Repeat: Data Analysis with Latent Variable Models\n\t\t(David M. Blei)\n\t\u003C\u002Fli\u003E\n\t\u003Cli\u003E\n\t\tProbabilistic graphical models: principles and techniques\n\t\t(Koller, Daphne, and Nir Friedman)\n\t\u003C\u002Fli\u003E\n\t\u003Cli\u003E\n\t\tModel-based Machine Learning\n\t\t(Christopher M. Bishop)\n\t\u003C\u002Fli\u003E\n\t\u003Cli\u003E\n\t\tMachine Learning. A probabilistic perspective\n\t\t(Kevin P. Murphy)\n\t\u003C\u002Fli\u003E\n\t\u003Cli\u003E\n\t\tAn overview of gradient descent optimization algorithms\n\t\t(Sebastian Rudes)\n\t\u003C\u002Fli\u003E\n\t\u003Cli\u003E\n\t\tVariational Bayesian inference (slides)\n\t\t(Kay H. Brodersen)\n\t\u003C\u002Fli\u003E\n\t\u003Cli\u003E\n\t\tVariational Inference\n\t\t(David M. Blei)\n\t\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\t\t"},{title:"Variational Inference I 🤖🎯",slug:"variational-inference-1",tags:[b,c,d],date:"2017-02-09",html:"\n\u003Cp class=\"post_p\"\u003E\n\tIn next posts I'm going to speak about probabilistic machine learning. Last months I was learning about this\n\tfield, specifically about a type of inference on these models known as Variational Inference, and I reckon can be\n\tinteresting to write a set of posts summarizing my experience about it thanks to my teacher\n\t\u003Ca href=\"https:\u002F\u002Ftwitter.com\u002FCapdevilaPujol\" class=\"post_link\" target=\"_blank\"\u003EJoan Capdevila\u003C\u002Fa\u003E.\n\u003C\u002Fp\u003E\n\u003Ch4 class=\"post_section_title text-2xl text-black\"\u003EProbabilistic Machine Learning\u003C\u002Fh4\u003E\n\u003Cp class=\"post_p\"\u003E\n\tLast decades studies about machine learning has caused the appearance of a wide variety of algorithms to solve\n\ta large set of problems covering areas such as driving autonomous vehicles, medical diagnosis, speech recognition\n\tor user ranking for marketing campaigns. These algorithms are mainly based on aa construction of a model which\n\tdescribes data as closely as possible.\n\u003C\u002Fp\u003E\n\u003Cp class=\"post_p\"\u003E\n\tA model is a compact description of a set of data that avoids us to make predictions about future samples. The main\n\tdifference between a conventional machine learning model and a probabilistic one is that the last avoids to model\n\tuncertainty, in other words, it avoids to know how probably is a prediction to be fulfilled. This aspect can\n\tbe very valuable in areas such as medicine or economy where risk about to take a bad decision can be detrimental to\n\ta person's health or lead to financial loss.\n\u003C\u002Fp\u003E\n\u003Cfigure class=\"post_figure flex justify-center\"\u003E\n\t\u003Cimg class=\"post_image\"\n\t\t src=\"variational-inference-1\u002Fschema_situation.png\"\n\t\t alt=\"Probabilistic Machine Learning situation\"\u003E\n\u003C\u002Ffigure\u003E\n\u003Cfigcaption class=\"post_image_figcaption\"\u003E\n\tProbabilistic machine learning situation at artificial intelligence area\n\u003C\u002Ffigcaption\u003E\n\u003Cp class=\"post_p\"\u003E\n\tThis kind of models uses the probability theory to model priori information, in this way the algorithm is not\n\tbased just on sample data. These models permit the use of different datasets to learn (this is useful when\n\twe have a small quantity of data) and to define complex models with a quantity of random variables required.\n\tThey also support online leaning, you don't need to retrain the full model each time you obtain new data, you just\n\trequire to update some probabilities. They are also very useful in decision-making, when a robust explanation of a\n\tmodel is required. Finally they are generative models, thanks to the distributions that they infer, they allow\n\tto generate new data simulating values ​​of any random variable of the model.\n\u003C\u002Fp\u003E\n\u003Cp class=\"post_p\"\u003E\n\tUnlike discriminant models which only model the probability of the variable to be predicted, a generative model is\n\ta complete model which uses all the variables (observed and latent), allowing multiple questions to be\n\tanswered.\n\u003C\u002Fp\u003E\n\u003Cp class=\"post_p\"\u003E\n\tThe construction of this type of models with latent variables is done following Box Loop philosophy. This loop\n\twas created by the statistician\n\t\u003Ca href=\"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FGeorge_E._P._Box\" class=\"post_link\" target=\"_blank\"\u003EGeorge E. P. Box\u003C\u002Fa\u003E. This loop is\n\titerated several times during the construction of a probabilistic model.\n\u003C\u002Fp\u003E\n\u003Cfigure class=\"post_figure flex justify-center\"\u003E\n\t\u003Cimg class=\"post_image\"\n\t\t src=\"variational-inference-1\u002Fbox_loop.png\"\n\t\t alt=\"Box loop\"\u003E\n\u003C\u002Ffigure\u003E\n\u003Cfigcaption class=\"post_image_figcaption\"\u003E\n\tBox loop graphical scheme\n\u003C\u002Ffigcaption\u003E\n\u003Cul class=\"post_list\"\u003E\n\t\u003Cli\u003EFirst, probabilistic model is made based on environment knowledge that we already have.\u003C\u002Fli\u003E\n\t\u003Cli\u003EAfter some patterns are discovered using the model previously defined and with an inference method.\u003C\u002Fli\u003E\n\t\u003Cli\u003E\n\t\tFinally the model has to be validated. If it was not good enough we would go back to step 1 unless it would be\n\t\tused to describe or predict new data.\n\t\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Ch4 class=\"post_section_title text-2xl text-black\"\u003EBayesian inference\u003C\u002Fh4\u003E\n\u003Cp class=\"post_p\"\u003E\n\tBayesian inference tries to reveal a hidden structure in data that cannot be directly observed. For\n\ttraditional machine learning methods parameters are values that are determined by optimization algorithms\n\tminimizing an error function. The bayesian point of view is a little bit different. For a bayesian all the\n\tunknown parameters are described by probability distributions and observation of evidence avoids to update\n\tthese distrubutions using Bayes rule.\n\u003C\u002Fp\u003E\n\u003Ch5 class=\"post_section_title text-xl text-black\"\u003EBayes rule\u003C\u002Fh5\u003E\n\u003Cp class=\"post_p\"\u003E\n\tThe first thing you need to be clear about to understand Bayesian inference is Bayes rule. Bayes rule indicates\n\thow a priori probability about an event has to be updated after observe evidences about it. From a bayesian point\n\tof view there are no differences between parameters and observed variables, both are random variables. I'm going to\n\tuse x to reference observed variables and θ to reference latent variables.\n\u003C\u002Fp\u003E\n\u003Cfigure class=\"post_figure flex justify-center\"\u003E\n\t\u003Cimg class=\"post_image md:w-1\u002F6 sm:w-1\u002F2\"\n\t\t src=\"variational-inference-1\u002Fbayes_rule.png\"\n\t\t alt=\"Bayes rule\"\u003E\n\u003C\u002Ffigure\u003E\n\u003Cp class=\"post_p\"\u003E\n\tThe following explains what is each term of the formula, being x and &theta; data and model\n\tparameters respectively:\n\u003C\u002Fp\u003E\n\u003Cul class=\"post_list\"\u003E\n\t\u003Cli\u003E\n\t\t\u003Cstrong\u003EPosterior p(&theta;|x)\u003C\u002Fstrong\u003E: It is the probability of data, the probability that the model with &theta;\n\t\tparameters has generated x data.\n\t\u003C\u002Fli\u003E\n\t\u003Cli\u003E\n\t\t\u003Cstrong\u003ELikelihood p(x|&theta;)\u003C\u002Fstrong\u003E: It is the probability of data assuming that are modeled by a\n\t\tparametrized distribution. The way to calculate it depends on the model. Usually it is used to assess\n\t\tthe quality of a model.\n\t\u003C\u002Fli\u003E\n\t\u003Cli\u003E\n\t\t\u003Cstrong\u003EPrior p(&theta;)\u003C\u002Fstrong\u003E: It is the probability of the parameters. In this factor of the formula\n\t\tis where prior knowledge is reflected, information that we have before to observe any data\n\t\tobservation.\n\t\u003C\u002Fli\u003E\n\t\u003Cli\u003E\n\t\t\u003Cstrong\u003EEvidence p(x)\u003C\u002Fstrong\u003E: This is the evidence from data. It is calculated as &int; p(x,&theta;)d&theta;. Usually it\n\t\tcan't be calculated but, as it is a normalizing constant of the model, it does not affect. When two\n\t\tprobabilistic models are compared, important factors are the ones which depend on θ because p(x) will be\n\t\tthe same for each model because it just depends on data.\n\t\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Cp class=\"post_p\"\u003E\n\tProduct p(x|θ)p(θ) is also known as joint probability: p(θ,x).\n\u003C\u002Fp\u003E\n\u003Cp class=\"post_p\"\u003E\n\tUse the likelihood to estimate θ parameters is known as Maximum Likelihood Estimation (MLE) while if you take\n\tthe prior into account then is known as Maximum A Posteriori estimation (MAP). MLE and MAP are equivalents if\n\tthere is an uniform prior. However these methods only allow estimating a mean, a median and a mode of the\n\tposterior and maybe your goal requires to model uncertainty or to generate new data. In theses cases we would need\n\tto know posterior distribution. As we will see, methods as Variational Inference (VI) or Markov Chain Monte Carlo\n\t(MCMC) allow to infer this distribution. Taking into account the Bayes normalizer constant p(x) is what allows\n\tthese methods to calculate a posterior distribution.\n\u003C\u002Fp\u003E\n\u003Cp class=\"post_p\"\u003E\n\tA summary of this formula would be: At the beginning we have a belief (prior) about an event θ (p(θ)), for example,\n\tthat the height of Barcelona population is described by a Normal distribution. After we observe evidences (x),\n\ta sample of the heights of Barcelona population. With this evidence our belief about θ has to be updated, in\n\tother words, Normal distribution which described the height of Barcelona population has to be updated. This\n\tchange is reflected by the posterior distribution (p(θ|x)). In this example we can appreciate the support to\n\tonline learning that offers this kind of models.\n\u003C\u002Fp\u003E\n\u003Cfigure class=\"post_figure flex justify-center\"\u003E\n\t\u003Cimg class=\"post_image\"\n\t\t src=\"variational-inference-1\u002Fonline_learning.png\"\n\t\t alt=\"Online learning\"\u003E\n\u003C\u002Ffigure\u003E\n\u003Cp class=\"post_p\"\u003E\n\tAt the end it is an iterative process of updating beliefs (prior) based on evidences (x) where posterior of one\n\titeration will be the prior of next one.\n\u003C\u002Fp\u003E\n\u003Cp class=\"post_p\"\u003E\n\tPosterior inference algorithms avoid to analyze information under certain assumptions (priors) discovering a\n\thidden structure which best fits with our data. When all relations between model random variable are\n\t\u003Ca href=\"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FConjugate_prior\" class=\"post_link\" target=\"_blank\"\u003Econjugated\u003C\u002Fa\u003E, this is, when joint\n\tdistribution has the same form as the prior, posterior can be calculated analytically. This is the\n\tcase of models like Dirichlet-Categorical o Normal Inverse Wishart-Normal. In the opposite case, the problem of\n\tthis formula resides in the calculation of the evidence (p(x)). For many models of interest (no conjugated) to\n\tcalculate a posterior is computationally intractable because the integral over all latent variables of data\n\tthat requires to calculate the evidence. For these models, another strategy is required to obtain the posterior,\n\tfor that reason the calculation of the posterior becomes an approximation problem.\n\u003C\u002Fp\u003E\n\u003Ch5 class=\"post_section_title text-xl text-black\"\u003EPosterior approximation\u003C\u002Fh5\u003E\n\u003Cp class=\"post_p\"\u003E\n\tProbabilistic machine learning uses latent variables to describe data hidden structure, some relations\n\tbetween observed and latent variables are modeled using probability distributions and inference algorithms are used\n\tto estimate the posterior, that is the conditional distribution about latent variables given the observed\n\tvariables. Due to the fact that in most cases we are working in spaces with many dimensions, the calculation of\n\tposterior expectation, E(p(θ|x)), is impossible to obtain analytically and computationally, for this reason some\n\tinference methods have been created to approximate this distribution. Bayesian inference concept comes from the\n\tset of tools which have been developed to approximate this posterior and it is one of the central problems in\n\tbayesian statistics. Nowadays there are 2 algorithmic branches:\n\u003C\u002Fp\u003E\n\u003Cul class=\"post_list\"\u003E\n\t\u003Cli\u003E\n\t\t\u003Ca href=\"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FMarkov_chain_Monte_Carlo\" class=\"post_link\" target=\"_blank\"\u003E\n\t\t\tMarkov Chain Monte Carlo (MCMC)\n\t\t\u003C\u002Fa\u003E:\n\t\tSampling approximate inference.\n\t\u003C\u002Fli\u003E\n\t\u003Cli\u003E\n\t\t\u003Ca href=\"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FVariational_Bayesian_methods\" class=\"post_link\" target=\"_blank\"\u003E\n\t\t\tVariational Inference (VI)\n\t\t\u003C\u002Fa\u003E:\n\t\tStructural approximate inference.\n\t\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Cp class=\"post_p\"\u003E\n\tOn the one hand, MCMC is based on the construction of a Markov chain over all latent variables being the posterior\n\tits stationary distribution. After the chain is executed till it arrives to its equilibrium point.\n\tFinally, results obtained sampling the Markov chain in its stationary section, are the posterior samples. The\n\tbest known algorithms of this family are\n\t\u003Ca href=\"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FMetropolis%E2%80%93Hastings_algorithm\" class=\"post_link\" target=\"_blank\"\u003EMetropolis–Hastings\u003C\u002Fa\u003E\n\t, \u003Ca href=\"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FGibbs_sampling\" class=\"post_link\" target=\"_blank\"\u003EGibbs sampling\u003C\u002Fa\u003E and\n\t\u003Ca href=\"{https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FHybrid_Monte_Carlo\"\u003EHamiltonian Monte Carlo\u003C\u002Fa\u003E.\n\u003C\u002Fp\u003E\n\u003Cp class=\"post_p\"\u003E\n\tOn the other hand, Variational Inference approximates the posterior creating an analytical approximation, the\n\tvariational model, which is adjusted in order to reduce the distance with the posterior. In this family the\n\tproblem is transformed from an approximation one to an optimization one.\n\u003C\u002Fp\u003E\n\u003Ch5 class=\"post_section_title text-xl text-black\"\u003EProbabilistic Graphical Models\u003C\u002Fh5\u003E\n\u003Cp class=\"post_p\"\u003E\n\tIn the bayesian field, a model represents a joint probability over all random variables of the problem.\n\u003C\u002Fp\u003E\n\u003Cfigure class=\"post_figure flex justify-center\"\u003E\n\t\u003Cimg class=\"post_image md:w-1\u002F5 sm:w-4\u002F5\"\n\t\t src=\"variational-inference-1\u002Fjoint_dist.png\"\n\t\t alt=\"Joint distribution\"\u003E\n\u003C\u002Ffigure\u003E\n\u003Cp class=\"post_p\"\u003E\n\tA very practical way of representing these models is using\n\t\u003Ca href=\"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FGraphical_model\" class=\"post_link\" target=\"_blank\"\u003Eprobabilistic graphical models\u003C\u002Fa\u003E. A\n\tprobabilistic graphical model is a directed graph where nodes are random variables and edges are dependency\n\trelations between those variables. For example, joint distribution probabilistic graphical model is:\n\u003C\u002Fp\u003E\n\u003Cfigure class=\"post_figure flex justify-center\"\u003E\n\t\u003Cimg class=\"post_image md:w-1\u002F4 sm:w-64\"\n\t\t src=\"variational-inference-1\u002Fjoint_dist2.png\"\n\t\t alt=\"Joint distribution\"\u003E\n\u003C\u002Ffigure\u003E\n\u003Cfigure class=\"post_figure flex justify-center\"\u003E\n\t\u003Cimg class=\"post_image\"\n\t\t src=\"variational-inference-1\u002Fgraphical_model.png\"\n\t\t alt=\"Joint distribution\"\u003E\n\u003C\u002Ffigure\u003E\n\u003Cp class=\"post_p\"\u003E\n\tAt this context, p(x|y) represents the conditional probability in which \u003Ci\u003Ex\u003C\u002Fi\u003E variable depends on the\n\tvalue of y. At this kind of diagrams also exists another components called plates.\n\u003C\u002Fp\u003E\n\u003Cfigure class=\"post_figure flex justify-center\"\u003E\n\t\u003Cimg class=\"post_image\"\n\t\t src=\"variational-inference-1\u002Fplate.png\"\n\t\t alt=\"Plate\"\u003E\n\u003C\u002Ffigure\u003E\n\u003Cp class=\"post_p\"\u003E\n\tThis notation indicates a vector of n random variables \u003Ci\u003Ex\u003C\u002Fi\u003E.\n\u003C\u002Fp\u003E\n\u003Ch5 class=\"post_section_title text-xl text-black\"\u003ELocal and global variables\u003C\u002Fh5\u003E\n\u003Cp class=\"post_p\"\u003E\n\tIn a probabilistic model two types of random variables can be distinguished: globals and locals. A global variable\n\tis the one which is shared between all dataset examples while a local variable is owned by each example. For\n\texample, the following probabilistic graphical model, \u003Ci\u003Ey\u003C\u002Fi\u003E variable is local while \u003Ci\u003Ez\u003C\u002Fi\u003E variable is a\n\tglobal one. When a node appears obscured it means that it is an observed variable.\n\u003C\u002Fp\u003E\n\u003Cfigure class=\"post_figure flex justify-center\"\u003E\n\t\u003Cimg class=\"post_image\"\n\t\t src=\"variational-inference-1\u002Fglobal_local_variables.png\"\n\t\t alt=\"Model example\"\u003E\n\u003C\u002Ffigure\u003E\n\u003Ch4 class=\"post_section_title text-2xl text-black\"\u003EVariational inference\u003C\u002Fh4\u003E\n\u003Cp class=\"post_p\"\u003E\n\tIn next posts we will focus on the different variational strategies for posterior approach.\n\u003C\u002Fp\u003E\n\u003Ch5 class=\"post_section_title text-xl text-black\"\u003EStrategy\u003C\u002Fh5\u003E\n\u003Cp class=\"post_p\"\u003E\n\tAs already mentioned, Variational Inference consists in defining a distribution, q(&theta;|&lambda;), whose parameters &lambda;\n\twill be optimized in order to reduce the differences with the posterior p(&theta;|x). This new distribution is known as\n\tvariational model and the posterior as probabilistic model. To summarize, VI goal is to optimize variational model\n\t&lambda; parameters in order to reduce the distance with probabilistic model p(&theta;|x). &lambda; parameters are known also as\n\tvariational parameters.\n\u003C\u002Fp\u003E\n\u003Cfigure class=\"post_figure flex justify-center\"\u003E\n\t\u003Cimg class=\"post_image\"\n\t\t src=\"variational-inference-1\u002Fvariational_inference.jpg\"\n\t\t alt=\"Variational inference\"\u003E\n\u003C\u002Ffigure\u003E\n\u003Ch5 class=\"post_section_title text-xl text-black\"\u003EKullback-Leibler divergence\u003C\u002Fh5\u003E\n\u003Cp class=\"post_p\"\u003E\n\tTo calculate Euclidean distance between distribution parameters to establish the similarity between both is an\n\timperfect measure since we are comparing distributions and not points. Imagine a Normal distribution with 0 mean\n\tand 5 variance, N(0, 5), and another one with 5 mean and 5 variance, N(5, 5). These two distributions are very\n\tsimilar but they are separated by an Euclidean distance of 5. If now we compare first defined Normal distribution\n\tN(0, 5) with another Normal distribution with 2 mean and 7 variance, N(2, 7), looks like they are more different\n\tbut Euclidean distance that separates them is 4. For this reason we have to use another measure: Kullback-Leibler\n\tdivergence (KL).\n\u003C\u002Fp\u003E\n\u003Cp class=\"post_p\"\u003E\n\tKL is a divergence, in other words, a non-simmetric distance, it isn't the same to calculate KL[p(θ|x)||q(θ|λ)]\n\t(forwards KL) than KL[q(θ|λ)||p(θ|x)] (reverse KL). The fact of use one or another rises to different algorithms:\n\t \u003Ca href=\"https:\u002F\u002Ftminka.github.io\u002Fpapers\u002Fep\u002Fminka-ep-uai.pdf\" class=\"post_link\" target=\"_blank\"\u003EExpectation Propagation\u003C\u002Fa\u003E uses\n\tforwards KL while VI uses reverse KL. In general terms, Expectation Propagation is more computationally expensive.\n\tKL quantifies loss information when you approximate one distribution with another. It is based on the concept of\n\tentropy. Entropy measures the quantity of information that own data and is defined as follows:\n\u003C\u002Fp\u003E\n\u003Cfigure class=\"post_figure flex justify-center\"\u003E\n\t\u003Cimg class=\"post_image\"\n\t\t src=\"variational-inference-1\u002Fentropy.png\"\n\t\t alt=\"Entropy\"\u003E\n\u003C\u002Ffigure\u003E\n\u003Cp class=\"post_p\"\u003E\n\tKL definition consists in modify entropy formula to take q distribution into account.\n\u003C\u002Fp\u003E\n\u003Cfigure class=\"post_figure flex justify-center\"\u003E\n\t\u003Cimg class=\"post_image\"\n\t\t src=\"variational-inference-1\u002Fkl1.png\"\n\t\t alt=\"Kullback-Leibler divergence\"\u003E\n\u003C\u002Ffigure\u003E\n\u003Cp class=\"post_p\"\u003E\n\tIf we adapt this form to the VI problem and we apply logarithms properties we have:\n\u003C\u002Fp\u003E\n\u003Cfigure class=\"post_figure flex justify-center\"\u003E\n\t\u003Cimg class=\"post_image md:w-1\u002F3 sm:4\u002F5\"\n\t\t src=\"variational-inference-1\u002Fkl_rule.png\"\n\t\t alt=\"Kullback-Leibler definition\"\u003E\n\u003C\u002Ffigure\u003E\n\u003Cp class=\"post_p\"\u003E\n\tThis divergence avoids us to find the real similarity between two probability distributions and it is the measure\n\tthat is minimized at VI algorithm.\n\u003C\u002Fp\u003E\n\u003Cfigure class=\"post_figure flex justify-center\"\u003E\n\t\u003Cimg class=\"post_image\"\n\t\t src=\"variational-inference-1\u002Fkullback-leibler1.png\"\n\t\t alt=\"Kullback-Leibler divergence\"\u003E\n\u003C\u002Ffigure\u003E\n\u003Cfigcaption class=\"post_image_figcaption\"\u003E\n\tForwards and reverse KL comparison to approximate a bimodal distribution. Blue part represents distribution\n\tto be approximated and red one the approximation. \u003Cstrong\u003Ea\u003C\u002Fstrong\u003E image is an approximation\n\twith forwards KL and \u003Cstrong\u003Eb\u003C\u002Fstrong\u003E and \u003Cstrong\u003Ec\u003C\u002Fstrong\u003E are approximation with reverse KL.\n\u003C\u002Ffigcaption\u003E\n\u003Cfigure class=\"post_figure flex justify-center\"\u003E\n\t\u003Cimg class=\"post_image\"\n\t\t src=\"variational-inference-1\u002Fkullback-leibler2.png\"\n\t\t alt=\"Kullback-Leibler visualization\"\u003E\n\u003C\u002Ffigure\u003E\n\u003Cfigcaption class=\"post_image_figcaption\"\u003E\n\tForwards and reverse KL comparison to approximate a unimodal distribution. Blue part represents distribution\n\tto be approximated and red one the approximation. \u003Cstrong\u003Ea\u003C\u002Fstrong\u003E image is an approximation\n\twith forwards KL and \u003Cstrong\u003Eb\u003C\u002Fstrong\u003E is an approximation with reverse KL.\n\u003C\u002Ffigcaption\u003E\n\u003Ch5 class=\"post_section_title text-xl text-black\"\u003EMean-Field assumption\u003C\u002Fh5\u003E\n\u003Cp class=\"post_p\"\u003E\n\tIn order to define a treatable distribution over all latent variables to approximate the posterior we can\n\tsimplify variational model optimization assuming that it is a factorized model. It is to suppose that q(&theta;|&lambda;) is\n\tcomposed by a set of distributions q\u003Csub\u003Ei\u003C\u002Fsub\u003E(&theta;\u003Csub\u003Ei\u003C\u002Fsub\u003E|&lambda;\u003Csub\u003Ei\u003C\u002Fsub\u003E) (of the\n\t\u003Ca href=\"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FExponential_family\" class=\"post_link\" target=\"_blank\"\u003EExponential Family\u003C\u002Fa\u003E). Each one of\n\tthese distributions has its own parameters &lambda;\u003Csub\u003Ei\u003C\u002Fsub\u003E which could be optimized individually.\n\u003C\u002Fp\u003E\n\u003Cfigure class=\"post_figure flex justify-center\"\u003E\n\t\u003Cimg class=\"post_image md:w-1\u002F5 sm:w-4\u002F5\"\n\t\t src=\"variational-inference-1\u002Fmean_field.png\"\n\t\t alt=\"Mean-Field assumption\"\u003E\n\u003C\u002Ffigure\u003E\n\u003Cp class=\"post_p\"\u003E\n\tMain goal of this post is not more than to present the basic idea of Variational Inference and its main players.\n\tIn following posts we will go deeper into these algorithms and programming tools to code this models will be\n\tpresented.\n\u003C\u002Fp\u003E\n\u003Ch4 class=\"post_section_title text-2xl text-bold\"\u003EReferences\u003C\u002Fh4\u003E\n\u003Cul class=\"post_list\"\u003E\n\t\u003Cli\u003E\n\t\tJournal of the American Statistical AssociationGeorge (E. P. Box)\n\t\u003C\u002Fli\u003E\n\t\u003Cli\u003E\n\t\tBuild, Compute, Critique, Repeat: Data Analysis with Latent Variable Models\n\t\t(David M. Blei)\n\t\u003C\u002Fli\u003E\n\t\u003Cli\u003E\n\t\tProbabilistic graphical models: principles and techniques\n\t\t(Koller, Daphne, and Nir Friedman)\n\t\u003C\u002Fli\u003E\n\t\u003Cli\u003E\n\t\tModel-based Machine Learning\n\t\t(Christopher M. Bishop)\n\t\u003C\u002Fli\u003E\n\t\u003Cli\u003E\n\t\tMachine Learning. A probabilistic perspective\n\t\t(Kevin P. Murphy)\n\t\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\t\t"}]}}("#python","#probabilistic-machine-learning","#variational-inference","#statistics"))]};if('serviceWorker' in navigator)navigator.serviceWorker.register('/service-worker.js');(function(){try{eval("async function x(){}");var main="/client/client.8e6d9fe5.js"}catch(e){main="/client/legacy/client.f5a415ce.js"};var s=document.createElement("script");try{new Function("if(0)import('')")();s.src=main;s.type="module";s.crossOrigin="use-credentials";}catch(e){s.src="/client/shimport@1.0.1.js";s.setAttribute("data-main",main);}document.head.appendChild(s);}());</script> 