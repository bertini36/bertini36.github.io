<!doctype html> <html lang=en> <head> <meta charset=utf-8> <meta content="width=device-width,initial-scale=1" name=viewport> <meta content=#333333 name=theme-color> <base href=/ > <link href=https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css rel=stylesheet> <link href=global.css rel=stylesheet> <link href=manifest.json rel=manifest crossorigin=use-credentials> <link href=black_logo.png rel=icon type=image/png> <link href=client/main.3948952183.css rel=stylesheet> <noscript id=sapper-head-start></noscript><title>Alberto Pou - Blog</title><meta content="All blog posts of Alberto Pou" name=description><noscript id=sapper-head-end></noscript> </head> <body> <div id=sapper> <nav class="svelte-71l2nh border-b-2 font-light px-4 py-0"><ul class="svelte-71l2nh m-0 p-0"><li class=float-left><a href=. class="hover:text-red-500 px-3 block no-underline py-4 svelte-71l2nh">Home</a></li> <li class=float-left><a href=blog class="hover:text-red-500 px-3 block no-underline py-4 svelte-71l2nh" rel=prefetch aria-current=page>Blog</a></li> <li class=float-right><a href=https://github.com/bertini36/blog class="hover:text-red-500 px-3 block no-underline py-4 text-black" target=_blank><i class="fa fa-github"></i></a></ul></nav> <main class=svelte-1uhnsl8> <div class=posts><div class=mb-8><div class=mb-4><span class="text-base text-gray-600">#python</span> <span class="text-base text-gray-600">#twitter</span> <span class="text-base text-gray-600">#text-mining</span> <span class="text-base text-gray-600">#machine-learning</span> <span class="text-base text-gray-600">#lda</span> <h1 class="hover:text-red-500 mb-0 pb-0 text-3xl"><a href=blog/profiling-on-social-networks rel=prefetch hreflang=en>Profiling on social networks</a></h1> <span class="text-base text-gray-600">a year ago</span></div> <p> <div class=content> <p class=post_p> Last weekend I was coding an application for Twitter timelines analysis which I called <a href=https://github.com/bertini36/profiler class=post_link target=_blank>Profiler</a> (I was just bored). Some years ago I was working on probabilistic models and there was one which got my attention: ...</div><p></p> <div class=mt-2><a href=blog/profiling-on-social-networks class="hover:text-red-500 px-3 border border-black hover:border-red-500 py-1" rel=prefetch hreflang=en>Continue reading »</a></div> </div><div class=mb-8><div class=mb-4><span class="text-base text-gray-600">#python</span> <span class="text-base text-gray-600">#tensorflow</span> <span class="text-base text-gray-600">#autograd</span> <span class="text-base text-gray-600">#linear-regression</span> <h1 class="hover:text-red-500 mb-0 pb-0 text-3xl"><a href=blog/automatic-differentiation rel=prefetch hreflang=en>Automatic differentiation</a></h1> <span class="text-base text-gray-600">3 years ago</span></div> <p> <p class=post_p> Derivatives, specifically gradients (derivatives in more than one dimension spaces) and hesians (second derivatives), have become in a fundamental of machine learning. Gradient is a vector which indicates the maximum slope direction of a function at the evaluated point. This is important to move around the function space to find relative minimums or maximums (to...<p></p> <div class=mt-2><a href=blog/automatic-differentiation class="hover:text-red-500 px-3 border border-black hover:border-red-500 py-1" rel=prefetch hreflang=en>Continue reading »</a></div> </div></div></main></div> <script>__SAPPER__={baseUrl:"",preloaded:[void 0,(function(a){return {posts:[{title:"Profiling on social networks",slug:"profiling-on-social-networks",tags:[a,"#twitter","#text-mining","#machine-learning","#lda"],date:"2019-06-22",html:"\n\u003Cdiv class=\"content\"\u003E\n\t\u003Cp class=\"post_p\"\u003E\n\t\tLast weekend I was coding an application for Twitter timelines analysis which I called\n\t\t\u003Ca href=\"https:\u002F\u002Fgithub.com\u002Fbertini36\u002Fprofiler\" class=\"post_link\" target=\"_blank\"\u003EProfiler\u003C\u002Fa\u003E (I was just bored). Some years ago I\n\t\twas working on probabilistic models and there was one which got my attention:\n\t\t\u003Ca href=\"http:\u002F\u002Fwww.jmlr.org\u002Fpapers\u002Fvolume3\u002Fblei03a\u002Fblei03a.pdf\" class=\"post_link\" target=\"_blank\"\u003ELatent Dirichlet Allocation\u003C\u002Fa\u003E.\n\t\u003C\u002Fp\u003E\n\t\u003Cp class=\"post_p\"\u003E\n\t\tThis model was developed by David Blei, Andrew Ng y Michael I. Jordan and tries to find topics in document collections.\n\t\tIn other words, it groups text documents into topics that the model itself discovers. In this post I'm not going\n\t\tto explain the model structure and its inference, for this what better than to read the\n\t\t\u003Ca href=\"http:\u002F\u002Fwww.jmlr.org\u002Fpapers\u002Fvolume3\u002Fblei03a\u002Fblei03a.pdf\" class=\"post_link\" target=\"_blank\"\u003Epaper\u003C\u002Fa\u003E itself.\n\t\u003C\u002Fp\u003E\n\t\u003Cp class=\"post_p\"\u003E\n\t\tThis model is based on estimating  \u003Ca href=\"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FDirichlet_distribution\" class=\"post_link\" target=\"_blank\"\u003E\n\t\tDirichlet distributions\u003C\u002Fa\u003E. This kind of distributions model the probability of membership to a set of classes.\n\t\tSpecifically the model builds (using an iterative procedure) Dirichlet distributions to model the\n\t\tprobability of a word referring to a concrete topic and the probability of membership from each document to\n\t\teach of the topics. The parameters estimation of these distributions can be done using different types of\n\t\tBayesian inference like \u003Ca href=\"https:\u002F\u002Falbertopou.herokuapp.com\u002F#variational_inference_1\" class=\"post_link\" target=\"_blank\"\u003EVariational Inference\u003C\u002Fa\u003E\n\t\tor sampling methods as \u003Ca href=\"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FMarkov_chain_Monte_Carlo\" class=\"post_link\" target=\"_blank\"\u003EMarkov Chain Monte Carlo\u003C\u002Fa\u003E.\n\t\u003C\u002Fp\u003E\n\t\u003Cp class=\"post_p\"\u003E\n\t\tProfiler uses this probabilistic model to identify, given a Twitter timeline (set of user tweets), several\n\t\ttopics that this user writes about. From this idea I coded a Python application that downloads all these tweets of\n\t\ta user, stores them, preprocesses them and finally looks for its main topics.\n\t\u003C\u002Fp\u003E\n\t\u003Ch4 class=\"post_section_title text-2xl text-black\"\u003ETechnological stack\u003C\u002Fh4\u003E\n\t\u003Cul class=\"post_list\"\u003E\n\t\t\u003Cli\u003E\n\t\t\tTwitter data is downloaded using \u003Ca href=\"https:\u002F\u002Fgithub.com\u002Ftweepy\u002Ftweepy\" class=\"post_link\" target=\"_blank\"\u003ETweepy\u003C\u002Fa\u003E.\n\t\t\u003C\u002Fli\u003E\n\t\t\u003Cli\u003E\n\t\t\tTo store the timelines I used a \u003Ca href=\"https:\u002F\u002Fwww.mongodb.com\u002F\" class=\"post_link\" target=\"_blank\"\u003EMongoDB\u003C\u002Fa\u003E database with\n\t\t\twhich I interacted using \u003Ca href=\"https:\u002F\u002Fgithub.com\u002Fmongodb\u002Fmongo-python-driver\" class=\"post_link\" target=\"_blank\"\u003EPymongo\u003C\u002Fa\u003E.\n\t\t\u003C\u002Fli\u003E\n\t\t\u003Cli\u003E\n\t\t\tTextual data preprocessing was done using libraries as  \u003Ca href=\"https:\u002F\u002Fgithub.com\u002Fpandas-dev\u002Fpandas\" class=\"post_link\" target=\"_blank\"\u003EPandas\u003C\u002Fa\u003E\n\t\t\tand \u003Ca href=\"https:\u002F\u002Fgithub.com\u002Fnltk\u002Fnltk\" class=\"post_link\" target=\"_blank\"\u003ENLTK\u003C\u002Fa\u003E. This preprocessing consisted on cleaning\n\t\t\ttweets to avoid deviations (delete emoticons, capital letters, symbols, digits, ...)\n\t\t\u003C\u002Fli\u003E\n\t\t\u003Cli\u003E\n\t\t\tTo infer the model I used  \u003Ca href=\"https:\u002F\u002Fgithub.com\u002FRaRe-Technologies\u002Fgensim\" class=\"post_link\" target=\"_blank\"\u003EGensim\u003C\u002Fa\u003E\n\t\t\tlibrary. This library has a concurrent LDA implementation.\n\t\t\u003C\u002Fli\u003E\n\t\t\u003Cli\u003E\n\t\t\tTo plot obtained results I used \u003Ca href=\"https:\u002F\u002Fgithub.com\u002Fbmabey\u002FpyLDAvis\" class=\"post_link\" target=\"_blank\"\u003EpyLDAvis\u003C\u002Fa\u003E\n\t\t\tlibrary which generates interactive HTML that avoids to explore the results and check which words\n\t\t\tare the most important in each topic.\n\t\t\u003C\u002Fli\u003E\n\t\t\u003Cli\u003E\n\t\t\tI coded also a command interface to ease interact with the library. To do this I used Google\n\t\t\t\u003Ca href=\"https:\u002F\u002Fgithub.com\u002Fgoogle\u002Fpython-fire\" class=\"post_link\" target=\"_blank\"\u003EFire\u003C\u002Fa\u003E library.\n\t\t\u003C\u002Fli\u003E\n\t\t\u003Cli\u003E\n\t\t\tI configured the development environment using\n\t\t\t\u003Ca href=\"https:\u002F\u002Fwww.docker.com\u002F\" class=\"post_link\" target=\"_blank\"\u003EDocker\u003C\u002Fa\u003E,\n\t\t\t\u003Ca href=\"https:\u002F\u002Fdocs.docker.com\u002Fcompose\u002F\" class=\"post_link\" target=\"_blank\"\u003EDocker compose\u003C\u002Fa\u003E and\n\t\t\t\u003Ca href=\"https:\u002F\u002Ftravis-ci.org\u002F\" class=\"post_link\" target=\"_blank\"\u003ETravis\u003C\u002Fa\u003E to execute automatic tests.\n\t\t\u003C\u002Fli\u003E\n\t\u003C\u002Ful\u003E\n\t\u003Ch4 class=\"post_section_title text-2xl text-black\"\u003ECommand interface\u003C\u002Fh4\u003E\n\t\u003Cp class=\"post_p\"\u003E\n\t\tProfiler has a command interface. With the next command you could analize some Spain politicians:\n\t\u003C\u002Fp\u003E\n\t\u003Cdiv class=\"post_code\"\u003E\n\t\t\u003Cpre\u003E\u003Ccode\u003Emake run_all timelines=Albert_Rivera,sanchezcastejon,Pablo_Iglesias_,pablocasado_ n_topics=5\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\t\u003C\u002Fdiv\u003E\n\t\u003Cp class=\"post_p\"\u003E\n\t\tYou can find Profiler installation steps at the\n\t\t\u003Ca href=\"https:\u002F\u002Fgithub.com\u002Fbertini36\u002Fprofiler\u002Fblob\u002Fmaster\u002FREADME.md\" class=\"post_link\" target=\"_blank\"\u003Erepository\u003C\u002Fa\u003E.\n\t\tIn this \u003Ca href=\"https:\u002F\u002Fgithub.com\u002Fbertini36\u002Fprofiler\u002Fblob\u002Fmaster\u002Fsrc\u002Fsettings.py\" class=\"post_link\" target=\"_blank\"\u003Efile\u003C\u002Fa\u003E you\n\t\tcan configure some application behaviours about the data preprocessing and about the model.\n\t\u003C\u002Fp\u003E\n\t\u003Ch4 class=\"post_section_title text-2xl text-black\"\u003EExample of obtained results\u003C\u002Fh4\u003E\n\t\u003Cdiv class=\"post_image_div md:px-40\"\u003E\n\t\t\u003Cfigure\u003E\n\t\t\t\u003Cimg class=\"post_image\" src=\"profiling-on-social-networks\u002Fprofiler1.png\" alt=\"Results\"\u002F\u003E\n\t\t\t\u003Cfigcaption class=\"post_image_figcaption\"\u003E\n\t\t\t\tGroup 1\n\t\t\t\u003C\u002Ffigcaption\u003E\n\t\t\u003C\u002Ffigure\u003E\n\t\u003C\u002Fdiv\u003E\n\t\u003Cdiv class=\"post_image_div md:px-40\"\u003E\n\t\t\u003Cfigure\u003Efrom javascript\n\t\t\t\u003Cimg class=\"post_image\" src=\"profiling-on-social-networks\u002Fprofiler2.png\" alt=\"Results\"\u002F\u003E\n\t\t\t\u003Cfigcaption class=\"post_image_figcaption\"\u003E\n\t\t\t\tGroup 4\n\t\t\t\u003C\u002Ffigcaption\u003E\n\t\t\u003C\u002Ffigure\u003E\n\t\u003C\u002Fdiv\u003E\n\t\u003Cp class=\"post_p mb-10\"\u003E\n\t\tObtained results using tweets are not as good as obtained using posts or article news. This is because tweets\n\t\tare short text documents. This reduced length causes a much smaller vocabulary and therefore a difficulty\n\t\tfor the model to identify differentiated topics. However in this example we can see some interesting groups.\n\t\tThese results are from Pedro Sánchez timeline (president of Spain). At group 1 we could see that it refers to\n\t\ttweets about last campaign and at group 4 we see that refers to the issue of sexist violence.\n\t\u003C\u002Fp\u003E\n\u003C\u002Fdiv\u003E\n\t\t"},{title:"Automatic differentiation",slug:"automatic-differentiation",tags:[a,"#tensorflow","#autograd","#linear-regression"],date:"2017-05-03",html:"\n\u003Cp class=\"post_p\"\u003E\n\tDerivatives, specifically gradients (derivatives in more than one dimension spaces) and hesians (second\n\tderivatives), have become in a fundamental of machine learning. Gradient is a vector which indicates the maximum\n\tslope direction of a function at the evaluated point. This is important to move around the function space to find\n\trelative minimums or maximums (to minimize or maximize the function). Hesian gives information about the concavity\n\tand convexity of the function. Some algorithms use it to improve the exploratory movement over the function space\n\tand to find minimums and maximums faster. Automatic Differentiation (AD) is a efficiently and accurately\n\tprocedure of derivatives calculation of numerical functions, represented as computer programs. As already mentioned,\n\tthe use of derivatives is very important for model optimization. From a mathematical point of view a model is\n\tnothing more than a function that you want to minimize or maximize and derivatives are the tool to do it.\n\u003C\u002Fp\u003E\n\u003Cp class=\"post_p\"\u003E\n\tThere are several methods of derivative calculation with a computer:\n\u003C\u002Fp\u003E\n\u003Cul class=\"post_list\"\u003E\n\t\u003Cli\u003E\n\t\t\u003Cstrong\u003ENumerical differentiation.\u003C\u002Fstrong\u003E\n\t\tThis method uses derivative definition to approximate using samples of the original function. In this way\n\t\twe can approximate the gradient ∇f as:\n\t\t\u003Cdiv class=\"post_image_div md:px-64\"\u003E\n\t\t\t\u003Cfigure\u003E\n\t\t\t\t\u003Cimg class=\"post_image\"\n\t\t\t\t\t src=\"automatic-differentiation\u002Fderivative_deffinition.png\"\n\t\t\t\t\t alt=\"Derivative deffinition\"\u002F\u003E\n\t\t\t\u003C\u002Ffigure\u003E\n\t\t\u003C\u002Fdiv\u003E\n\t\twhere e\u003Csmall\u003Ei\u003C\u002Fsmall\u003E is i\u003Csmall\u003Eth\u003C\u002Fsmall\u003E unitary vector and h\u003E0 is the step size for the approximation.\n\t\u003C\u002Fli\u003E\n\t\u003Cli\u003E\n\t\t\u003Cstrong\u003ESymbolic differentiation.\u003C\u002Fstrong\u003E\n\t\tIt consists on automatic manipulation of the mathematical expressions to obtain the derivatives (similar to\n\t\twhat we did at school). It requires to implement derivative rules. The problem of this kind of derivation is\n\t\tthat it can produce long symbolic expressions which are difficult to evaluate.\n\t\u003C\u002Fli\u003E\n\t\u003Cli\u003E\n\t\t\u003Cstrong\u003EAutomatic Differentiation\u003C\u002Fstrong\u003E.\n\t\tIt is based on the fact that all functions can be decomposed into a finite number of operations which derivative\n\t\tis known. Combining these derivatives the derivative of the original function can be computed. Applying the\n\t\tchain rule to each elementary operation of the function we obtain the trace for the calculation of the\n\t\treal function derivative.\n\t\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Cp class=\"post_p\"\u003E\n\tNext image shows the differences between the three methods.\n\u003C\u002Fp\u003E\n\u003Cdiv class=\"post_image_div md:px-10\"\u003E\n\t\u003Cfigure\u003E\n\t\t\u003Cimg class=\"post_image\"\n\t\t\t src=\"automatic-differentiation\u002Fderivative_strategies.png\"\n\t\t\t alt=\"Derivative strategies\"\u002F\u003E\n\t\t\u003Cfigcaption class=\"post_image_figcaption\"\u003E\n\t\t\tMethodologies for calculating derivatives.\n\t\t\u003C\u002Ffigcaption\u003E\n\t\u003C\u002Ffigure\u003E\n\u003C\u002Fdiv\u003E\n\u003Cp class=\"post_p\"\u003E\n\tThere are two types of Automatic Differentiation (AD): forward mode and reverse mode. On the one hand forward mode\n\tevaluates the different parts of the function forward and then it does the same for each part of the derivative till\n\treal function derivative is obtained. On the other hand reverse mode evaluates the different parts of the function\n\tforward but after, from the derivative of the function, it obtains the partial derivatives.\n\tThis is how the backpropagation method works in neural networks, which needs the partial\n\tderivatives to update the weights of each of the neural network layers. This method avoids to reuse calculus\n\talready computed and to calculate derivatives in a very efficient way.\n\u003C\u002Fp\u003E\n\u003Cdiv class=\"post_image_div md:px-40\"\u003E\n\t\u003Cfigure\u003E\n\t\t\u003Cimg class=\"post_image\"\n\t\t\t src=\"automatic-differentiation\u002Fforward_AD.png\"\n\t\t\t alt=\"Automatic differentiation forward mode\"\u002F\u003E\n\t\t\u003Cfigcaption class=\"post_image_figcaption\"\u003E\n\t\t\tAD forward mode schema.\n\t\t\u003C\u002Ffigcaption\u003E\n\t\u003C\u002Ffigure\u003E\n\u003C\u002Fdiv\u003E\n\u003Cdiv class=\"post_image_div md:px-40\"\u003E\n\t\u003Cfigure\u003E\n\t\t\u003Cimg class=\"post_image\"\n\t\t\t src=\"automatic-differentiation\u002Fbackward_AD.png\"\n\t\t\t alt=\"Automatic differentiation reverse mode\"\u002F\u003E\n\t\t\u003Cfigcaption class=\"post_image_figcaption\"\u003E\n\t\t\tAD reverse mode schema.\n\t\t\u003C\u002Ffigcaption\u003E\n\t\u003C\u002Ffigure\u003E\n\u003C\u002Fdiv\u003E\n\u003Ch4 class=\"post_section_title text-2xl text-bold\"\u003EDerivatives tools\u003C\u002Fh4\u003E\n\u003Cp class=\"post_p\"\u003E\n\tIn this post we will be focus on function parameters optimization using Automatic Differentiation. Here are listed\n\tsome software packages to calculate derivatives and gradients:\n\u003C\u002Fp\u003E\n\u003Cul class=\"post_list\"\u003E\n\t\u003Cli\u003E\u003Ca href=\"https:\u002F\u002Fwww.tensorflow.org\u002F\" class=\"post_link\" target=\"_blank\"\u003ETensorflow\u003C\u002Fa\u003E: It uses AD reverse mode.\u003C\u002Fli\u003E\n\t\u003Cli\u003E\u003Ca href=\"http:\u002F\u002Fdeeplearning.net\u002Fsoftware\u002Ftheano\u002F\" class=\"post_link\" target=\"_blank\"\u003ETheano\u003C\u002Fa\u003E: It uses symbolic differentiation.\u003C\u002Fli\u003E\n\t\u003Cli\u003E\u003Ca href=\"https:\u002F\u002Fwww.wolfram.com\u002Fmathematica\u002F\" class=\"post_link\" target=\"_blank\"\u003EMathematica\u003C\u002Fa\u003E: It uses symbolic differentiation.\u003C\u002Fli\u003E\n\t\u003Cli\u003E\u003Ca href=\"https:\u002F\u002Fgithub.com\u002FHIPS\u002Fautograd\" class=\"post_link\" target=\"_blank\"\u003EAutograd\u003C\u002Fa\u003E: It uses AD reverse mode.\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Ch4 class=\"post_section_title text-2xl text-bold\"\u003ETensorflow\u003C\u002Fh4\u003E\n\u003Cp class=\"post_p\"\u003E\n\tIt is an open source library developed by Google for numerical computation using flow graphs. Before to execute\n\ta program, Tensorflow makes a flow graph where nodes represent mathematical operations and edges represent\n\tmultidimensional data vectors also called tensors. The construction of this graph avoids to obtain the most profit\n\tof system CPUs and GPUs where the program is executed. Then, completely transparent to the programmer, Tensorflow\n\tparallels everything it can among the resources it dispose.\n\u003C\u002Fp\u003E\n\u003Cp class=\"post_p\"\u003E\n\tThis library was originally designed for \u003Ca href=\"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FDeep_learning\" class=\"post_link\" target=\"_blank\"\u003Edeep learning\u003C\u002Fa\u003E,\n\tthe machine learning branch that studies neural networks. Tensorflow avoids, in a easy way, to implement Deep Neural\n\tNetworks (DNN), Convulational Neural Networks (CNN) and Recurrent Neural Networks (RNN). However last versions\n\thave focused on satisfying the rest of machine learning community by trying to convert the\n    library in a standard for programming models of all branches. Specifically they have developed a module called\n\t\u003Ca href=\"https:\u002F\u002Fgithub.com\u002Ftensorflow\u002Ftensorflow\u002Fblob\u002Fmaster\u002Ftensorflow\u002Fcontrib\u002Flearn\u002Fpython\u002Flearn\u002FREADME.md\" class=\"post_link\" target=\"_blank\"\u003ETFLearn\u003C\u002Fa\u003E\n\twhich has a set of models ready to use and also have updated its syntax with the intention to be more close to\n\t\u003Ca href=\"http:\u002F\u002Fscikit-learn.org\u002Fstable\u002F\" class=\"post_link\" target=\"_blank\"\u003EScikit-learn\u003C\u002Fa\u003E syntax which is one of the most popular and important\n\tmachine learning libraries.\n\u003C\u002Fp\u003E\n\u003Cp class=\"post_p\"\u003E\n\tOne of the most interesting aspects of this library is that it implements AD reverse model in a very elegant way.\n\tThe coder defines a model indicating its parameters as variables and practically automatically, after specify the\n\tinference algorithm, Tensorflow is in charge of calculating gradients and apply them in optimization procedures.\n\u003C\u002Fp\u003E\n\u003Ch4 class=\"post_section_title text-2xl text-bold\"\u003EUsage examples\u003C\u002Fh4\u003E\n\u003Cp class=\"post_p\"\u003E\n\tBelow is the code to optimize the parameters of a lineal regression model with Tensorflow and with Autograd (both\n\tuse AD reverse model to get gradients). A lineal regression model is defined by the equation:\n\u003C\u002Fp\u003E\n\u003Cdiv class=\"post_image_div md:px-64\"\u003E\n\t\u003Cfigure class=\"md:px-40\"\u003E\n\t\t\u003Cimg class=\"post_image\"\n\t\t\t src=\"automatic-differentiation\u002Flinear_regression.png\"\n\t\t\t alt=\"Lineal regression equation\"\u003E\n\t\u003C\u002Ffigure\u003E\n\u003C\u002Fdiv\u003E\n\u003Cp class=\"post_p\"\u003E\n\tWhere w represents the weight and b the bias. AD will find values for these parameters and these values\n\twill minimize Mean Squared Error.\n\u003C\u002Fp\u003E\n\u003Cp class=\"post_p\"\u003E\n\tThe model is defined as code in the following way. In Tensorflow parameters to be optimized of a function are\n\tdefined as variables (lines 29 and 30). Later a cost function is defined based on these parameters,\n\t\u003Ca href=\"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FMean_squared_error\" class=\"post_link\" target=\"_blank\"\u003EMean Squared Error\u003C\u002Fa\u003E (line 36). Then optimization\n\talgorithm is specified, in this case \u003Ca href=\"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FGradient_descent\" class=\"post_link\" target=\"_blank\"\u003EGradient Descent\u003C\u002Fa\u003E.\n\tAnd finally we write the code to train the model (last lines). This loop, in each iteration, get a sample from\n\tthe dataset and derive the cost function to obtain the direction (gradient vector) of the local minimum, in\n\tother words, the direction that reduces Mean Squared Error. With this gradient vector weight and bias parameters\n\twill be updated (transparently to the programmer). In this way, when a sufficient number of iterations have been\n\tmade, values for the parameters that minimize the cost function will have been obtained (a local minimum will\n\thave been found).\n\u003C\u002Fp\u003E\n\u003Cdiv class=\"post_code\"\u003E\n\t\u003Cpre\u003E\u003Ccode class=\"lang-python\"\u003Eimport matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow as tf\n\nrng = np.random\n\n# Parameters\nlearning_rate = 0.01\ntraining_epochs = 100\n\n# Training data\ntrain_X = np.asarray([3.3, 4.4, 5.5, 6.71, 6.93, 4.168, 9.779, 6.182, 7.59,\n                      2.167, 7.042, 10.791, 5.313, 7.997, 5.654, 9.27, 3.1])\ntrain_Y = np.asarray([1.7, 2.76, 2.09, 3.19, 1.694, 1.573, 3.366, 2.596, 2.53,\n                      1.221, 2.827, 3.465, 1.65, 2.904, 2.42, 2.94, 1.3])\nn_samples = train_X.shape[0]\n\n# Graph input data\nX = tf.placeholder('float')\nY = tf.placeholder('float')\n\n# Optimizable parameters with random initialization\nweight = tf.Variable(rng.randn(), name='weight')\nbias = tf.Variable(rng.randn(), name='bias')\n\n# Linear model\npredictions = (X * weight) + bias\n\n# Loss function: Mean Squared Error\nloss = tf.reduce_sum(tf.pow(predictions-Y, 2))\u002F(2*n_samples)\n\n# Gradient descent optimizer\noptimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n\n# Initializing the variables\ninit = tf.global_variables_initializer()\n\n# Launch the graph\nwith tf.Session() as sess:\n    sess.run(init)\n    for epoch in range(training_epochs):\n        for (x, y) in zip(train_X, train_Y):\n            sess.run(optimizer, feed_dict={X: x, Y: y})\n    train_error = sess.run(loss, feed_dict={X: train_X, Y: train_Y})\n    print('Train error={}'.format(train_error))\n\n    # Test error\n    test_X = np.asarray([6.83, 4.668, 8.9, 7.91, 5.7, 8.7, 3.1, 2.1])\n    test_Y = np.asarray([1.84, 2.273, 3.2, 2.831, 2.92, 3.24, 1.35, 1.03])\n    test_error = sess.run(\n        tf.reduce_sum(tf.pow(predictions - Y, 2)) \u002F (2 * test_X.shape[0]),\n        feed_dict={X: test_X, Y: test_Y})\n    print('Test error={}'.format(test_error))\n\n    print('Weight={} Bias={}'.format(sess.run(weight), sess.run(bias)))\n\n    # Graphic display\n    plt.plot(train_X, train_Y, 'ro', label='Original data')\n    plt.plot(train_X, sess.run(weight) * train_X\n             + sess.run(bias), label='Fitted line')\n    plt.legend()\n    plt.show()\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003C\u002Fdiv\u003E\n\u003Cdiv class=\"text-center\"\u003E\n\t\u003Csmall class=\"text-gray-900 leading-relaxed text-sm\"\u003E\n\t\tLearning weight and bias parameters of a linear regression model with Tensorflow.\n\t\u003C\u002Fsmall\u003E\n\u003C\u002Fdiv\u003E\n\u003Cdiv class=\"post_image_div md:px-40\"\u003E\n\t\u003Cfigure\u003E\n\t\t\u003Cimg class=\"post_image\"\n\t\t\t src=\"automatic-differentiation\u002Flinear_regression_tf_results.png\"\n\t\t\t alt=\"Linear regression with Tensoflow\"\u003E\n\t\t\u003Cfigcaption class=\"post_image_figcaption\"\u003E\n\t\t\tOptimization results of model parameters of a lineal regression using Tensorflow.\n\t\t\u003C\u002Ffigcaption\u003E\n\t\u003C\u002Ffigure\u003E\n\u003C\u002Fdiv\u003E\n\u003Cp class=\"post_p\"\u003E\n\tUsing Autograd all is more visible than in Tensorflow. A cost function is defined with the model parameters (lines\n\t25-31) and then get gradients in each iteration to update weight and bias parameters (lines 37-40).\n\u003C\u002Fp\u003E\n\u003Cdiv class=\"post_code\"\u003E\n\t\u003Cpre\u003E\u003Ccode class=\"lang-python\"\u003Eimport autograd.numpy as np\nimport matplotlib.pyplot as plt\nfrom autograd import elementwise_grad\n\nrng = np.random\n\n# Parameters\nlearning_rate = 0.01\ntraining_epochs = 100\n\n# Training data\ntrain_X = np.array([3.3, 4.4, 5.5, 6.71, 6.93, 4.168, 9.779, 6.182, 7.59,\n                    2.167, 7.042, 10.791, 5.313, 7.997, 5.654, 9.27, 3.1])\ntrain_Y = np.array([1.7, 2.76, 2.09, 3.19, 1.694, 1.573, 3.366, 2.596, 2.53,\n                    1.221, 2.827, 3.465, 1.65, 2.904, 2.42, 2.94, 1.3])\nn_samples = train_X.shape[0]\n\n\ndef loss((weight, bias)):\n    \"\"\" Loss function: Mean Squared Error \"\"\"\n    predictions = (train_X * weight) + bias\n    return np.sum(np.power(predictions - train_Y, 2) \u002F (2 * n_samples))\n\n# Function that returns gradients of loss function\ngradient_fun = elementwise_grad(loss)\n\n# Optimizable parameters with random initialization\nweight = rng.randn()\nbias = rng.randn()\n\nfor epoch in range(training_epochs):\n    gradients = gradient_fun((weight, bias))\n    weight -= gradients[0] * learning_rate\n    bias -= gradients[1] * learning_rate\nprint('Train error={}'.format(loss((weight, bias))))\n\n# Test error\ntest_X = np.array([6.83, 4.668, 8.9, 7.91, 5.7, 8.7, 3.1, 2.1])\ntest_Y = np.array([1.84, 2.273, 3.2, 2.831, 2.92, 3.24, 1.35, 1.03])\npredictions = (test_X * weight) + bias\nprint('Test error={}'.format(\n    np.sum(np.power(predictions - test_Y, 2) \u002F (2 * n_samples))))\n\nprint('Weight={} Bias={}'.format(weight, bias))\n\n# Graphic display\nplt.plot(train_X, train_Y, 'ro', label='Original data')\nplt.plot(train_X, weight * train_X + bias, label='Fitted line')\nplt.legend()\nplt.show()\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cdiv class=\"text-center\"\u003E\n\t\u003Csmall class=\"text-gray-900 leading-relaxed text-sm\"\u003E\n\t\tLearning weight and bias parameters of a linear regression model with Autograd.\n\t\u003C\u002Fsmall\u003E\n\u003C\u002Fdiv\u003E\n\u003Cdiv class=\"post_image_div md:px-40\"\u003E\n\t\u003Cfigure\u003E\n\t\t\u003Cimg class=\"post_image\"\n\t\t\t src=\"automatic-differentiation\u002Flinear_regression_ag_results.png\"\n\t\t\t alt=\"Linear regression with Tensoflow\"\u003E\n\t\t\u003Cfigcaption class=\"post_image_figcaption\"\u003E\n\t\t\tOptimization results of model parameters of a lineal regression using Autograd.\n\t\t\u003C\u002Ffigcaption\u003E\n\t\u003C\u002Ffigure\u003E\n\u003C\u002Fdiv\u003E\n\u003Cp class=\"post_p\"\u003E\n\tThe main objective of this post was to uncover a bit the black box that involves the optimization of models using\n\ttools as Tensorflow, Theano, \u003Ca href=\"http:\u002F\u002Fpytorch.org\u002F\" class=\"post_link\" target=\"_blank\"\u003EPytorch\u003C\u002Fa\u003E, ...\n\u003C\u002Fp\u003E\n\u003Ch4 class=\"post_section_title text-2xl text-bold\"\u003EReferences\u003C\u002Fh4\u003E\n\u003Cul class=\"post_list\"\u003E\n\t\u003Cli\u003E\n\t\tAutomatic differentiation in machine learning: a survey \u003Cbr\u003E\n\t\tAtilim Gunes Baydin, Barak A. Pearlmutter, Alexey Andreyevich Radul, Jeffrey Mark Siskind\n\t\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\t\t"}]}}("#python"))]};if('serviceWorker' in navigator)navigator.serviceWorker.register('/service-worker.js');(function(){try{eval("async function x(){}");var main="/client/client.cbf787a5.js"}catch(e){main="/client/legacy/client.f2d3b60e.js"};var s=document.createElement("script");try{new Function("if(0)import('')")();s.src=main;s.type="module";s.crossOrigin="use-credentials";}catch(e){s.src="/client/shimport@1.0.1.js";s.setAttribute("data-main",main);}document.head.appendChild(s);}());</script> 