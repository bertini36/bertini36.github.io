{"title":"Automatic differentiation","slug":"automatic-differentiation","tags":["#python","#tensorflow","#autograd","#linear-regression"],"date":"2017-05-03","html":"\n<p class=\"post_p\">\n\tDerivatives, specifically gradients (derivatives in more than one dimension spaces) and hesians (second\n\tderivatives), have become in a fundamental of machine learning. Gradient is a vector which indicates the maximum\n\tslope direction of a function at the evaluated point. This is important to move around the function space to find\n\trelative minimums or maximums (to minimize or maximize the function). Hesian gives information about the concavity\n\tand convexity of the function. Some algorithms use it to improve the exploratory movement over the function space\n\tand to find minimums and maximums faster. Automatic Differentiation (AD) is a efficiently and accurately\n\tprocedure of derivatives calculation of numerical functions, represented as computer programs. As already mentioned,\n\tthe use of derivatives is very important for model optimization. From a mathematical point of view a model is\n\tnothing more than a function that you want to minimize or maximize and derivatives are the tool to do it.\n</p>\n<p class=\"post_p\">\n\tThere are several methods of derivative calculation with a computer:\n</p>\n<ul class=\"post_list\">\n\t<li>\n\t\t<strong>Numerical differentiation.</strong>\n\t\tThis method uses derivative definition to approximate using samples of the original function. In this way\n\t\twe can approximate the gradient ∇f as:\n\t\t<div class=\"post_image_div md:px-64\">\n\t\t\t<figure>\n\t\t\t\t<img class=\"post_image\"\n\t\t\t\t\t src=\"automatic-differentiation/derivative_deffinition.png\"\n\t\t\t\t\t alt=\"Derivative deffinition\"/>\n\t\t\t</figure>\n\t\t</div>\n\t\twhere e<small>i</small> is i<small>th</small> unitary vector and h>0 is the step size for the approximation.\n\t</li>\n\t<li>\n\t\t<strong>Symbolic differentiation.</strong>\n\t\tIt consists on automatic manipulation of the mathematical expressions to obtain the derivatives (similar to\n\t\twhat we did at school). It requires to implement derivative rules. The problem of this kind of derivation is\n\t\tthat it can produce long symbolic expressions which are difficult to evaluate.\n\t</li>\n\t<li>\n\t\t<strong>Automatic Differentiation</strong>.\n\t\tIt is based on the fact that all functions can be decomposed into a finite number of operations which derivative\n\t\tis known. Combining these derivatives the derivative of the original function can be computed. Applying the\n\t\tchain rule to each elementary operation of the function we obtain the trace for the calculation of the\n\t\treal function derivative.\n\t</li>\n</ul>\n<p class=\"post_p\">\n\tNext image shows the differences between the three methods.\n</p>\n<div class=\"post_image_div md:px-10\">\n\t<figure>\n\t\t<img class=\"post_image\"\n\t\t\t src=\"automatic-differentiation/derivative_strategies.png\"\n\t\t\t alt=\"Derivative strategies\"/>\n\t\t<figcaption class=\"post_image_figcaption\">\n\t\t\tMethodologies for calculating derivatives.\n\t\t</figcaption>\n\t</figure>\n</div>\n<p class=\"post_p\">\n\tThere are two types of Automatic Differentiation (AD): forward mode and reverse mode. On the one hand forward mode\n\tevaluates the different parts of the function forward and then it does the same for each part of the derivative till\n\treal function derivative is obtained. On the other hand reverse mode evaluates the different parts of the function\n\tforward but after, from the derivative of the function, it obtains the partial derivatives.\n\tThis is how the backpropagation method works in neural networks, which needs the partial\n\tderivatives to update the weights of each of the neural network layers. This method avoids to reuse calculus\n\talready computed and to calculate derivatives in a very efficient way.\n</p>\n<div class=\"post_image_div md:px-40\">\n\t<figure>\n\t\t<img class=\"post_image\"\n\t\t\t src=\"automatic-differentiation/forward_AD.png\"\n\t\t\t alt=\"Automatic differentiation forward mode\"/>\n\t\t<figcaption class=\"post_image_figcaption\">\n\t\t\tAD forward mode schema.\n\t\t</figcaption>\n\t</figure>\n</div>\n<div class=\"post_image_div md:px-40\">\n\t<figure>\n\t\t<img class=\"post_image\"\n\t\t\t src=\"automatic-differentiation/backward_AD.png\"\n\t\t\t alt=\"Automatic differentiation reverse mode\"/>\n\t\t<figcaption class=\"post_image_figcaption\">\n\t\t\tAD reverse mode schema.\n\t\t</figcaption>\n\t</figure>\n</div>\n<h4 class=\"post_section_title text-2xl text-bold\">Derivatives tools</h4>\n<p class=\"post_p\">\n\tIn this post we will be focus on function parameters optimization using Automatic Differentiation. Here are listed\n\tsome software packages to calculate derivatives and gradients:\n</p>\n<ul class=\"post_list\">\n\t<li><a href=\"https://www.tensorflow.org/\" class=\"post_link\" target=\"_blank\">Tensorflow</a>: It uses AD reverse mode.</li>\n\t<li><a href=\"http://deeplearning.net/software/theano/\" class=\"post_link\" target=\"_blank\">Theano</a>: It uses symbolic differentiation.</li>\n\t<li><a href=\"https://www.wolfram.com/mathematica/\" class=\"post_link\" target=\"_blank\">Mathematica</a>: It uses symbolic differentiation.</li>\n\t<li><a href=\"https://github.com/HIPS/autograd\" class=\"post_link\" target=\"_blank\">Autograd</a>: It uses AD reverse mode.</li>\n</ul>\n<h4 class=\"post_section_title text-2xl text-bold\">Tensorflow</h4>\n<p class=\"post_p\">\n\tIt is an open source library developed by Google for numerical computation using flow graphs. Before to execute\n\ta program, Tensorflow makes a flow graph where nodes represent mathematical operations and edges represent\n\tmultidimensional data vectors also called tensors. The construction of this graph avoids to obtain the most profit\n\tof system CPUs and GPUs where the program is executed. Then, completely transparent to the programmer, Tensorflow\n\tparallels everything it can among the resources it dispose.\n</p>\n<p class=\"post_p\">\n\tThis library was originally designed for <a href=\"https://en.wikipedia.org/wiki/Deep_learning\" class=\"post_link\" target=\"_blank\">deep learning</a>,\n\tthe machine learning branch that studies neural networks. Tensorflow avoids, in a easy way, to implement Deep Neural\n\tNetworks (DNN), Convulational Neural Networks (CNN) and Recurrent Neural Networks (RNN). However last versions\n\thave focused on satisfying the rest of machine learning community by trying to convert the\n    library in a standard for programming models of all branches. Specifically they have developed a module called\n\t<a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/README.md\" class=\"post_link\" target=\"_blank\">TFLearn</a>\n\twhich has a set of models ready to use and also have updated its syntax with the intention to be more close to\n\t<a href=\"http://scikit-learn.org/stable/\" class=\"post_link\" target=\"_blank\">Scikit-learn</a> syntax which is one of the most popular and important\n\tmachine learning libraries.\n</p>\n<p class=\"post_p\">\n\tOne of the most interesting aspects of this library is that it implements AD reverse model in a very elegant way.\n\tThe coder defines a model indicating its parameters as variables and practically automatically, after specify the\n\tinference algorithm, Tensorflow is in charge of calculating gradients and apply them in optimization procedures.\n</p>\n<h4 class=\"post_section_title text-2xl text-bold\">Usage examples</h4>\n<p class=\"post_p\">\n\tBelow is the code to optimize the parameters of a lineal regression model with Tensorflow and with Autograd (both\n\tuse AD reverse model to get gradients). A lineal regression model is defined by the equation:\n</p>\n<div class=\"post_image_div md:px-64\">\n\t<figure class=\"md:px-40\">\n\t\t<img class=\"post_image\"\n\t\t\t src=\"automatic-differentiation/linear_regression.png\"\n\t\t\t alt=\"Lineal regression equation\">\n\t</figure>\n</div>\n<p class=\"post_p\">\n\tWhere w represents the weight and b the bias. AD will find values for these parameters and these values\n\twill minimize Mean Squared Error.\n</p>\n<p class=\"post_p\">\n\tThe model is defined as code in the following way. In Tensorflow parameters to be optimized of a function are\n\tdefined as variables (lines 29 and 30). Later a cost function is defined based on these parameters,\n\t<a href=\"https://en.wikipedia.org/wiki/Mean_squared_error\" class=\"post_link\" target=\"_blank\">Mean Squared Error</a> (line 36). Then optimization\n\talgorithm is specified, in this case <a href=\"https://en.wikipedia.org/wiki/Gradient_descent\" class=\"post_link\" target=\"_blank\">Gradient Descent</a>.\n\tAnd finally we write the code to train the model (last lines). This loop, in each iteration, get a sample from\n\tthe dataset and derive the cost function to obtain the direction (gradient vector) of the local minimum, in\n\tother words, the direction that reduces Mean Squared Error. With this gradient vector weight and bias parameters\n\twill be updated (transparently to the programmer). In this way, when a sufficient number of iterations have been\n\tmade, values for the parameters that minimize the cost function will have been obtained (a local minimum will\n\thave been found).\n</p>\n<div class=\"post_code\">\n\t<pre><code class=\"lang-python\">import matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow as tf\n\nrng = np.random\n\n# Parameters\nlearning_rate = 0.01\ntraining_epochs = 100\n\n# Training data\ntrain_X = np.asarray([3.3, 4.4, 5.5, 6.71, 6.93, 4.168, 9.779, 6.182, 7.59,\n                      2.167, 7.042, 10.791, 5.313, 7.997, 5.654, 9.27, 3.1])\ntrain_Y = np.asarray([1.7, 2.76, 2.09, 3.19, 1.694, 1.573, 3.366, 2.596, 2.53,\n                      1.221, 2.827, 3.465, 1.65, 2.904, 2.42, 2.94, 1.3])\nn_samples = train_X.shape[0]\n\n# Graph input data\nX = tf.placeholder('float')\nY = tf.placeholder('float')\n\n# Optimizable parameters with random initialization\nweight = tf.Variable(rng.randn(), name='weight')\nbias = tf.Variable(rng.randn(), name='bias')\n\n# Linear model\npredictions = (X * weight) + bias\n\n# Loss function: Mean Squared Error\nloss = tf.reduce_sum(tf.pow(predictions-Y, 2))/(2*n_samples)\n\n# Gradient descent optimizer\noptimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n\n# Initializing the variables\ninit = tf.global_variables_initializer()\n\n# Launch the graph\nwith tf.Session() as sess:\n    sess.run(init)\n    for epoch in range(training_epochs):\n        for (x, y) in zip(train_X, train_Y):\n            sess.run(optimizer, feed_dict={X: x, Y: y})\n    train_error = sess.run(loss, feed_dict={X: train_X, Y: train_Y})\n    print('Train error={}'.format(train_error))\n\n    # Test error\n    test_X = np.asarray([6.83, 4.668, 8.9, 7.91, 5.7, 8.7, 3.1, 2.1])\n    test_Y = np.asarray([1.84, 2.273, 3.2, 2.831, 2.92, 3.24, 1.35, 1.03])\n    test_error = sess.run(\n        tf.reduce_sum(tf.pow(predictions - Y, 2)) / (2 * test_X.shape[0]),\n        feed_dict={X: test_X, Y: test_Y})\n    print('Test error={}'.format(test_error))\n\n    print('Weight={} Bias={}'.format(sess.run(weight), sess.run(bias)))\n\n    # Graphic display\n    plt.plot(train_X, train_Y, 'ro', label='Original data')\n    plt.plot(train_X, sess.run(weight) * train_X\n             + sess.run(bias), label='Fitted line')\n    plt.legend()\n    plt.show()</code></pre>\n</div>\n<div class=\"text-center\">\n\t<small class=\"text-gray-900 leading-relaxed text-sm\">\n\t\tLearning weight and bias parameters of a linear regression model with Tensorflow.\n\t</small>\n</div>\n<div class=\"post_image_div md:px-40\">\n\t<figure>\n\t\t<img class=\"post_image\"\n\t\t\t src=\"automatic-differentiation/linear_regression_tf_results.png\"\n\t\t\t alt=\"Linear regression with Tensoflow\">\n\t\t<figcaption class=\"post_image_figcaption\">\n\t\t\tOptimization results of model parameters of a lineal regression using Tensorflow.\n\t\t</figcaption>\n\t</figure>\n</div>\n<p class=\"post_p\">\n\tUsing Autograd all is more visible than in Tensorflow. A cost function is defined with the model parameters (lines\n\t25-31) and then get gradients in each iteration to update weight and bias parameters (lines 37-40).\n</p>\n<div class=\"post_code\">\n\t<pre><code class=\"lang-python\">import autograd.numpy as np\nimport matplotlib.pyplot as plt\nfrom autograd import elementwise_grad\n\nrng = np.random\n\n# Parameters\nlearning_rate = 0.01\ntraining_epochs = 100\n\n# Training data\ntrain_X = np.array([3.3, 4.4, 5.5, 6.71, 6.93, 4.168, 9.779, 6.182, 7.59,\n                    2.167, 7.042, 10.791, 5.313, 7.997, 5.654, 9.27, 3.1])\ntrain_Y = np.array([1.7, 2.76, 2.09, 3.19, 1.694, 1.573, 3.366, 2.596, 2.53,\n                    1.221, 2.827, 3.465, 1.65, 2.904, 2.42, 2.94, 1.3])\nn_samples = train_X.shape[0]\n\n\ndef loss((weight, bias)):\n    \"\"\" Loss function: Mean Squared Error \"\"\"\n    predictions = (train_X * weight) + bias\n    return np.sum(np.power(predictions - train_Y, 2) / (2 * n_samples))\n\n# Function that returns gradients of loss function\ngradient_fun = elementwise_grad(loss)\n\n# Optimizable parameters with random initialization\nweight = rng.randn()\nbias = rng.randn()\n\nfor epoch in range(training_epochs):\n    gradients = gradient_fun((weight, bias))\n    weight -= gradients[0] * learning_rate\n    bias -= gradients[1] * learning_rate\nprint('Train error={}'.format(loss((weight, bias))))\n\n# Test error\ntest_X = np.array([6.83, 4.668, 8.9, 7.91, 5.7, 8.7, 3.1, 2.1])\ntest_Y = np.array([1.84, 2.273, 3.2, 2.831, 2.92, 3.24, 1.35, 1.03])\npredictions = (test_X * weight) + bias\nprint('Test error={}'.format(\n    np.sum(np.power(predictions - test_Y, 2) / (2 * n_samples))))\n\nprint('Weight={} Bias={}'.format(weight, bias))\n\n# Graphic display\nplt.plot(train_X, train_Y, 'ro', label='Original data')\nplt.plot(train_X, weight * train_X + bias, label='Fitted line')\nplt.legend()\nplt.show()</code></pre>\n<div class=\"text-center\">\n\t<small class=\"text-gray-900 leading-relaxed text-sm\">\n\t\tLearning weight and bias parameters of a linear regression model with Autograd.\n\t</small>\n</div>\n<div class=\"post_image_div md:px-40\">\n\t<figure>\n\t\t<img class=\"post_image\"\n\t\t\t src=\"automatic-differentiation/linear_regression_ag_results.png\"\n\t\t\t alt=\"Linear regression with Tensoflow\">\n\t\t<figcaption class=\"post_image_figcaption\">\n\t\t\tOptimization results of model parameters of a lineal regression using Autograd.\n\t\t</figcaption>\n\t</figure>\n</div>\n<p class=\"post_p\">\n\tThe main objective of this post was to uncover a bit the black box that involves the optimization of models using\n\ttools as Tensorflow, Theano, <a href=\"http://pytorch.org/\" class=\"post_link\" target=\"_blank\">Pytorch</a>, ...\n</p>\n<h4 class=\"post_section_title text-2xl text-bold\">References</h4>\n<ul class=\"post_list\">\n\t<li>\n\t\tAutomatic differentiation in machine learning: a survey <br>\n\t\tAtilim Gunes Baydin, Barak A. Pearlmutter, Alexey Andreyevich Radul, Jeffrey Mark Siskind\n\t</li>\n</ul>\n\t\t"}