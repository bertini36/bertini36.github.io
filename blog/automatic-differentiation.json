{"title":"Automatic differentiation","slug":"automatic-differentiation","tags":["#python","#tensorflow","#autograd","#linear-regression"],"date":"2017-05-03","html":"\n<div class=\"content\">\n\t<p class=\"post_p\">\n\t\tDerivatives, specifically gradients (derivatives in more than one dimension spaces) and hesians (second\n\t\tderivatives), have become in a fundamental of machine learning. Gradient is a vector which indicates the maximum\n\t\tslope direction of a function at the evaluated point. This is important to move around the function space to find\n\t\trelative minimums or maximums (to minimize or maximize the function). Hesian gives information about the concavity\n\t\tand convexity of the function. Some algorithms use it to improve the exploratory movement over the function space\n\t\tand to find minimums and maximums faster. Automatic Differentiation (AD) is a efficiently and accurately\n\t\tprocedure of derivatives calculation of numerical functions, represented as computer programs. As already mentioned,\n\t\tthe use of derivatives is very important for model optimization. From a mathematical point of view a model is\n\t\tnothing more than a function that you want to minimize or maximize and derivatives are the tool to do it.\n\t</p>\n\t<p class=\"post_p\">\n\t\tThere are several methods of derivative calculation with a computer:\n\t</p>\n\t<ul class=\"post_list\">\n\t\t<li>\n\t\t\t<strong>Numerical differentiation.</strong>\n\t\t\tThis method uses derivative definition to approximate using samples of the original function. In this way\n\t\t\twe can approximate the gradient âˆ‡f as:\n\t\t\t<div class=\"post_image_div md:px-64\">\n\t\t\t\t<figure>\n\t\t\t\t\t<img class=\"post_image\" src=\"automatic-differentiation/derivative_deffinition.png\" alt=\"Derivative deffinition\">\n\t\t\t\t</figure>\n\t\t\t</div>\n\t\t\twhere e<small>i</small> is i<small>th</small> unitary vector and h>0 is the step size for the approximation.\n\t\t</li>\n\t</ul>\n</div>\n\t\t"}