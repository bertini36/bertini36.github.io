<!doctype html> <html lang=en> <head> <meta charset=utf-8> <meta content="width=device-width,initial-scale=1" name=viewport> <meta content=#333333 name=theme-color> <base href=/ > <link href=https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css rel=stylesheet> <link href=global.css rel=stylesheet> <link href=manifest.json rel=manifest crossorigin=use-credentials> <link href=black_logo.png rel=icon type=image/png> <link href=client/main.3899701111.css rel=stylesheet> <noscript id=sapper-head-start></noscript><title>Automatic differentiation</title><noscript id=sapper-head-end></noscript> </head> <body> <div id=sapper> <nav class="svelte-71l2nh border-b-2 font-light px-4 py-0"><ul class="svelte-71l2nh m-0 p-0"><li class=float-left><a class="svelte-71l2nh block hover:text-red-500 no-underline px-3 py-4" href=.>Home</a></li> <li class=float-left><a class="svelte-71l2nh block hover:text-red-500 no-underline px-3 py-4" href=blog aria-current=page rel=prefetch>Blog</a></li> <li class=float-right><a class="block hover:text-red-500 no-underline px-3 py-4 text-black" href=https://github.com/bertini36/blog target=_blank><i class="fa fa-github"></i></a></ul></nav> <main class=svelte-1uhnsl8> <h1>Automatic differentiation</h1> <div class="content svelte-1l0v1qr"> <p class=post_p> Derivatives, specifically gradients (derivatives in more than one dimension spaces) and hesians (second derivatives), have become in a fundamental of machine learning. Gradient is a vector which indicates the maximum slope direction of a function at the evaluated point. This is important to move around the function space to find relative minimums or maximums (to minimize or maximize the function). Hesian gives information about the concavity and convexity of the function. Some algorithms use it to improve the exploratory movement over the function space and to find minimums and maximums faster. Automatic Differentiation (AD) is a efficiently and accurately procedure of derivatives calculation of numerical functions, represented as computer programs. As already mentioned, the use of derivatives is very important for model optimization. From a mathematical point of view a model is nothing more than a function that you want to minimize or maximize and derivatives are the tool to do it. </p> <p class=post_p> There are several methods of derivative calculation with a computer: </p> <ul class=post_list> <li> <strong>Numerical differentiation.</strong> This method uses derivative definition to approximate using samples of the original function. In this way we can approximate the gradient ∇f as: <div class="post_image_div md:px-64"> <figure> <img alt="Derivative deffinition" class=post_image src=automatic-differentiation/derivative_deffinition.png> </figure> </div> where e<small>i</small> is i<small>th</small> unitary vector and h>0 is the step size for the approximation. </li> <li> <strong>Symbolic differentiation.</strong> It consists on automatic manipulation of the mathematical expressions to obtain the derivatives (similar to what we did at school). It requires to implement derivative rules. The problem of this kind of derivation is that it can produce long symbolic expressions which are difficult to evaluate. </li> <li> <strong>Automatic Differentiation</strong>. It is based on the fact that all functions can be decomposed into a finite number of operations which derivative is known. Combining these derivatives the derivative of the original function can be computed. Applying the chain rule to each elementary operation of the function we obtain the trace for the calculation of the real function derivative. </li> </ul> <p class=post_p> Next image shows the differences between the three methods. </p> <div class="post_image_div md:px-10"> <figure> <img alt="Derivative strategies" class=post_image src=automatic-differentiation/derivative_strategies.png> <figcaption class=post_image_figcaption> Methodologies for calculating derivatives. </figcaption> </figure> </div> <p class=post_p> There are two types of Automatic Differentiation (AD): forward mode and reverse mode. On the one hand forward mode evaluates the different parts of the function forward and then it does the same for each part of the derivative till real function derivative is obtained. On the other hand reverse mode evaluates the different parts of the function forward but after, from the derivative of the function, it obtains the partial derivatives. This is how the backpropagation method works in neural networks, which needs the partial derivatives to update the weights of each of the neural network layers. This method avoids to reuse calculus already computed and to calculate derivatives in a very efficient way. </p> <div class="post_image_div md:px-40"> <figure> <img alt="Automatic differentiation forward mode" class=post_image src=automatic-differentiation/forward_AD.png> <figcaption class=post_image_figcaption> AD forward mode schema. </figcaption> </figure> </div> <div class="post_image_div md:px-40"> <figure> <img alt="Automatic differentiation reverse mode" class=post_image src=automatic-differentiation/backward_AD.png> <figcaption class=post_image_figcaption> AD reverse mode schema. </figcaption> </figure> </div> <h4 class="post_section_title text-2xl text-bold">Derivatives tools</h4> <p class=post_p> In this post we will be focus on function parameters optimization using Automatic Differentiation. Here are listed some software packages to calculate derivatives and gradients: </p> <ul class=post_list> <li><a class=post_link href=https://www.tensorflow.org/ target=_blank>Tensorflow</a>: It uses AD reverse mode.</li> <li><a class=post_link href=http://deeplearning.net/software/theano/ target=_blank>Theano</a>: It uses symbolic differentiation.</li> <li><a class=post_link href=https://www.wolfram.com/mathematica/ target=_blank>Mathematica</a>: It uses symbolic differentiation.</li> <li><a class=post_link href=https://github.com/HIPS/autograd target=_blank>Autograd</a>: It uses AD reverse mode.</li> </ul> <h4 class="post_section_title text-2xl text-bold">Tensorflow</h4> <p class=post_p> It is an open source library developed by Google for numerical computation using flow graphs. Before to execute a program, Tensorflow makes a flow graph where nodes represent mathematical operations and edges represent multidimensional data vectors also called tensors. The construction of this graph avoids to obtain the most profit of system CPUs and GPUs where the program is executed. Then, completely transparent to the programmer, Tensorflow parallels everything it can among the resources it dispose. </p> <p class=post_p> This library was originally designed for <a class=post_link href=https://en.wikipedia.org/wiki/Deep_learning target=_blank>deep learning</a>, the machine learning branch that studies neural networks. Tensorflow avoids, in a easy way, to implement Deep Neural Networks (DNN), Convulational Neural Networks (CNN) and Recurrent Neural Networks (RNN). However last versions have focused on satisfying the rest of machine learning community by trying to convert the     library in a standard for programming models of all branches. Specifically they have developed a module called <a class=post_link href=https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/README.md target=_blank>TFLearn</a> which has a set of models ready to use and also have updated its syntax with the intention to be more close to <a class=post_link href=http://scikit-learn.org/stable/ target=_blank>Scikit-learn</a> syntax which is one of the most popular and important machine learning libraries. </p> <p class=post_p> One of the most interesting aspects of this library is that it implements AD reverse model in a very elegant way. The coder defines a model indicating its parameters as variables and practically automatically, after specify the inference algorithm, Tensorflow is in charge of calculating gradients and apply them in optimization procedures. </p> <h4 class="post_section_title text-2xl text-bold">Usage examples</h4> <p class=post_p> Below is the code to optimize the parameters of a lineal regression model with Tensorflow and with Autograd (both use AD reverse model to get gradients). A lineal regression model is defined by the equation: </p> <div class="post_image_div md:px-64"> <figure class=md:px-40> <img alt="Lineal regression equation" class=post_image src=automatic-differentiation/linear_regression.png> </figure> </div> <p class=post_p> Where w represents the weight and b the bias. AD will find values for these parameters and these values will minimize Mean Squared Error. </p> <p class=post_p> The model is defined as code in the following way. In Tensorflow parameters to be optimized of a function are defined as variables (lines 29 and 30). Later a cost function is defined based on these parameters, <a class=post_link href=https://en.wikipedia.org/wiki/Mean_squared_error target=_blank>Mean Squared Error</a> (line 36). Then optimization algorithm is specified, in this case <a class=post_link href=https://en.wikipedia.org/wiki/Gradient_descent target=_blank>Gradient Descent</a>. And finally we write the code to train the model (last lines). This loop, in each iteration, get a sample from the dataset and derive the cost function to obtain the direction (gradient vector) of the local minimum, in other words, the direction that reduces Mean Squared Error. With this gradient vector weight and bias parameters will be updated (transparently to the programmer). In this way, when a sufficient number of iterations have been made, values for the parameters that minimize the cost function will have been obtained (a local minimum will have been found). </p> <div class=post_code> <pre><code class=lang-python>import matplotlib.pyplot as plt
import numpy as np
import tensorflow as tf

rng = np.random

# Parameters
learning_rate = 0.01
training_epochs = 100

# Training data
train_X = np.asarray([3.3, 4.4, 5.5, 6.71, 6.93, 4.168, 9.779, 6.182, 7.59,
                      2.167, 7.042, 10.791, 5.313, 7.997, 5.654, 9.27, 3.1])
train_Y = np.asarray([1.7, 2.76, 2.09, 3.19, 1.694, 1.573, 3.366, 2.596, 2.53,
                      1.221, 2.827, 3.465, 1.65, 2.904, 2.42, 2.94, 1.3])
n_samples = train_X.shape[0]

# Graph input data
X = tf.placeholder('float')
Y = tf.placeholder('float')

# Optimizable parameters with random initialization
weight = tf.Variable(rng.randn(), name='weight')
bias = tf.Variable(rng.randn(), name='bias')

# Linear model
predictions = (X * weight) + bias

# Loss function: Mean Squared Error
loss = tf.reduce_sum(tf.pow(predictions-Y, 2))/(2*n_samples)

# Gradient descent optimizer
optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)

# Initializing the variables
init = tf.global_variables_initializer()

# Launch the graph
with tf.Session() as sess:
    sess.run(init)
    for epoch in range(training_epochs):
        for (x, y) in zip(train_X, train_Y):
            sess.run(optimizer, feed_dict={X: x, Y: y})
    train_error = sess.run(loss, feed_dict={X: train_X, Y: train_Y})
    print('Train error={}'.format(train_error))

    # Test error
    test_X = np.asarray([6.83, 4.668, 8.9, 7.91, 5.7, 8.7, 3.1, 2.1])
    test_Y = np.asarray([1.84, 2.273, 3.2, 2.831, 2.92, 3.24, 1.35, 1.03])
    test_error = sess.run(
        tf.reduce_sum(tf.pow(predictions - Y, 2)) / (2 * test_X.shape[0]),
        feed_dict={X: test_X, Y: test_Y})
    print('Test error={}'.format(test_error))

    print('Weight={} Bias={}'.format(sess.run(weight), sess.run(bias)))

    # Graphic display
    plt.plot(train_X, train_Y, 'ro', label='Original data')
    plt.plot(train_X, sess.run(weight) * train_X
             + sess.run(bias), label='Fitted line')
    plt.legend()
    plt.show()</code></pre> </div> <div class=text-center> <small class="leading-relaxed text-gray-900 text-sm"> Learning weight and bias parameters of a linear regression model with Tensorflow. </small> </div> <div class="post_image_div md:px-40"> <figure> <img alt="Linear regression with Tensoflow" class=post_image src=automatic-differentiation/linear_regression_tf_results.png> <figcaption class=post_image_figcaption> Optimization results of model parameters of a lineal regression using Tensorflow. </figcaption> </figure> </div> <p class=post_p> Using Autograd all is more visible than in Tensorflow. A cost function is defined with the model parameters (lines 25-31) and then get gradients in each iteration to update weight and bias parameters (lines 37-40). </p> <div class=post_code> <pre><code class=lang-python>import autograd.numpy as np
import matplotlib.pyplot as plt
from autograd import elementwise_grad

rng = np.random

# Parameters
learning_rate = 0.01
training_epochs = 100

# Training data
train_X = np.array([3.3, 4.4, 5.5, 6.71, 6.93, 4.168, 9.779, 6.182, 7.59,
                    2.167, 7.042, 10.791, 5.313, 7.997, 5.654, 9.27, 3.1])
train_Y = np.array([1.7, 2.76, 2.09, 3.19, 1.694, 1.573, 3.366, 2.596, 2.53,
                    1.221, 2.827, 3.465, 1.65, 2.904, 2.42, 2.94, 1.3])
n_samples = train_X.shape[0]


def loss((weight, bias)):
    """ Loss function: Mean Squared Error """
    predictions = (train_X * weight) + bias
    return np.sum(np.power(predictions - train_Y, 2) / (2 * n_samples))

# Function that returns gradients of loss function
gradient_fun = elementwise_grad(loss)

# Optimizable parameters with random initialization
weight = rng.randn()
bias = rng.randn()

for epoch in range(training_epochs):
    gradients = gradient_fun((weight, bias))
    weight -= gradients[0] * learning_rate
    bias -= gradients[1] * learning_rate
print('Train error={}'.format(loss((weight, bias))))

# Test error
test_X = np.array([6.83, 4.668, 8.9, 7.91, 5.7, 8.7, 3.1, 2.1])
test_Y = np.array([1.84, 2.273, 3.2, 2.831, 2.92, 3.24, 1.35, 1.03])
predictions = (test_X * weight) + bias
print('Test error={}'.format(
    np.sum(np.power(predictions - test_Y, 2) / (2 * n_samples))))

print('Weight={} Bias={}'.format(weight, bias))

# Graphic display
plt.plot(train_X, train_Y, 'ro', label='Original data')
plt.plot(train_X, weight * train_X + bias, label='Fitted line')
plt.legend()
plt.show()</code></pre> <div class=text-center> <small class="leading-relaxed text-gray-900 text-sm"> Learning weight and bias parameters of a linear regression model with Autograd. </small> </div> <div class="post_image_div md:px-40"> <figure> <img alt="Linear regression with Tensoflow" class=post_image src=automatic-differentiation/linear_regression_ag_results.png> <figcaption class=post_image_figcaption> Optimization results of model parameters of a lineal regression using Autograd. </figcaption> </figure> </div> <p class=post_p> The main objective of this post was to uncover a bit the black box that involves the optimization of models using tools as Tensorflow, Theano, <a class=post_link href=http://pytorch.org/ target=_blank>Pytorch</a>, ... </p> <h4 class="post_section_title text-2xl text-bold">References</h4> <ul class=post_list> <li> Automatic differentiation in machine learning: a survey <br> Atilim Gunes Baydin, Barak A. Pearlmutter, Alexey Andreyevich Radul, Jeffrey Mark Siskind </li> </ul> </div></div></main></div> <script>__SAPPER__={baseUrl:"",preloaded:[void 0,null,{post:{title:"Automatic differentiation",slug:"automatic-differentiation",tags:["#python","#tensorflow","#autograd","#linear-regression"],date:"2017-05-03",html:"\n\u003Cp class=\"post_p\"\u003E\n\tDerivatives, specifically gradients (derivatives in more than one dimension spaces) and hesians (second\n\tderivatives), have become in a fundamental of machine learning. Gradient is a vector which indicates the maximum\n\tslope direction of a function at the evaluated point. This is important to move around the function space to find\n\trelative minimums or maximums (to minimize or maximize the function). Hesian gives information about the concavity\n\tand convexity of the function. Some algorithms use it to improve the exploratory movement over the function space\n\tand to find minimums and maximums faster. Automatic Differentiation (AD) is a efficiently and accurately\n\tprocedure of derivatives calculation of numerical functions, represented as computer programs. As already mentioned,\n\tthe use of derivatives is very important for model optimization. From a mathematical point of view a model is\n\tnothing more than a function that you want to minimize or maximize and derivatives are the tool to do it.\n\u003C\u002Fp\u003E\n\u003Cp class=\"post_p\"\u003E\n\tThere are several methods of derivative calculation with a computer:\n\u003C\u002Fp\u003E\n\u003Cul class=\"post_list\"\u003E\n\t\u003Cli\u003E\n\t\t\u003Cstrong\u003ENumerical differentiation.\u003C\u002Fstrong\u003E\n\t\tThis method uses derivative definition to approximate using samples of the original function. In this way\n\t\twe can approximate the gradient ∇f as:\n\t\t\u003Cdiv class=\"post_image_div md:px-64\"\u003E\n\t\t\t\u003Cfigure\u003E\n\t\t\t\t\u003Cimg class=\"post_image\"\n\t\t\t\t\t src=\"automatic-differentiation\u002Fderivative_deffinition.png\"\n\t\t\t\t\t alt=\"Derivative deffinition\"\u003E\n\t\t\t\u003C\u002Ffigure\u003E\n\t\t\u003C\u002Fdiv\u003E\n\t\twhere e\u003Csmall\u003Ei\u003C\u002Fsmall\u003E is i\u003Csmall\u003Eth\u003C\u002Fsmall\u003E unitary vector and h\u003E0 is the step size for the approximation.\n\t\u003C\u002Fli\u003E\n\t\u003Cli\u003E\n\t\t\u003Cstrong\u003ESymbolic differentiation.\u003C\u002Fstrong\u003E\n\t\tIt consists on automatic manipulation of the mathematical expressions to obtain the derivatives (similar to\n\t\twhat we did at school). It requires to implement derivative rules. The problem of this kind of derivation is\n\t\tthat it can produce long symbolic expressions which are difficult to evaluate.\n\t\u003C\u002Fli\u003E\n\t\u003Cli\u003E\n\t\t\u003Cstrong\u003EAutomatic Differentiation\u003C\u002Fstrong\u003E.\n\t\tIt is based on the fact that all functions can be decomposed into a finite number of operations which derivative\n\t\tis known. Combining these derivatives the derivative of the original function can be computed. Applying the\n\t\tchain rule to each elementary operation of the function we obtain the trace for the calculation of the\n\t\treal function derivative.\n\t\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Cp class=\"post_p\"\u003E\n\tNext image shows the differences between the three methods.\n\u003C\u002Fp\u003E\n\u003Cdiv class=\"post_image_div md:px-10\"\u003E\n\t\u003Cfigure\u003E\n\t\t\u003Cimg class=\"post_image\"\n\t\t\t src=\"automatic-differentiation\u002Fderivative_strategies.png\"\n\t\t\t alt=\"Derivative strategies\"\u003E\n\t\t\u003Cfigcaption class=\"post_image_figcaption\"\u003E\n\t\t\tMethodologies for calculating derivatives.\n\t\t\u003C\u002Ffigcaption\u003E\n\t\u003C\u002Ffigure\u003E\n\u003C\u002Fdiv\u003E\n\u003Cp class=\"post_p\"\u003E\n\tThere are two types of Automatic Differentiation (AD): forward mode and reverse mode. On the one hand forward mode\n\tevaluates the different parts of the function forward and then it does the same for each part of the derivative till\n\treal function derivative is obtained. On the other hand reverse mode evaluates the different parts of the function\n\tforward but after, from the derivative of the function, it obtains the partial derivatives.\n\tThis is how the backpropagation method works in neural networks, which needs the partial\n\tderivatives to update the weights of each of the neural network layers. This method avoids to reuse calculus\n\talready computed and to calculate derivatives in a very efficient way.\n\u003C\u002Fp\u003E\n\u003Cdiv class=\"post_image_div md:px-40\"\u003E\n\t\u003Cfigure\u003E\n\t\t\u003Cimg class=\"post_image\"\n\t\t\t src=\"automatic-differentiation\u002Fforward_AD.png\"\n\t\t\t alt=\"Automatic differentiation forward mode\"\u003E\n\t\t\u003Cfigcaption class=\"post_image_figcaption\"\u003E\n\t\t\tAD forward mode schema.\n\t\t\u003C\u002Ffigcaption\u003E\n\t\u003C\u002Ffigure\u003E\n\u003C\u002Fdiv\u003E\n\u003Cdiv class=\"post_image_div md:px-40\"\u003E\n\t\u003Cfigure\u003E\n\t\t\u003Cimg class=\"post_image\"\n\t\t\t src=\"automatic-differentiation\u002Fbackward_AD.png\"\n\t\t\t alt=\"Automatic differentiation reverse mode\"\u003E\n\t\t\u003Cfigcaption class=\"post_image_figcaption\"\u003E\n\t\t\tAD reverse mode schema.\n\t\t\u003C\u002Ffigcaption\u003E\n\t\u003C\u002Ffigure\u003E\n\u003C\u002Fdiv\u003E\n\u003Ch4 class=\"post_section_title text-2xl text-bold\"\u003EDerivatives tools\u003C\u002Fh4\u003E\n\u003Cp class=\"post_p\"\u003E\n\tIn this post we will be focus on function parameters optimization using Automatic Differentiation. Here are listed\n\tsome software packages to calculate derivatives and gradients:\n\u003C\u002Fp\u003E\n\u003Cul class=\"post_list\"\u003E\n\t\u003Cli\u003E\u003Ca href=\"https:\u002F\u002Fwww.tensorflow.org\u002F\" class=\"post_link\" target=\"_blank\"\u003ETensorflow\u003C\u002Fa\u003E: It uses AD reverse mode.\u003C\u002Fli\u003E\n\t\u003Cli\u003E\u003Ca href=\"http:\u002F\u002Fdeeplearning.net\u002Fsoftware\u002Ftheano\u002F\" class=\"post_link\" target=\"_blank\"\u003ETheano\u003C\u002Fa\u003E: It uses symbolic differentiation.\u003C\u002Fli\u003E\n\t\u003Cli\u003E\u003Ca href=\"https:\u002F\u002Fwww.wolfram.com\u002Fmathematica\u002F\" class=\"post_link\" target=\"_blank\"\u003EMathematica\u003C\u002Fa\u003E: It uses symbolic differentiation.\u003C\u002Fli\u003E\n\t\u003Cli\u003E\u003Ca href=\"https:\u002F\u002Fgithub.com\u002FHIPS\u002Fautograd\" class=\"post_link\" target=\"_blank\"\u003EAutograd\u003C\u002Fa\u003E: It uses AD reverse mode.\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\u003Ch4 class=\"post_section_title text-2xl text-bold\"\u003ETensorflow\u003C\u002Fh4\u003E\n\u003Cp class=\"post_p\"\u003E\n\tIt is an open source library developed by Google for numerical computation using flow graphs. Before to execute\n\ta program, Tensorflow makes a flow graph where nodes represent mathematical operations and edges represent\n\tmultidimensional data vectors also called tensors. The construction of this graph avoids to obtain the most profit\n\tof system CPUs and GPUs where the program is executed. Then, completely transparent to the programmer, Tensorflow\n\tparallels everything it can among the resources it dispose.\n\u003C\u002Fp\u003E\n\u003Cp class=\"post_p\"\u003E\n\tThis library was originally designed for \u003Ca href=\"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FDeep_learning\" class=\"post_link\" target=\"_blank\"\u003Edeep learning\u003C\u002Fa\u003E,\n\tthe machine learning branch that studies neural networks. Tensorflow avoids, in a easy way, to implement Deep Neural\n\tNetworks (DNN), Convulational Neural Networks (CNN) and Recurrent Neural Networks (RNN). However last versions\n\thave focused on satisfying the rest of machine learning community by trying to convert the\n    library in a standard for programming models of all branches. Specifically they have developed a module called\n\t\u003Ca href=\"https:\u002F\u002Fgithub.com\u002Ftensorflow\u002Ftensorflow\u002Fblob\u002Fmaster\u002Ftensorflow\u002Fcontrib\u002Flearn\u002Fpython\u002Flearn\u002FREADME.md\" class=\"post_link\" target=\"_blank\"\u003ETFLearn\u003C\u002Fa\u003E\n\twhich has a set of models ready to use and also have updated its syntax with the intention to be more close to\n\t\u003Ca href=\"http:\u002F\u002Fscikit-learn.org\u002Fstable\u002F\" class=\"post_link\" target=\"_blank\"\u003EScikit-learn\u003C\u002Fa\u003E syntax which is one of the most popular and important\n\tmachine learning libraries.\n\u003C\u002Fp\u003E\n\u003Cp class=\"post_p\"\u003E\n\tOne of the most interesting aspects of this library is that it implements AD reverse model in a very elegant way.\n\tThe coder defines a model indicating its parameters as variables and practically automatically, after specify the\n\tinference algorithm, Tensorflow is in charge of calculating gradients and apply them in optimization procedures.\n\u003C\u002Fp\u003E\n\u003Ch4 class=\"post_section_title text-2xl text-bold\"\u003EUsage examples\u003C\u002Fh4\u003E\n\u003Cp class=\"post_p\"\u003E\n\tBelow is the code to optimize the parameters of a lineal regression model with Tensorflow and with Autograd (both\n\tuse AD reverse model to get gradients). A lineal regression model is defined by the equation:\n\u003C\u002Fp\u003E\n\u003Cdiv class=\"post_image_div md:px-64\"\u003E\n\t\u003Cfigure class=\"md:px-40\"\u003E\n\t\t\u003Cimg class=\"post_image\"\n\t\t\t src=\"automatic-differentiation\u002Flinear_regression.png\"\n\t\t\t alt=\"Lineal regression equation\"\u003E\n\t\u003C\u002Ffigure\u003E\n\u003C\u002Fdiv\u003E\n\u003Cp class=\"post_p\"\u003E\n\tWhere w represents the weight and b the bias. AD will find values for these parameters and these values\n\twill minimize Mean Squared Error.\n\u003C\u002Fp\u003E\n\u003Cp class=\"post_p\"\u003E\n\tThe model is defined as code in the following way. In Tensorflow parameters to be optimized of a function are\n\tdefined as variables (lines 29 and 30). Later a cost function is defined based on these parameters,\n\t\u003Ca href=\"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FMean_squared_error\" class=\"post_link\" target=\"_blank\"\u003EMean Squared Error\u003C\u002Fa\u003E (line 36). Then optimization\n\talgorithm is specified, in this case \u003Ca href=\"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FGradient_descent\" class=\"post_link\" target=\"_blank\"\u003EGradient Descent\u003C\u002Fa\u003E.\n\tAnd finally we write the code to train the model (last lines). This loop, in each iteration, get a sample from\n\tthe dataset and derive the cost function to obtain the direction (gradient vector) of the local minimum, in\n\tother words, the direction that reduces Mean Squared Error. With this gradient vector weight and bias parameters\n\twill be updated (transparently to the programmer). In this way, when a sufficient number of iterations have been\n\tmade, values for the parameters that minimize the cost function will have been obtained (a local minimum will\n\thave been found).\n\u003C\u002Fp\u003E\n\u003Cdiv class=\"post_code\"\u003E\n\t\u003Cpre\u003E\u003Ccode class=\"lang-python\"\u003Eimport matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow as tf\n\nrng = np.random\n\n# Parameters\nlearning_rate = 0.01\ntraining_epochs = 100\n\n# Training data\ntrain_X = np.asarray([3.3, 4.4, 5.5, 6.71, 6.93, 4.168, 9.779, 6.182, 7.59,\n                      2.167, 7.042, 10.791, 5.313, 7.997, 5.654, 9.27, 3.1])\ntrain_Y = np.asarray([1.7, 2.76, 2.09, 3.19, 1.694, 1.573, 3.366, 2.596, 2.53,\n                      1.221, 2.827, 3.465, 1.65, 2.904, 2.42, 2.94, 1.3])\nn_samples = train_X.shape[0]\n\n# Graph input data\nX = tf.placeholder('float')\nY = tf.placeholder('float')\n\n# Optimizable parameters with random initialization\nweight = tf.Variable(rng.randn(), name='weight')\nbias = tf.Variable(rng.randn(), name='bias')\n\n# Linear model\npredictions = (X * weight) + bias\n\n# Loss function: Mean Squared Error\nloss = tf.reduce_sum(tf.pow(predictions-Y, 2))\u002F(2*n_samples)\n\n# Gradient descent optimizer\noptimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n\n# Initializing the variables\ninit = tf.global_variables_initializer()\n\n# Launch the graph\nwith tf.Session() as sess:\n    sess.run(init)\n    for epoch in range(training_epochs):\n        for (x, y) in zip(train_X, train_Y):\n            sess.run(optimizer, feed_dict={X: x, Y: y})\n    train_error = sess.run(loss, feed_dict={X: train_X, Y: train_Y})\n    print('Train error={}'.format(train_error))\n\n    # Test error\n    test_X = np.asarray([6.83, 4.668, 8.9, 7.91, 5.7, 8.7, 3.1, 2.1])\n    test_Y = np.asarray([1.84, 2.273, 3.2, 2.831, 2.92, 3.24, 1.35, 1.03])\n    test_error = sess.run(\n        tf.reduce_sum(tf.pow(predictions - Y, 2)) \u002F (2 * test_X.shape[0]),\n        feed_dict={X: test_X, Y: test_Y})\n    print('Test error={}'.format(test_error))\n\n    print('Weight={} Bias={}'.format(sess.run(weight), sess.run(bias)))\n\n    # Graphic display\n    plt.plot(train_X, train_Y, 'ro', label='Original data')\n    plt.plot(train_X, sess.run(weight) * train_X\n             + sess.run(bias), label='Fitted line')\n    plt.legend()\n    plt.show()\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003C\u002Fdiv\u003E\n\u003Cdiv class=\"text-center\"\u003E\n\t\u003Csmall class=\"text-gray-900 leading-relaxed text-sm\"\u003E\n\t\tLearning weight and bias parameters of a linear regression model with Tensorflow.\n\t\u003C\u002Fsmall\u003E\n\u003C\u002Fdiv\u003E\n\u003Cdiv class=\"post_image_div md:px-40\"\u003E\n\t\u003Cfigure\u003E\n\t\t\u003Cimg class=\"post_image\"\n\t\t\t src=\"automatic-differentiation\u002Flinear_regression_tf_results.png\"\n\t\t\t alt=\"Linear regression with Tensoflow\"\u003E\n\t\t\u003Cfigcaption class=\"post_image_figcaption\"\u003E\n\t\t\tOptimization results of model parameters of a lineal regression using Tensorflow.\n\t\t\u003C\u002Ffigcaption\u003E\n\t\u003C\u002Ffigure\u003E\n\u003C\u002Fdiv\u003E\n\u003Cp class=\"post_p\"\u003E\n\tUsing Autograd all is more visible than in Tensorflow. A cost function is defined with the model parameters (lines\n\t25-31) and then get gradients in each iteration to update weight and bias parameters (lines 37-40).\n\u003C\u002Fp\u003E\n\u003Cdiv class=\"post_code\"\u003E\n\t\u003Cpre\u003E\u003Ccode class=\"lang-python\"\u003Eimport autograd.numpy as np\nimport matplotlib.pyplot as plt\nfrom autograd import elementwise_grad\n\nrng = np.random\n\n# Parameters\nlearning_rate = 0.01\ntraining_epochs = 100\n\n# Training data\ntrain_X = np.array([3.3, 4.4, 5.5, 6.71, 6.93, 4.168, 9.779, 6.182, 7.59,\n                    2.167, 7.042, 10.791, 5.313, 7.997, 5.654, 9.27, 3.1])\ntrain_Y = np.array([1.7, 2.76, 2.09, 3.19, 1.694, 1.573, 3.366, 2.596, 2.53,\n                    1.221, 2.827, 3.465, 1.65, 2.904, 2.42, 2.94, 1.3])\nn_samples = train_X.shape[0]\n\n\ndef loss((weight, bias)):\n    \"\"\" Loss function: Mean Squared Error \"\"\"\n    predictions = (train_X * weight) + bias\n    return np.sum(np.power(predictions - train_Y, 2) \u002F (2 * n_samples))\n\n# Function that returns gradients of loss function\ngradient_fun = elementwise_grad(loss)\n\n# Optimizable parameters with random initialization\nweight = rng.randn()\nbias = rng.randn()\n\nfor epoch in range(training_epochs):\n    gradients = gradient_fun((weight, bias))\n    weight -= gradients[0] * learning_rate\n    bias -= gradients[1] * learning_rate\nprint('Train error={}'.format(loss((weight, bias))))\n\n# Test error\ntest_X = np.array([6.83, 4.668, 8.9, 7.91, 5.7, 8.7, 3.1, 2.1])\ntest_Y = np.array([1.84, 2.273, 3.2, 2.831, 2.92, 3.24, 1.35, 1.03])\npredictions = (test_X * weight) + bias\nprint('Test error={}'.format(\n    np.sum(np.power(predictions - test_Y, 2) \u002F (2 * n_samples))))\n\nprint('Weight={} Bias={}'.format(weight, bias))\n\n# Graphic display\nplt.plot(train_X, train_Y, 'ro', label='Original data')\nplt.plot(train_X, weight * train_X + bias, label='Fitted line')\nplt.legend()\nplt.show()\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\n\u003Cdiv class=\"text-center\"\u003E\n\t\u003Csmall class=\"text-gray-900 leading-relaxed text-sm\"\u003E\n\t\tLearning weight and bias parameters of a linear regression model with Autograd.\n\t\u003C\u002Fsmall\u003E\n\u003C\u002Fdiv\u003E\n\u003Cdiv class=\"post_image_div md:px-40\"\u003E\n\t\u003Cfigure\u003E\n\t\t\u003Cimg class=\"post_image\"\n\t\t\t src=\"automatic-differentiation\u002Flinear_regression_ag_results.png\"\n\t\t\t alt=\"Linear regression with Tensoflow\"\u003E\n\t\t\u003Cfigcaption class=\"post_image_figcaption\"\u003E\n\t\t\tOptimization results of model parameters of a lineal regression using Autograd.\n\t\t\u003C\u002Ffigcaption\u003E\n\t\u003C\u002Ffigure\u003E\n\u003C\u002Fdiv\u003E\n\u003Cp class=\"post_p\"\u003E\n\tThe main objective of this post was to uncover a bit the black box that involves the optimization of models using\n\ttools as Tensorflow, Theano, \u003Ca href=\"http:\u002F\u002Fpytorch.org\u002F\" class=\"post_link\" target=\"_blank\"\u003EPytorch\u003C\u002Fa\u003E, ...\n\u003C\u002Fp\u003E\n\u003Ch4 class=\"post_section_title text-2xl text-bold\"\u003EReferences\u003C\u002Fh4\u003E\n\u003Cul class=\"post_list\"\u003E\n\t\u003Cli\u003E\n\t\tAutomatic differentiation in machine learning: a survey \u003Cbr\u003E\n\t\tAtilim Gunes Baydin, Barak A. Pearlmutter, Alexey Andreyevich Radul, Jeffrey Mark Siskind\n\t\u003C\u002Fli\u003E\n\u003C\u002Ful\u003E\n\t\t"}}]};if('serviceWorker' in navigator)navigator.serviceWorker.register('/service-worker.js');(function(){try{eval("async function x(){}");var main="/client/client.cbf787a5.js"}catch(e){main="/client/legacy/client.f2d3b60e.js"};var s=document.createElement("script");try{new Function("if(0)import('')")();s.src=main;s.type="module";s.crossOrigin="use-credentials";}catch(e){s.src="/client/shimport@1.0.1.js";s.setAttribute("data-main",main);}document.head.appendChild(s);}());</script> 