<!doctype html> <html lang=en> <head> <meta charset=utf-8> <meta content="width=device-width,initial-scale=1" name=viewport> <meta content=#333333 name=theme-color> <base href=/ > <link href=https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css rel=stylesheet> <link href=global.css rel=stylesheet> <link href=manifest.json rel=manifest crossorigin=use-credentials> <link href=black_logo.png rel=icon type=image/png> <link href=client/main.2531131659.css rel=stylesheet> <noscript id=sapper-head-start></noscript><title>Automatic differentiation</title><noscript id=sapper-head-end></noscript> </head> <body> <div id=sapper> <nav class="svelte-71l2nh border-b-2 font-light px-4 py-0"><ul class="svelte-71l2nh m-0 p-0"><li class=float-left><a class="svelte-71l2nh block hover:text-red-500 no-underline px-3 py-4" href=.>Home</a></li> <li class=float-left><a class="svelte-71l2nh block hover:text-red-500 no-underline px-3 py-4" href=blog aria-current=page rel=prefetch>Blog</a></li> <li class=float-right><a class="block hover:text-red-500 no-underline px-3 py-4 text-black" href=https://github.com/bertini36/blog target=_blank><i class="fa fa-github"></i></a></ul></nav> <main class=svelte-1uhnsl8> <h1>Automatic differentiation</h1> <div class="content svelte-6r8x5z"> <div class=content> <p class=post_p> Derivatives, specifically gradients (derivatives in more than one dimension spaces) and hesians (second derivatives), have become in a fundamental of machine learning. Gradient is a vector which indicates the maximum slope direction of a function at the evaluated point. This is important to move around the function space to find relative minimums or maximums (to minimize or maximize the function). Hesian gives information about the concavity and convexity of the function. Some algorithms use it to improve the exploratory movement over the function space and to find minimums and maximums faster. Automatic Differentiation (AD) is a efficiently and accurately procedure of derivatives calculation of numerical functions, represented as computer programs. As already mentioned, the use of derivatives is very important for model optimization. From a mathematical point of view a model is nothing more than a function that you want to minimize or maximize and derivatives are the tool to do it. </p> <p class=post_p> There are several methods of derivative calculation with a computer: </p> <ul class=post_list> <li> <strong>Numerical differentiation.</strong> This method uses derivative definition to approximate using samples of the original function. In this way we can approximate the gradient ∇f as: <div class="md:px-64 post_image_div"> <figure> <img alt="Derivative deffinition" class=post_image src=automatic-differentiation/derivative_deffinition.png> </figure> </div> where e<small>i</small> is i<small>th</small> unitary vector and h>0 is the step size for the approximation. </li> </ul> </div> </div></main></div> <script>__SAPPER__={baseUrl:"",preloaded:[void 0,null,{post:{title:"Automatic differentiation",slug:"automatic-differentiation",tags:["#python","#tensorflow","#autograd","#linear-regression"],date:"2017-05-03",html:"\n\u003Cdiv class=\"content\"\u003E\n\t\u003Cp class=\"post_p\"\u003E\n\t\tDerivatives, specifically gradients (derivatives in more than one dimension spaces) and hesians (second\n\t\tderivatives), have become in a fundamental of machine learning. Gradient is a vector which indicates the maximum\n\t\tslope direction of a function at the evaluated point. This is important to move around the function space to find\n\t\trelative minimums or maximums (to minimize or maximize the function). Hesian gives information about the concavity\n\t\tand convexity of the function. Some algorithms use it to improve the exploratory movement over the function space\n\t\tand to find minimums and maximums faster. Automatic Differentiation (AD) is a efficiently and accurately\n\t\tprocedure of derivatives calculation of numerical functions, represented as computer programs. As already mentioned,\n\t\tthe use of derivatives is very important for model optimization. From a mathematical point of view a model is\n\t\tnothing more than a function that you want to minimize or maximize and derivatives are the tool to do it.\n\t\u003C\u002Fp\u003E\n\t\u003Cp class=\"post_p\"\u003E\n\t\tThere are several methods of derivative calculation with a computer:\n\t\u003C\u002Fp\u003E\n\t\u003Cul class=\"post_list\"\u003E\n\t\t\u003Cli\u003E\n\t\t\t\u003Cstrong\u003ENumerical differentiation.\u003C\u002Fstrong\u003E\n\t\t\tThis method uses derivative definition to approximate using samples of the original function. In this way\n\t\t\twe can approximate the gradient ∇f as:\n\t\t\t\u003Cdiv class=\"post_image_div md:px-64\"\u003E\n\t\t\t\t\u003Cfigure\u003E\n\t\t\t\t\t\u003Cimg class=\"post_image\" src=\"automatic-differentiation\u002Fderivative_deffinition.png\" alt=\"Derivative deffinition\"\u003E\n\t\t\t\t\u003C\u002Ffigure\u003E\n\t\t\t\u003C\u002Fdiv\u003E\n\t\t\twhere e\u003Csmall\u003Ei\u003C\u002Fsmall\u003E is i\u003Csmall\u003Eth\u003C\u002Fsmall\u003E unitary vector and h\u003E0 is the step size for the approximation.\n\t\t\u003C\u002Fli\u003E\n\t\u003C\u002Ful\u003E\n\u003C\u002Fdiv\u003E\n\t\t"}}]};if('serviceWorker' in navigator)navigator.serviceWorker.register('/service-worker.js');(function(){try{eval("async function x(){}");var main="/client/client.50b15d54.js"}catch(e){main="/client/legacy/client.18c157b7.js"};var s=document.createElement("script");try{new Function("if(0)import('')")();s.src=main;s.type="module";s.crossOrigin="use-credentials";}catch(e){s.src="/client/shimport@1.0.1.js";s.setAttribute("data-main",main);}document.head.appendChild(s);}());</script> 