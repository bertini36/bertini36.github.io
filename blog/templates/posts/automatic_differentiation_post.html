{% load static %}

<p class="text-md text-gray-900 leading-relaxed mt-4">
    Derivatives, specifically gradients (derivatives in more than one dimension spaces) and hesians (second
    derivatives), have become in a fundamental of machine learning. Gradient is a vector which indicates the maximum
    slope direction of a function at the evaluated point. This is important to move around the function space to find
    relative minimums or maximums (to minimize or maximize the function). Hesian gives information about the concavity
    and convexity of the function. Some algorithms use it to improve the exploratory movement over the function space
    and to find minimums and maximums faster. Automatic Differentiation (AD) is a efficiently and accurately
    procedure of derivatives calculation of numerical functions, represented as computer programs. As already mentioned,
    the use of derivatives is very important for model optimization. From a mathematical point of view a model is
    nothing more than a function that you want to minimize or maximize and derivatives are the tool to do it.
</p>
<p class="text-md text-gray-900 leading-relaxed mt-4">
    There are several methods of derivative calculation with a computer:
</p>
<ul class="list-disc px-6 text-md text-gray-900 leading-relaxed mt-4">
    <li>
        <strong>Numerical differentiation.</strong>
        This method uses derivative definition to approximate using samples of the original function. In this way
        we can approximate the gradient ∇f as:
        <div class="mx-auto px-64 mb-6 mt-6">
            <figure>
                <img class="w-full h-full"
                     src="{% static 'img/posts/automatic_differentiation/derivative_deffinition.png' %}"
                     alt="Derivative deffinition">
            </figure>
        </div>
        where e<small>i</small> is i<small>th</small> unitary vector and h>0 is the step size for the approximation.
    </li>
    <li>
        <strong>Symbolic differentiation.</strong>
        It consists on automatic manipulation of the mathematical expressions to obtain the derivatives (similar to
        what we did at school). It requires to implement derivative rules. The problem of this kind of derivation is
        that it can produce long symbolic expressions which are difficult to evaluate.
    </li>
    <li>
        <strong>Automatic Differentiation</strong>.
        It is based on the fact that all functions can be decomposed into a finite number of operations which derivative
        is known. Combining these derivatives the derivative of the original function can be computed. Applying the
        chain rule to each elementary operation of the function we obtain the trace for the calculation of the
        real function derivative.
    </li>
</ul>
<p class="text-md text-gray-900 leading-relaxed mt-4">
    Next image shows the differences between the three methods.
</p>
<div class="mx-auto px-10 mb-6 mt-6">
    <figure>
        <img class="w-full h-full"
             src="{% static 'img/posts/automatic_differentiation/derivative_strategies.png' %}"
             alt="Derivative strategies">
        <figcaption class="text-center text-gray-900 leading-relaxed text-sm">
            Methodologies for calculating derivatives.
        </figcaption>
    </figure>
</div>
<p class="text-md text-gray-900 leading-relaxed mt-4">
    There are two types of Automatic Differentiation (AD): forward mode and reverse mode. On the one hand forward mode
    evaluates the different parts of the function forward and then it does the same for each part of the derivative till
    real function derivative is obtained. On the other hand reverse mode evaluates the different parts of the function
    forward but after, from the derivative of the function, it obtains the partial derivatives.
    This is how the backpropagation method works in neural networks, which needs the partial
    derivatives to update the weights of each of the neural network layers. This method avoids to reuse calculus
    already computed and to calculate derivatives in a very efficient way.
</p>
<div class="mx-auto px-40 mb-6 mt-6">
    <figure>
        <img class="w-full h-full"
             src="{% static 'img/posts/automatic_differentiation/forward_AD.png' %}"
             alt="Automatic differentiation forward mode">
        <figcaption class="text-center text-gray-900 leading-relaxed text-sm">
            AD forward mode schema.
        </figcaption>
    </figure>
</div>
<div class="mx-auto px-40 mb-6 mt-6">
    <figure>
        <img class="w-full h-full"
             src="{% static 'img/posts/automatic_differentiation/backward_AD.png' %}"
             alt="Automatic differentiation reverse mode">
        <figcaption class="text-center text-gray-900 leading-relaxed text-sm">
            AD reverse mode schema.
        </figcaption>
    </figure>
</div>
<h4 class="mt-6 mb-2">Derivatives tools</h4>
<p class="text-md text-gray-900 leading-relaxed mt-4">
    In this post we will be focus on function parameters optimization using Automatic Differentiation. Here are listed
    some software packages to calculate derivatives and gradients:
</p>
<ul class="list-disc px-6 text-md text-gray-900 leading-relaxed mt-4">
    <li><a href="https://www.tensorflow.org/" style="color: #feb72b" target="_blank">Tensorflow</a>: It uses AD reverse mode.</li>
    <li><a href="http://deeplearning.net/software/theano/" style="color: #feb72b" target="_blank">Theano</a>: It uses symbolic differentiation.</li>
    <li><a href="https://www.wolfram.com/mathematica/" style="color: #feb72b" target="_blank">Mathematica</a>: It uses symbolic differentiation.</li>
    <li><a href="https://github.com/HIPS/autograd" style="color: #feb72b" target="_blank">Autograd</a>: It uses AD reverse mode.</li>
</ul>
<h4 class="mt-6 mb-2">Tensorflow</h4>
<p class="text-md text-gray-900 leading-relaxed mt-4">
    It is an open source library developed by Google for numerical computation using flow graphs. Before to execute
    a program, Tensorflow makes a flow graph where nodes represent mathematical operations and edges represent
    multidimensional data vectors also called tensors. The construction of this graph avoids to obtain the most profit
    of system CPUs and GPUs where the program is executed. Then, completely transparent to the programmer, Tensorflow
    parallels everything it can among the resources it dispose.
</p>
<p class="text-md text-gray-900 leading-relaxed mt-4">
    This library was originally designed for <a href="https://en.wikipedia.org/wiki/Deep_learning" style="color: #feb72b" target="_blank">deep learning</a>,
    the machine learning branch that studies neural networks. Tensorflow avoids, in a easy way, to implement Deep Neural
    Networks (DNN), Convulational Neural Networks (CNN) and Recurrent Neural Networks (RNN). However last versions
    have focused on satisfying the rest of machine learning community by trying to convert the
    library in a standard for programming models of all branches. Specifically they have developed a module called
    <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/README.md" style="color: #feb72b" target="_blank">TFLearn</a>
    which has a set of models ready to use and also have updated its syntax with the intention to be more close to
    <a href="http://scikit-learn.org/stable/" style="color: #feb72b" target="_blank">Scikit-learn</a> syntax which is one of the most popular and important
    machine learning libraries.
</p>
<p class="text-md text-gray-900 leading-relaxed mt-4">
    One of the most interesting aspects of this library is that it implements AD reverse model in a very elegant way.
    The coder defines a model indicating its parameters as variables and practically automatically, after specify the
    inference algorithm, Tensorflow is in charge of calculating gradients and apply them in optimization procedures.
</p>
<h4 class="mt-6 mb-2">Usage examples</h4>
<p class="text-md text-gray-900 leading-relaxed mt-4">
    Below is the code to optimize the parameters of a lineal regression model with Tensorflow and with Autograd (both
    use AD reverse model to get gradients). A lineal regression model is defined by the equation:
</p>
<div class="mx-auto px-64 mb-6 mt-6">
    <figure>
        <img class="w-1/2 h-full"
             src="{% static 'img/posts/automatic_differentiation/regresion_lineal.png' %}"
             alt="Lineal regression equation">
    </figure>
</div>
<p class="text-md text-gray-900 leading-relaxed mt-4">
    Where w represents the weight and b the bias. AD will find values for these parameters and these values
    will minimize Mean Squared Error.
</p>
<p class="text-md text-gray-900 leading-relaxed mt-4">
    The model is defined as code in the following way. In Tensorflow parameters to be optimized of a function are
    defined as variables (lines 29 and 30). Later a cost function is defined based on these parameters,
    <a href="https://en.wikipedia.org/wiki/Mean_squared_error" style="color: #feb72b" target="_blank">Mean Squared Error</a> (line 36). Then optimization
    algorithm is specified, in this case <a href="https://en.wikipedia.org/wiki/Gradient_descent" style="color: #feb72b" target="_blank">Gradient Descent</a>.
    And finally we write the code to train the model (last lines). This loop, in each iteration, get a sample from
    the dataset and derive the cost function to obtain the direction (gradient vector) of the local minimum, in
    other words, the direction that reduces Mean Squared Error. With this gradient vector weight and bias parameters
    will be updated (transparently to the programmer). In this way, when a sufficient number of iterations have been
    made, values for the parameters that minimize the cost function will have been obtained (a local minimum will
    have been found).
</p>
<div class="mb-6 mt-6" gist="https://gist.github.com/bertini36/2d60bee837edc107fda03a4cc1fef196.js"></div>
<div class="text-center">
    <small class="text-gray-900 leading-relaxed text-sm">
        Learning weight and bias parameters of a linear regression model with Tensorflow.
    </small>
</div>
<div class="mx-auto px-40 mb-6 mt-6">
    <figure>
        <img class="w-full h-full"
             src="{% static 'img/posts/automatic_differentiation/linear_regression_tf_results.png' %}"
             alt="Linear regression with Tensoflow">
        <figcaption class="text-center text-gray-900 leading-relaxed text-sm">
            Optimization results of model parameters of a lineal regression using Tensorflow.
        </figcaption>
    </figure>
</div>
<p class="text-md text-gray-900 leading-relaxed mt-4">
    Using Autograd all is more visible than in Tensorflow. A cost function is defined with the model parameters (lines
    25-31) and then get gradients in each iteration to update weight and bias parameters (lines 37-40).
</p>
<div class="mb-6 mt-6" gist="https://gist.github.com/bertini36/c3d01aa45bb9ff313c81fcd03517bc7a.js"></div>
<div class="text-center">
    <small class="text-gray-900 leading-relaxed text-sm">
        Learning weight and bias parameters of a linear regression model with Autograd.
    </small>
</div>
<div class="mx-auto px-40  mb-6 mt-6">
    <figure>
        <img class="w-full h-full"
             src="{% static 'img/posts/automatic_differentiation/linear_regression_ag_results.png' %}"
             alt="Linear regression with Tensoflow">
        <figcaption class="text-center text-gray-900 leading-relaxed text-sm">
            Optimization results of model parameters of a lineal regression using Autograd.
        </figcaption>
    </figure>
</div>
<p class="text-md text-gray-900 leading-relaxed mt-4">
    The main objective of this post was to uncover a bit the black box that involves the optimization of models using
    tools as Tensorflow, Theano, <a href="http://pytorch.org/" style="color: #feb72b" target="_blank">Pytorch</a>, ...
</p>
<h4 class="mt-6 mb-2">References</h4>
<ul class="list-disc px-6 text-md text-gray-900 leading-relaxed mt-4">
    <li>
        Automatic differentiation in machine learning: a survey <br>
        Atilim Gunes Baydin, Barak A. Pearlmutter, Alexey Andreyevich Radul, Jeffrey Mark Siskind
    </li>
</ul>

<h5 class="mt-10 mb-10 text-center text-bold text-base" style="color: #feb72b">05/03/2017</h5>
