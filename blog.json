[{"title":"Profiling on social networks","slug":"profiling-on-social-networks","tags":["#python","#twitter","#text-mining","#machine-learning","#lda"],"date":"2019-06-22","html":"\n<p class=\"post_p\">\n\tLast weekend I was coding an application for Twitter timelines analysis which I called\n\t<a href=\"https://github.com/bertini36/profiler\" class=\"post_link\" target=\"_blank\">Profiler</a> (I was just bored). Some years ago I\n\twas working on probabilistic models and there was one which got my attention:\n\t<a href=\"http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf\" class=\"post_link\" target=\"_blank\">Latent Dirichlet Allocation</a>.\n</p>\n<p class=\"post_p\">\n\tThis model was developed by David Blei, Andrew Ng y Michael I. Jordan and tries to find topics in document collections.\n\tIn other words, it groups text documents into topics that the model itself discovers. In this post I'm not going\n\tto explain the model structure and its inference, for this what better than to read the\n\t<a href=\"http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf\" class=\"post_link\" target=\"_blank\">paper</a> itself.\n</p>\n<p class=\"post_p\">\n\tThis model is based on estimating  <a href=\"https://en.wikipedia.org/wiki/Dirichlet_distribution\" class=\"post_link\" target=\"_blank\">\n\tDirichlet distributions</a>. This kind of distributions model the probability of membership to a set of classes.\n\tSpecifically the model builds (using an iterative procedure) Dirichlet distributions to model the\n\tprobability of a word referring to a concrete topic and the probability of membership from each document to\n\teach of the topics. The parameters estimation of these distributions can be done using different types of\n\tBayesian inference like <a href=\"https://albertopou.herokuapp.com/#variational_inference_1\" class=\"post_link\" target=\"_blank\">Variational Inference</a>\n\tor sampling methods as <a href=\"https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo\" class=\"post_link\" target=\"_blank\">Markov Chain Monte Carlo</a>.\n</p>\n<p class=\"post_p\">\n\tProfiler uses this probabilistic model to identify, given a Twitter timeline (set of user tweets), several\n\ttopics that this user writes about. From this idea I coded a Python application that downloads all these tweets of\n\ta user, stores them, preprocesses them and finally looks for its main topics.\n</p>\n<h4 class=\"post_section_title text-2xl text-black\">Technological stack</h4>\n<ul class=\"post_list\">\n\t<li>\n\t\tTwitter data is downloaded using <a href=\"https://github.com/tweepy/tweepy\" class=\"post_link\" target=\"_blank\">Tweepy</a>.\n\t</li>\n\t<li>\n\t\tTo store the timelines I used a <a href=\"https://www.mongodb.com/\" class=\"post_link\" target=\"_blank\">MongoDB</a> database with\n\t\twhich I interacted using <a href=\"https://github.com/mongodb/mongo-python-driver\" class=\"post_link\" target=\"_blank\">Pymongo</a>.\n\t</li>\n\t<li>\n\t\tTextual data preprocessing was done using libraries as  <a href=\"https://github.com/pandas-dev/pandas\" class=\"post_link\" target=\"_blank\">Pandas</a>\n\t\tand <a href=\"https://github.com/nltk/nltk\" class=\"post_link\" target=\"_blank\">NLTK</a>. This preprocessing consisted on cleaning\n\t\ttweets to avoid deviations (delete emoticons, capital letters, symbols, digits, ...)\n\t</li>\n\t<li>\n\t\tTo infer the model I used  <a href=\"https://github.com/RaRe-Technologies/gensim\" class=\"post_link\" target=\"_blank\">Gensim</a>\n\t\tlibrary. This library has a concurrent LDA implementation.\n\t</li>\n\t<li>\n\t\tTo plot obtained results I used <a href=\"https://github.com/bmabey/pyLDAvis\" class=\"post_link\" target=\"_blank\">pyLDAvis</a>\n\t\tlibrary which generates interactive HTML that avoids to explore the results and check which words\n\t\tare the most important in each topic.\n\t</li>\n\t<li>\n\t\tI coded also a command interface to ease interact with the library. To do this I used Google\n\t\t<a href=\"https://github.com/google/python-fire\" class=\"post_link\" target=\"_blank\">Fire</a> library.\n\t</li>\n\t<li>\n\t\tI configured the development environment using\n\t\t<a href=\"https://www.docker.com/\" class=\"post_link\" target=\"_blank\">Docker</a>,\n\t\t<a href=\"https://docs.docker.com/compose/\" class=\"post_link\" target=\"_blank\">Docker compose</a> and\n\t\t<a href=\"https://travis-ci.org/\" class=\"post_link\" target=\"_blank\">Travis</a> to execute automatic tests.\n\t</li>\n</ul>\n<h4 class=\"post_section_title text-2xl text-black\">Command interface</h4>\n<p class=\"post_p\">\n\tProfiler has a command interface. With the next command you could analize some Spain politicians:\n</p>\n<div class=\"post_code\">\n\t<pre><code>make run_all timelines=Albert_Rivera,sanchezcastejon,Pablo_Iglesias_,pablocasado_ n_topics=5</code></pre>\n</div>\n<p class=\"post_p\">\n\tYou can find Profiler installation steps at the\n\t<a href=\"https://github.com/bertini36/profiler/blob/master/README.md\" class=\"post_link\" target=\"_blank\">repository</a>.\n\tIn this <a href=\"https://github.com/bertini36/profiler/blob/master/src/settings.py\" class=\"post_link\" target=\"_blank\">file</a> you\n\tcan configure some application behaviours about the data preprocessing and about the model.\n</p>\n<h4 class=\"post_section_title text-2xl text-black\">Example of obtained results</h4>\n<figure class=\"post_figure flex justify-center\">\n\t<img class=\"md:w-3/4\" src=\"profiling-on-social-networks/profiler1.png\" alt=\"Results\">\n</figure>\n<figcaption class=\"post_image_figcaption\">\n\tGroup 1\n</figcaption>\n<figure class=\"post_figure flex justify-center\">\n\t<img class=\"md:w-3/4\" src=\"profiling-on-social-networks/profiler2.png\" alt=\"Results\">\n</figure>\n<figcaption class=\"post_image_figcaption\">\n\tGroup 4\n</figcaption>\n<p class=\"post_p\">\n\tObtained results using tweets are not as good as obtained using posts or article news. This is because tweets\n\tare short text documents. This reduced length causes a much smaller vocabulary and therefore a difficulty\n\tfor the model to identify differentiated topics. However in this example we can see some interesting groups.\n\tThese results are from Pedro Sánchez timeline (president of Spain). At group 1 we could see that it refers to\n\ttweets about last campaign and at group 4 we see that refers to the issue of sexist violence.\n</p>\n\t\t"},{"title":"Automatic differentiation","slug":"automatic-differentiation","tags":["#python","#tensorflow","#autograd","#linear-regression"],"date":"2017-05-03","html":"\n<p class=\"post_p\">\n\tDerivatives, specifically gradients (derivatives in more than one dimension spaces) and hesians (second\n\tderivatives), have become in a fundamental of machine learning. Gradient is a vector which indicates the maximum\n\tslope direction of a function at the evaluated point. This is important to move around the function space to find\n\trelative minimums or maximums (to minimize or maximize the function). Hesian gives information about the concavity\n\tand convexity of the function. Some algorithms use it to improve the exploratory movement over the function space\n\tand to find minimums and maximums faster. Automatic Differentiation (AD) is a efficiently and accurately\n\tprocedure of derivatives calculation of numerical functions, represented as computer programs. As already mentioned,\n\tthe use of derivatives is very important for model optimization. From a mathematical point of view a model is\n\tnothing more than a function that you want to minimize or maximize and derivatives are the tool to do it.\n</p>\n<p class=\"post_p\">\n\tThere are several methods of derivative calculation with a computer:\n</p>\n<ul class=\"post_list\">\n\t<li>\n\t\t<strong>Numerical differentiation.</strong>\n\t\tThis method uses derivative definition to approximate using samples of the original function. In this way\n\t\twe can approximate the gradient ∇f as:\n\t\t<figure class=\"post_figure flex justify-center\">\n\t\t\t<img class=\"md:w-1/2\" src=\"automatic-differentiation/derivative_deffinition.png\" alt=\"Derivative deffinition\">\n\t\t</figure>\n\t\twhere e<small>i</small> is i<small>th</small> unitary vector and h>0 is the step size for the approximation.\n\t</li>\n\t<li>\n\t\t<strong>Symbolic differentiation.</strong>\n\t\tIt consists on automatic manipulation of the mathematical expressions to obtain the derivatives (similar to\n\t\twhat we did at school). It requires to implement derivative rules. The problem of this kind of derivation is\n\t\tthat it can produce long symbolic expressions which are difficult to evaluate.\n\t</li>\n\t<li>\n\t\t<strong>Automatic Differentiation</strong>.\n\t\tIt is based on the fact that all functions can be decomposed into a finite number of operations which derivative\n\t\tis known. Combining these derivatives the derivative of the original function can be computed. Applying the\n\t\tchain rule to each elementary operation of the function we obtain the trace for the calculation of the\n\t\treal function derivative.\n\t</li>\n</ul>\n<p class=\"post_p\">\n\tNext image shows the differences between the three methods.\n</p>\n<figure class=\"post_figure flex justify-center\">\n\t<img src=\"automatic-differentiation/derivative_strategies.png\"\n\t\t alt=\"Derivative strategies\">\n</figure>\n<figcaption class=\"post_image_figcaption\">\n\tMethodologies for calculating derivatives.\n</figcaption>\n<p class=\"post_p\">\n\tThere are two types of Automatic Differentiation (AD): forward mode and reverse mode. On the one hand forward mode\n\tevaluates the different parts of the function forward and then it does the same for each part of the derivative till\n\treal function derivative is obtained. On the other hand reverse mode evaluates the different parts of the function\n\tforward but after, from the derivative of the function, it obtains the partial derivatives.\n\tThis is how the backpropagation method works in neural networks, which needs the partial\n\tderivatives to update the weights of each of the neural network layers. This method avoids to reuse calculus\n\talready computed and to calculate derivatives in a very efficient way.\n</p>\n<figure class=\"post_figure flex justify-center\">\n\t<img src=\"automatic-differentiation/forward_AD.png\"\n\t\t alt=\"Automatic differentiation forward mode\">\n</figure>\n<figcaption class=\"post_image_figcaption\">\n\tAD forward mode schema.\n</figcaption>\n<figure class=\"post_figure flex justify-center\">\n\t<img src=\"automatic-differentiation/backward_AD.png\"\n\t\t alt=\"Automatic differentiation reverse mode\">\n</figure>\n<figcaption class=\"post_image_figcaption\">\n\tAD reverse mode schema.\n</figcaption>\n<h4 class=\"post_section_title text-2xl text-black\">Derivatives tools</h4>\n<p class=\"post_p\">\n\tIn this post we will be focus on function parameters optimization using Automatic Differentiation. Here are listed\n\tsome software packages to calculate derivatives and gradients:\n</p>\n<ul class=\"post_list\">\n\t<li><a href=\"https://www.tensorflow.org/\" class=\"post_link\" target=\"_blank\">Tensorflow</a>: It uses AD reverse mode.</li>\n\t<li><a href=\"http://deeplearning.net/software/theano/\" class=\"post_link\" target=\"_blank\">Theano</a>: It uses symbolic differentiation.</li>\n\t<li><a href=\"https://www.wolfram.com/mathematica/\" class=\"post_link\" target=\"_blank\">Mathematica</a>: It uses symbolic differentiation.</li>\n\t<li><a href=\"https://github.com/HIPS/autograd\" class=\"post_link\" target=\"_blank\">Autograd</a>: It uses AD reverse mode.</li>\n</ul>\n<h4 class=\"post_section_title text-2xl text-black\">Tensorflow</h4>\n<p class=\"post_p\">\n\tIt is an open source library developed by Google for numerical computation using flow graphs. Before to execute\n\ta program, Tensorflow makes a flow graph where nodes represent mathematical operations and edges represent\n\tmultidimensional data vectors also called tensors. The construction of this graph avoids to obtain the most profit\n\tof system CPUs and GPUs where the program is executed. Then, completely transparent to the programmer, Tensorflow\n\tparallels everything it can among the resources it dispose.\n</p>\n<p class=\"post_p\">\n\tThis library was originally designed for <a href=\"https://en.wikipedia.org/wiki/Deep_learning\" class=\"post_link\" target=\"_blank\">deep learning</a>,\n\tthe machine learning branch that studies neural networks. Tensorflow avoids, in a easy way, to implement Deep Neural\n\tNetworks (DNN), Convulational Neural Networks (CNN) and Recurrent Neural Networks (RNN). However last versions\n\thave focused on satisfying the rest of machine learning community by trying to convert the\n    library in a standard for programming models of all branches. Specifically they have developed a module called\n\t<a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/README.md\" class=\"post_link\" target=\"_blank\">TFLearn</a>\n\twhich has a set of models ready to use and also have updated its syntax with the intention to be more close to\n\t<a href=\"http://scikit-learn.org/stable/\" class=\"post_link\" target=\"_blank\">Scikit-learn</a> syntax which is one of the most popular and important\n\tmachine learning libraries.\n</p>\n<p class=\"post_p\">\n\tOne of the most interesting aspects of this library is that it implements AD reverse model in a very elegant way.\n\tThe coder defines a model indicating its parameters as variables and practically automatically, after specify the\n\tinference algorithm, Tensorflow is in charge of calculating gradients and apply them in optimization procedures.\n</p>\n<h4 class=\"post_section_title text-2xl text-black\">Usage examples</h4>\n<p class=\"post_p\">\n\tBelow is the code to optimize the parameters of a lineal regression model with Tensorflow and with Autograd (both\n\tuse AD reverse model to get gradients). A lineal regression model is defined by the equation:\n</p>\n<figure class=\"post_figure flex justify-center\">\n\t<img src=\"automatic-differentiation/linear_regression.png\"\n\t\t alt=\"Lineal regression equation\">\n</figure>\n<p class=\"post_p\">\n\tWhere w represents the weight and b the bias. AD will find values for these parameters and these values\n\twill minimize Mean Squared Error.\n</p>\n<p class=\"post_p\">\n\tThe model is defined as code in the following way. In Tensorflow parameters to be optimized of a function are\n\tdefined as variables. Later a cost function is defined based on these parameters,\n\t<a href=\"https://en.wikipedia.org/wiki/Mean_squared_error\" class=\"post_link\" target=\"_blank\">Mean Squared Error</a>. Then optimization\n\talgorithm is specified, in this case <a href=\"https://en.wikipedia.org/wiki/Gradient_descent\" class=\"post_link\" target=\"_blank\">Gradient Descent</a>.\n\tAnd finally we write the code to train the model (last lines). This loop, in each iteration, get a sample from\n\tthe dataset and derive the cost function to obtain the direction (gradient vector) of the local minimum, in\n\tother words, the direction that reduces Mean Squared Error. With this gradient vector weight and bias parameters\n\twill be updated (transparently to the programmer). In this way, when a sufficient number of iterations have been\n\tmade, values for the parameters that minimize the cost function will have been obtained (a local minimum will\n\thave been found).\n</p>\n<div class=\"post_code\">\n\t<pre><code>import matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow as tf\n\nrng = np.random\n\n# Parameters\nlearning_rate = 0.01\ntraining_epochs = 100\n\n# Training data\ntrain_X = np.asarray([3.3, 4.4, 5.5, 6.71, 6.93, 4.168, 9.779, 6.182, 7.59,\n\t  2.167, 7.042, 10.791, 5.313, 7.997, 5.654, 9.27, 3.1])\ntrain_Y = np.asarray([1.7, 2.76, 2.09, 3.19, 1.694, 1.573, 3.366, 2.596, 2.53,\n\t  1.221, 2.827, 3.465, 1.65, 2.904, 2.42, 2.94, 1.3])\nn_samples = train_X.shape[0]\n\n# Graph input data\nX = tf.placeholder('float')\nY = tf.placeholder('float')\n\n# Optimizable parameters with random initialization\nweight = tf.Variable(rng.randn(), name='weight')\nbias = tf.Variable(rng.randn(), name='bias')\n\n# Linear model\npredictions = (X * weight) + bias\n\n# Loss function: Mean Squared Error\nloss = tf.reduce_sum(tf.pow(predictions-Y, 2))/(2*n_samples)\n\n# Gradient descent optimizer\noptimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n\n# Initializing the variables\ninit = tf.global_variables_initializer()\n\n# Launch the graph\nwith tf.Session() as sess:\nsess.run(init)\nfor epoch in range(training_epochs):\n\tfor (x, y) in zip(train_X, train_Y):\n\t\tsess.run(optimizer, feed_dict={X: x, Y: y})\ntrain_error = sess.run(loss, feed_dict={X: train_X, Y: train_Y})\nprint('Train error={}'.format(train_error))\n\n# Test error\ntest_X = np.asarray([6.83, 4.668, 8.9, 7.91, 5.7, 8.7, 3.1, 2.1])\ntest_Y = np.asarray([1.84, 2.273, 3.2, 2.831, 2.92, 3.24, 1.35, 1.03])\ntest_error = sess.run(\n\ttf.reduce_sum(tf.pow(predictions - Y, 2)) / (2 * test_X.shape[0]),\n\tfeed_dict={X: test_X, Y: test_Y})\nprint('Test error={}'.format(test_error))\n\nprint('Weight={} Bias={}'.format(sess.run(weight), sess.run(bias)))\n\n# Graphic display\nplt.plot(train_X, train_Y, 'ro', label='Original data')\nplt.plot(train_X, sess.run(weight) * train_X\n\t\t + sess.run(bias), label='Fitted line')\nplt.legend()\nplt.show()</code></pre>\n</div>\n<div class=\"text-center\">\n\t<small class=\"text-gray-900 leading-relaxed text-sm\">\n\t\tLearning weight and bias parameters of a linear regression model with Tensorflow.\n\t</small>\n</div>\n<figure class=\"post_figure flex justify-center\">\n\t<img src=\"automatic-differentiation/linear_regression_tf_results.png\"\n\t\t alt=\"Linear regression with Tensoflow\">\n</figure>\n<figcaption class=\"post_image_figcaption\">\n\tOptimization results of model parameters of a lineal regression using Tensorflow.\n</figcaption>\n<p class=\"post_p\">\n\tUsing Autograd all is more visible than in Tensorflow. A cost function is defined with the model parameters\n\tand then get gradients in each iteration to update weight and bias parameters.\n</p>\n<div class=\"post_code\">\n\t<pre><code>import autograd.numpy as np\nimport matplotlib.pyplot as plt\nfrom autograd import elementwise_grad\n\nrng = np.random\n\n# Parameters\nlearning_rate = 0.01\ntraining_epochs = 100\n\n# Training data\ntrain_X = np.array([3.3, 4.4, 5.5, 6.71, 6.93, 4.168, 9.779, 6.182, 7.59,\n\t2.167, 7.042, 10.791, 5.313, 7.997, 5.654, 9.27, 3.1])\ntrain_Y = np.array([1.7, 2.76, 2.09, 3.19, 1.694, 1.573, 3.366, 2.596, 2.53,\n\t1.221, 2.827, 3.465, 1.65, 2.904, 2.42, 2.94, 1.3])\nn_samples = train_X.shape[0]\n\n\ndef loss((weight, bias)):\n\"\"\" Loss function: Mean Squared Error \"\"\"\npredictions = (train_X * weight) + bias\nreturn np.sum(np.power(predictions - train_Y, 2) / (2 * n_samples))\n\n# Function that returns gradients of loss function\ngradient_fun = elementwise_grad(loss)\n\n# Optimizable parameters with random initialization\nweight = rng.randn()\nbias = rng.randn()\n\nfor epoch in range(training_epochs):\ngradients = gradient_fun((weight, bias))\nweight -= gradients[0] * learning_rate\nbias -= gradients[1] * learning_rate\nprint('Train error={}'.format(loss((weight, bias))))\n\n# Test error\ntest_X = np.array([6.83, 4.668, 8.9, 7.91, 5.7, 8.7, 3.1, 2.1])\ntest_Y = np.array([1.84, 2.273, 3.2, 2.831, 2.92, 3.24, 1.35, 1.03])\npredictions = (test_X * weight) + bias\nprint('Test error={}'.format(\nnp.sum(np.power(predictions - test_Y, 2) / (2 * n_samples))))\n\nprint('Weight={} Bias={}'.format(weight, bias))\n\n# Graphic display\nplt.plot(train_X, train_Y, 'ro', label='Original data')\nplt.plot(train_X, weight * train_X + bias, label='Fitted line')\nplt.legend()\nplt.show()</code></pre>\n</div>\n<div class=\"text-center\">\n\t<small class=\"text-gray-900 leading-relaxed text-sm\">\n\t\tLearning weight and bias parameters of a linear regression model with Autograd.\n\t</small>\n</div>\n<figure class=\"post_figure flex justify-center\">\n\t<img src=\"automatic-differentiation/linear_regression_ag_results.png\"\n\t\t alt=\"Linear regression with Tensoflow\">\n</figure>\n<figcaption class=\"post_image_figcaption\">\n\tOptimization results of model parameters of a lineal regression using Autograd.\n</figcaption>\n<p class=\"post_p\">\n\tThe main objective of this post was to uncover a bit the black box that involves the optimization of models using\n\ttools as Tensorflow, Theano, <a href=\"http://pytorch.org/\" class=\"post_link\" target=\"_blank\">Pytorch</a>, ...\n</p>\n<h4 class=\"post_section_title text-2xl text-black\">References</h4>\n<ul class=\"post_list\">\n\t<li>\n\t\tAutomatic differentiation in machine learning: a survey <br>\n\t\tAtilim Gunes Baydin, Barak A. Pearlmutter, Alexey Andreyevich Radul, Jeffrey Mark Siskind\n\t</li>\n</ul>\n\t\t"},{"title":"Distributed matrix product","slug":"distributed-matrix-product","tags":["#CUDA","#MPI","#C","#maths"],"date":"2017-02-02","html":"\n<p class=\"post_p\">\n\tNowadays one of the main problems of any algorithm is its facility to scale it, in other words, its facility to\n\tbe executed over multiple cores (parallelization) or nodes (distribution). The amount of data that is available\n\ttoday has led the world of computing to develop technologies with which to parallelize and distribute processes\n\tare done more easily and automatically. Actually, each computationally expensive algorithm works in its\n\tdistributed version. In this way tasks as finding prime numbers, complex simulations or statistical prediction\n\tmodels, which would spend years to get results, can obtain it in a few hours thanks to the joint work of many\n\tcomputers.\n</p>\n<p class=\"post_p\">\n\tPrecisely this is what supercomputing centers like <a href=\"https://www.bsc.es/\" class=\"post_link\" target=\"_blank\">BSC</a> are\n\tengaged. BSC has a supercomputer, called Marenostrum III, which avoids to its users to run computationally\n\texpensive algorithms or algorithms that require a huge amount of data in a distributed way. Specifically,\n\tMarenostrum III has 3108 nodes each with 2 processors of 8 cores (over 49.728 cores).\n</p>\n<figure class=\"post_figure flex justify-center\">\n\t<img src=\"distributed-matrix-product/marenostrum.jpg\"\n\t\t alt=\"Marenostrum III supercomputer\">\n</figure>\n<figcaption class=\"post_image_figcaption\">Marenostrum III</figcaption>\n<p class=\"post_p\">\n\tIn this post I'm going to show, with a simple example, how to distribute a matrix product over several nodes and\n\thow to parallelize at each node. Matrix product is a mathematical operation that, when it is taken to the extreme\n\tusing very large matrices it is become in a very computationally expensive operation.\n</p>\n<figure class=\"post_figure flex justify-center\">\n\t<img src=\"distributed-matrix-product/matrix_product.png\"\n\t\t alt=\"Matrix product\">\n</figure>\n<figcaption class=\"post_image_figcaption\">Matrix product</figcaption>\n<p class=\"post_p\">\n\tSince each element of the resulting matrix does not depend on any other element of it we can distribute without\n\tany restriction. For this example I could use 4 nodes, so I decided that each node would be responsible for\n\tcalculating a quarter of the resulting matrix. A and B matrices will be sent to each node to avoid them to\n\tdo its computations. For the distribution I'm using <a href=\"https://www.open-mpi.org/\" class=\"post_link\" target=\"_blank\">MPI</a>\n\t(Message Passing Interface) technology. Using functions of this library data can be shared over the nodes\n\t(lines 123 and 124) and to specify, using process id, which part of the result matrix each node has to calculate\n\t(lines 116, 117 and 118).\n</p>\n<figure class=\"post_figure flex justify-center\">\n\t<img src=\"distributed-matrix-product/distributed_matrix_product.png\"\n\t\t alt=\"Distributed matrix product\">\n</figure>\n<figcaption class=\"post_image_figcaption\">Matrix product distribution and parallelization</figcaption>\n<p class=\"post_p\">\n\tNowadays, for parallel computation, the power of GPUs is being used. In our case each node of Marenostrum III has a\n\tNvidia K80 graphic card. This graphic cards can be used for parallel computation using Nvidia\n\t<a href=\"https://en.wikipedia.org/wiki/CUDA\" class=\"post_link\" target=\"_blank\">CUDA</a> (Compute Unified Device Architecture)\n\tplatform. GPUs are essentially a big number of simple processors that can be used to speed up some parts of the\n\tcode. For this, the program has to be decomposed in a big number of threads which will be executed concurrently.\n\tIn this example, each thread will be responsible for calculating just an element of the resulting matrix. Using\n\tCUDA it is required to define a block structure (threads group) delimiting each thread data domain using its\n\tthreadId and its blockId. These threads will be executed in one kernel, in other words, a GPU program.\n</p>\n<div class=\"post_code\">\n\t<pre><code>/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */\n/* MULTI-NODE AND PARALLEL MATRIX-MATRIX PRODUCT WITH MPI AND CUDA           */\n/*                                                                           */\n/* File:         mmpmpicuda.cu                                               */\n/* Author:       Alberto Pou Quirós (Github: bertini36)                      */ \n/* Description:  This program performs a matrix product (A * B = C)          */\n/*               distributing the computation between multiple nodes         */\n/*               with MPI technology and parallelizing the computation in    */\n/*               every node with Nvidia CUDA technology                      */\n/* Compilation:  nvcc -I/opt/mpi/bullxmpi/1.2.9.1/include                    */  \n/*               -L/opt/mpi/bullxmpi/1.2.9.1/lib -lmpi -ldl -lm -lnuma       */\n/*               -lrt -lnsl -lutil -lm -ldl mmpmpicuda.cu -o mmpmpicuda      */\n/* Strategy:                                                                 */\n/*                  Example 16x16 matrices with 4 nodes:                     */\n/*                   _________________16________________                     */\n/*                   |                                 |                     */\n/*                   |               NODE 1            | 4                   */\n/*                   |_________________________________|                     */\n/*                   |                                 |                     */\n/*                   |               NODE 2            | 4                   */\n/*              C =  |_________________________________|    16               */ \n/*                   |                                 |                     */\n/*                   |               NODE 3            | 4                   */\n/*                   |_________________________________|                     */\n/*                   |                                 |                     */ \n/*                   |               NODE 4            | 4                   */\n/*                   |_________________________________|                     */\n/*                                                                           */\n/*                  Node 1 computes 4 rows of result matrix:                 */\n/*                   __________________________________                      */\n/*                   |                                 |                     */\n/*                   |         4x16 CUDA block         |                     */\n/*                   |_________________________________|                     */\n/*                                                                           */\n/* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */\n\n#include &lt;sys/time.h&gt;\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;assert.h&gt;\n#include &lt;mpi.h&gt;\n\n#define N 1024\n\n#define err(format, ...) do { fprintf(stderr, format, ##__VA_ARGS__); exit(1); } while (0)\n\nstruct timeval start_time, end_time;\n\ninline void checkCuda(cudaError_t e) {\n    if (e != cudaSuccess) {\n        err(\"CUDA Error %d: %s\\n\", e, cudaGetErrorString(e));\n    }\n}\n\n__global__ void matrixProduct(double *matrix_a, double *matrix_b, double *matrix_c, int width, int nrows, int my_rank) {\n    int row = threadIdx.y + blockDim.y * blockIdx.y;\n    int col = threadIdx.x + blockDim.x * blockIdx.x; \n    matrix_c[row * width + col] = 0;\n    for (int k=0; k&lt;width; k++) {\n        matrix_c[row * width + col] += matrix_a[(row * width) + (my_rank * nrows * width) + k] * matrix_b[k * width + col];\n    }\n}\n\nvoid initializeMatrices(double matrix_a[N][N], double matrix_b[N][N]) {\n    int i, j;\n    srand(time(NULL));\n    for (i=0; i&lt;N; i++) {\n        for (j=0; j&lt;N; j++) {\n            matrix_a[i][j] = rand();\n            matrix_b[i][j] = rand();\n        }\n    }\n}\n\nvoid showMatrices(double matrix_a[N][N], double matrix_b[N][N], double matrix_c[N][N]) {\n    int i, j;\n    srand(time(NULL));\n    printf(\"***** MATRIX A ***** \\n\");\n    for (i=0; i&lt;N; i++) {\n        for (j=0; j&lt;N; j++) {\n            (j % N == N-1) ? printf(\"%.1f \\n\", matrix_a[i][j]) : printf(\"%.1f,\", matrix_a[i][j]);\n        }\n    }\n    printf(\"***** MATRIX B ***** \\n\");\n    for (i=0; i&lt;N; i++) {\n        for (j=0; j&lt;N; j++) {\n            (j % N == N-1) ? printf(\"%.1f \\n\", matrix_b[i][j]) : printf(\"%.1f,\", matrix_b[i][j]);\n        }\n    }\n    printf(\"***** RESULT MATRIX ***** \\n\");\n    for (int i=0; i&lt;N; i++) {\n        for (int j=0; j&lt;N; j++) {\n            (j % N == N-1) ? printf(\"%f \\n\", matrix_c[i][j]) : printf(\"%f,\", matrix_c[i][j]);\n        }\n    }\n}\n\nint main(int argc, char *argv[]) {\n\n    double A[N][N], B[N][N], C[N][N];\n    double *d_a, *d_b, *d_c;\n    int my_rank, comm_sz, from, to, nrows;\n  \n    // MPI initialization\n    MPI_Init (&argc, &argv);\n    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);    // Process id \n    MPI_Comm_size(MPI_COMM_WORLD, &comm_sz);    // Number of processors \n\n    if (N % comm_sz != 0) {\n        if (my_rank == 0) printf(\"Matrix size not divisible by number of processors \\n\");\n        MPI_Finalize();\n        exit(-1);\n    }\n\n    // Calculate interval lines to compute per node\n    from = my_rank * N / comm_sz;\n    to = (my_rank + 1) * N / comm_sz;\n    nrows = to - from;\n\n    if (my_rank == 0) { initializeMatrices(A, B); }\n\n    // Send A y B to every node\n    MPI_Bcast(A, N*N, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    MPI_Bcast(B, N*N, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    // Allocate memory in the device\n    checkCuda(cudaMalloc((void **) &d_a, N*N*sizeof(double)));\n    checkCuda(cudaMalloc((void **) &d_b, N*N*sizeof(double)));\n    checkCuda(cudaMalloc((void **) &d_c, (N*N/comm_sz)*sizeof(double)));\n\n    // Copy the information in the device\n    checkCuda(cudaMemcpy(d_a, A, N*N*sizeof(double), cudaMemcpyHostToDevice));\n    checkCuda(cudaMemcpy(d_b, B, N*N*sizeof(double), cudaMemcpyHostToDevice));\n\n    // CUDA threads structure definition\n    dim3 dimGrid(1);\n    dim3 dimBlock(N, nrows);    \n\n    MPI_Barrier(MPI_COMM_WORLD);\n    if (my_rank == 0) { gettimeofday(&start_time, NULL); }\n\n    // Kernel launch\n    matrixProduct&lt;&lt;&lt;dimGrid, dimBlock&gt;&gt;&gt;(d_a, d_b, d_c, N, nrows, my_rank);\n    checkCuda(cudaDeviceSynchronize());\n    checkCuda(cudaGetLastError());\n\n    // Calculate compute time\n    MPI_Barrier(MPI_COMM_WORLD);\n    if (my_rank == 0) { \n        gettimeofday(&end_time, NULL);\n        printf(\"Compute time: %.1f ms \\n\", (float) (end_time.tv_sec - start_time.tv_sec) * 1000 + (end_time.tv_usec - start_time.tv_usec) / 1000);\n     }\n\n    // Get results from device\n    checkCuda(cudaMemcpy(C[from], d_c, (nrows)*N*sizeof(double), cudaMemcpyDeviceToHost));\n\n    // Unify results from nodes\n    MPI_Gather(C[from], N*N/comm_sz, MPI_DOUBLE, C, N*N/comm_sz, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    // if (my_rank == 0)  { showMatrices(A, B, C); }\n\n    checkCuda(cudaFree(d_a));\n    checkCuda(cudaFree(d_b));\n    checkCuda(cudaFree(d_c));\n    \n    MPI_Finalize();\n    return 0;\n\n}</code></pre>\n</div>\n<p class=\"post_p\">\n\tOne of the main problems of CUDA is that to get the most out of it you have to know the technical specifications\n\tof the GPU on which it will run the code. Block size (number of threads) or the use of the memory hierarchy\n\tof the graphic card are aspects that the coder has to take into account when programming with this technology.\n</p>\n<p class=\"post_p\">\n\tAt this <a href=\"https://github.com/bertini36/distributedMatrixProduct\" class=\"post_link\" target=\"_blank\">Github repository</a>\n\tyou will find two versions of matrix product, one just using MPI technology and another just with CUDA. Also,\n\tat config folder, there are a script to know graphic card specifications and a possible CUDA configuration\n\tfor that card.\n</p>\n\t\t"}]